


\chapter{Introduction}
\section{Motivation and Background}
\section{Contributions and outline of the thesis}

The thesis is structured as follows.


complex outputs: multiple interdependent output variables and structured output space


\chapter{Background}

\newpage
\section{Single Label Classification}

\subsection{Preliminary and Notation}
We consider supervised learning problem which assumes an arbitrary input space $\Xcal$, an output space $\Ycal$, and the training samples coming in pairs $(x_i,y_i)\in\Xcal\times\Ycal$.
As we focus in this chapter standard supervised learning problem also known as binary classification, we explicitly assume the output space $\Ycal=\{-1,+1\}$ (instead of $\Ycal=\{0,+1\}$).
We point out that other kind of supervised learning problem can be formulated by altering the definition of the output space.
For example, by setting $\Ycal=\{1,\cdots,K\}$ we have multiclass classification problem, and by setting $\Ycal=\RR$ we have regression problem.
Additionally, we assume a feature map $\vphi:\Xcal\rightarrow\Fcal$, which embeds the input in some high dimensional feature space $\Fcal=\RR^d$.
In particular, $\vphi(\vx)$ is a real value vector of $d$ dimension.
Given a set of training data $\Scal=\{(\vx_1,y_1),\cdots,(\vx_m,y_m)\}$, the goal is to learn from a \textit{hypothesis class} $\Fcal$ a mapping function $f\in\Fcal:\Xcal\rightarrow\Ycal$  that maps an input $\vx\in\Xcal$ to an output $y\in\Ycal$. 

The hypothesis class we consider is a set of \textit{linear classifiers} that are parameterized by weight vector $\vw$ and bias term $b$ that takes the form
\begin{align}
	f(x;\vw,b) = \ip{\vw}{\vx} + b, \label{linearclassifier}
\end{align}
where by $\ip{\dot\,}{\dot\,}$ we denote inner product of two vectors.

% TODO: averaging perceptron
% TODO: averaging perceptron algorithm psudocode




\subsection{Perceptron}

The Rosenblatt's Perceptron algorithm \citep{Rosenblatt58,Rosenblatt62} is one type of linear classifiers that is parameterized by a weight vector $\vw$ and a bias term $b$. 
The decision boundary given by 
\begin{align*}
	f(x;\vw,b) = \ip{\vw}{\vx} + b =0,
\end{align*}
will separate the data into two classes.
The goal is to learn $\vw$ and $b$ by minimizing the distance of misclassified training examples to the decision boundary.

Assume data point $(x_i,y_i)$ is misclassified, the signed geometric distance from $x_i$ to decision boundary can be computed 
\begin{align*}
	D(x_i;\vw,b) = -\frac{y_i(\ip{\vw}{\vx_i}+b)}{\norm{\vw}}.
\end{align*}
Denote by $\Xcal^m$ the set of misclassified examples, the goal of perceptron is to minimize the distance over all misclassified examples defined as
\begin{definition}{Perceptron Objective Function}\label{perceptron}
	\begin{align*}
		\underset{\vw,b}{\minimize} \, D(\vw,b) = \underset{\vw,b}{\minimize} \left( -\sum_{x_i\in\Xcal^{m}}y_i[\ip{\vw}{\vx_i}+b]\right).
	\end{align*}
\end{definition}
To achieve (Definition~\ref{perceptron}), we assume $\Xcal^{m}$ is fixed and compute the partial gradient with respect to parameters
\begin{align*}
	\frac{\partial D(\vw,b)}{\partial\vw} = -\sum_{x_i\in\Xcal^m}{y_ix_i},\\
	\frac{\partial D(\vw,b)}{\partial b} = -\sum_{x_i\in\Xcal^m}{y_i}.
\end{align*}
In practice, the perceptron algorithm uses stochastic gradient descent that processes iteratively from training data one example at a time.
At each step, the algorithm makes sure the current $\vw$ and $b$ will correctly separate the current training example. 
Once a misclassified example is visited, it adjust the parameter according to the update rule
\begin{align*}
	\vw &\leftarrow \vw + \rho y_i x_i,\\
	b &\leftarrow b + \rho y_i,
\end{align*}
where $\rho$ is the perceptron learning rate.
\begin{theory}[Perceptron Convergence Theorem \citep{Block62the,Novikoff62}]\label{perceptron_theory}
	Given a sequence of training examples in pairs $\Scal=\{(x_1,y_1),\cdots,(x_m,y_m)\}$. 
	For simplicity, we consider a perceptron without bias term $b$.
	Assume $\norm{\vx_i}\le R$ for all $i\in\{1,\cdots,m\}$.
	Suppose for some $\gamma>0$ there exists a unit length vector $\norm{\hat{\vw}}=1$ such that $y_i(\ip{\hat{\vw}}{\vx_i})\ge\gamma$ for $i$ (training data $\Scal$ is linearly separable).
	Then the number of mistakes the perceptron algorithm makes on the sequence of training data $\Scal$ is at most ${R^2}/{\gamma^2}$.
\end{theory}
\begin{proof}
	Suppose the $k$'th mistake is made on the $i$'th training example, and current weight vector is $\vw^k$.
	In addition, we set $\vw^0=\vzero$.
	As the Perceptron makes a mistake on $(x_i,y_i)$, we immediately have
	\begin{align*}
		y_i\ip{\vw_k}{\vx_i} \le 0.
	\end{align*}
	According to update rule, we have
	\begin{align*}
		\vw^{k+1} = \vw^{k} + \rho y_ix_i.
	\end{align*}
	Then, the following holds
	\begin{align*}
		\ip{\vw^{k+1}}{\hat{\vw}} = \ip{\vw^k}{\hat{\vw}} + \rho y_i\ip{\vx_i}{\hat{\vw}} \ge \ip{\vw^k}{\hat{\vw}} + \rho\gamma.
	\end{align*}
	Straight forward induction give us
	\begin{align}
		\ip{\vw^{k+1}}{\hat{\vw}}\ge k\rho\gamma. \label{induction1}
	\end{align}
	We also have
	\begin{align*}
		\norm{\vw^{k+1}}^2 
		= \norm{\vw^{k} + \rho y_ix_i}^2
		= \norm{\vw^k}^2 + \rho^2\norm{\vx_i}^2 + 2\rho y_i\ip{\vx_i}{\vw^k}
		\le \norm{\vw^k}^2 + \rho^2R^2
	\end{align*}
	Straight forward induction give us
	\begin{align}
		\norm{\vw^{k+1}}^2\le k\rho^2R^2. \label{induction2}
	\end{align}
	Together with (\ref{induction1}) and (\ref{induction2}) we have
	\begin{align*}
		k\le{R^2}/{\gamma^2}.
	\end{align*}
\end{proof}
Theory~\ref{perceptron_theory} shows that if the data are linearly separable, the perceptron algorithm will make a finite number of mistakes. 
In particular, the Perceptron algorithm will iterate through the training set and will converge to a vector that well separates all training examples.
Moreover, the number of mistakes will be bounded by the minimum gap between the positive and negative examples.
That is the algorithm converges quickly when the gap $\gamma$ is big.
We have to keep in mind that the convergence theorem does not say anything about the quality of the solution, though it is built based on the the assumption that there is a ''gap'' between positive and negative examples.

The standard Perceptron algorithm supposes that the data are linearly separable, iterates over the training set, and eventually converges.
However, the setting is less interesting for most applications where the data might not be linearly separable or it takes too long for the algorithm to converge.
Therefore, we need to find out the best decision rule given a set of updates.

\textit{Voted Perceptron} developed in \citep{Freund99} is a straight forward modification of the standard Perceptron
As described in (Algorithm~\ref{voted_perceptron}), the Voted perceptron algorithm keeps track all weight vectors that appears during training phase. (line~\ref{perceptron_algorithm_weight_vector}) and their weights (line~\ref{perceptron_algorithm_weight}).
The weight can be seen as the 'survival time' of the weight vector during Perceptron training.
The prediction for a new test example $\vx_{ts}$ is computed by 
\begin{align}
	y_{ts} = \sign\left({\sum_{l=1}^{k}c^l(\sign{\ip{\vw^l}{\vx_{ts}}+b^{l}}})\right) \label{voted_perceptron_prediction}
\end{align}
using the weight vectors and the corresponding weights outputted from the algorithm (line~\ref{perceptron_algorithm_average}).
\begin{algorithm}
\caption{Voted Perceptron Learning Algorithm}
\label{voted_perceptron}
\begin{algorithmic}[1]
	\REQUIRE Training sample $\lbrace (x_i,\vy_i)\rbrace_{i=1}^m$, iteration limit $T$
	\ENSURE Weight parameter $\vw$
	\STATE $\vw^{1}=\vzero,c^1=0$
	\STATE $k=1$
	\FOR{$t=1\cdots T$}
		\FOR{$i=1\cdots m$}
			\STATE Compute $\hat{y} = \sign(\ip{\vw}{\vx_i})$
			\IF{$\hat{y} = y_i$}
				\STATE $c^k = c^k + 1$
			\ELSE
				\STATE $\vw^{k+1} = \vw^{k} + \rho y_ix_i$ \label{perceptron_algorithm_weight_vector}
				\STATE $c^{k+1} =1$ \label{perceptron_algorithm_weight}
				\STATE $k=k+1$
			\ENDIF
		\ENDFOR
	\ENDFOR
	\RETURN $\vw = \{\vw^{k}\}_{l=1}^{k},\,c = \{c^{k}\}_{l=1}^{k}$ \label{perceptron_algorithm_average}
\end{algorithmic}
\end{algorithm}

% TODO: vote perceptron error bound, add reference 
It is proved that the voted perceptron guarantees to achieve better generalization error than original perceptron \citep{}.
However, voted perceptron is not practical as it has to remember all weight vectors encountered during the training.
The \textit{averaged perceptron} is a simple alternative, where in stead of store all weight vectors the algorithm only keep the weighted sum $\sum_{l=1}^{k}c^k\vw^l$. 
The prediction for a new test example $\vx_{ts}$ is computed by 
\begin{align*}
	y_{ts} = \sign\left(c^l({\sum_{l=1}^{k}{\ip{\vw^l}{\vx_{ts}}+b^{l}}})\right) = \sign\left({{\ip{c^l\sum_{l=1}^{k}\vw^l}{\vx_{ts}}+c^l\sum_{l=1}^{k}b^{l}}}\right),
\end{align*}
which is extreme similar to (\ref{voted_perceptron_prediction}) except for the inside $\sign$ operation, and is more efficient to compute.

%
% logit
\subsection{Logistic Regression (\lr)}

% general introduction
Logistic regression is another important method in statistical machine learning and is closely related to the Perceptron and the Support Vector Machine (Section~\ref{sc_svm}).
It is a classification model rather than regression \citep{Bishop07}.
It models the conditional probability $p(y=+1|\vx)$ for a binary output variable $y\in\Ycal$.
%Without loss of generality, in the rest part of this section we use $p(\vx)$ to denote above conditional probability of $p(y|\vx)$ when fixing $y=1$.
To model the probability, we do not restrict to any particular form, as any unknown parameters can be estimated by \textit{maximum likelihood estimation} (MLE).
However, we are most interested in the simple linear model as described in (\ref{linearclassifier}).

% detailed model, decision boundary
To use linear model in Logistic Regression, the first choice is to let $p(y=+1|\vx)$ be a linear function of $\vx$, while the problem is the linear function is unbounded but the probability $p(\vx)\in[0,1]$.
Another choice is to let $\log p(y=+1|\vx)$ be a linear function of $\vx$. 
However, the problem is the log-likelihood ranges from zero to infinite but the linear function is unbounded.
The choice in Logistic Regression is to use logistic transformation of the original probability function
\begin{align*}
	\log\frac{p(y=+1|\vx)}{1-p(y=+1|\vx)} = \ip{\vw}{\vx} + b,
\end{align*}
which by solving for $p(y=+1|\vx)$ results in 
\begin{align}
	p(y=+1|\vx;\vw,b) = \frac{1}{1+\exponential{-\ip{\vw}{x}-b}}. \label{lr_1}
\end{align}
We can also compute
\begin{align}
	p(y=-1|\vx;\vw,b)=1-p(y=+1|\vx;\vw,b)=\frac{1}{1+\exponential{\ip{\vw}{x}+b}}. \label{lr_2}
\end{align}
Putting together (\ref{lr_1}) and (\ref{lr_2}), we define the Logistic Regression as
\begin{definition}{Logistic Regression}\label{logistic_regression}
	\begin{align*}
		p(y=\pm1|\vx;\vw,b) = \frac{1}{1+\exponential{-y_i(\ip{\vw}{x}-b)}}.
	\end{align*}
\end{definition}
Naturally, we expect the prediction $y=+1$ when $p(y=+1|x;\vw,b)\ge0.5$ and $y=-1$, when $p(y=+1|x;\vw,b)<0.5$.
The decision rule is similar to perceptron where we predict $y=+1$ when $\ip{\vw}{x}+b\ge0$, and $y=-1$ otherwise.
Besides decision boundary, Logistic Regression can output the class probability from (Definition~\ref{logistic_regression}) as the the 'distance' of the data point to the decision boundary.
It is the probabilistic output that makes Logistic Regression no more than a classifier, as it outputs detailed predictions (class probability).

% optimization
As the model is capable of outputting class probability not just class category, to learn a Logistic Regression model we attempt to learn parameters $\vw$ and $b$ by maximizing the probability (likelihood) of the training data.
In particular, the probability of training data $\vx$ with class label $y$ is $p(y|\vx)$.
The likelihood of parameters given data can be computed from
\begin{align}
	L(\vw,b;D) = \prod_{i=1}^mp(y_i|\vx_i). \label{lr_likelihood}
\end{align}
To apply MLE, it is easier if, instead of maximizing the likelihood, we maximize the log-likelihood, which turn the product (\ref{lr_likelihood}) into sum
\begin{align}
	\log L(\vw,b|D) = \sum_{i=1}^{m}\log p(y_i|\vx_i) = -\sum_{i=1}^{m}\log (1+\exponential{-y_i(\ip{\vx}{\vw}+b)}) \label{lr_likelihood_sum}.
\end{align}

% logistic regression of other type
Though MLE training for Logistic Regression model meet our need of fitting training data, it does not guarantee to generalize well on test data.
To achieve better generalization power, various smoothing techniques have been proposed \citep{Chen99,Chen00,Goodman03}, among which adding Gaussian prior on weight parameter $\vw$ is one of the standard treatment.
In practice, a zero-mean spherical Gaussian with variance $\sigma^2$ is assumed on $\vw$.
Thus, the maximum likelihood problem (\ref{lr_likelihood}) is transformed into \textit{Maximum A-Posteriori} (MAP) problem of the following form
\begin{align}
	p(\vw,b|D;\sigma^2) = p(\vw|\sigma^2)\prod_{i=1}^mp(y_i|\vx_i)=\exponential{-\frac{\norm{\vw}^2}{\sigma^2}}\prod_{i=1}^{m}{\frac{1}{1+\exponential{-y_i(\ip{\vx}{\vw}+b)}}}. \label{lr_posteriori}
\end{align}
Instead of maximizing the posteriori (\ref{lr_posteriori}), it is easier to maximize the log-posteriori
\begin{align}
	\log p(\vw,b|D;\sigma^2) = -{\frac{\norm{\vw}^2}{\sigma^2}} - \sum_{i=1}^{m}\log{(1+\exponential{-y_i(\ip{\vx}{\vw}+b)})}. \label{lr_posteriori_log}
\end{align}

Numbers of optimization techniques have been proposed to solve the maximization problem (\ref{lr_posteriori_log}) in Logistic Regression, many of which are covered in the survey \citep{Minka03}.
For example, \textit{Itrative Scaling} methods were proposed and continuously developed in \citep{Darroch72,Pietra97,Berger97,Goodman02Sequential,Jin03a}.
A quasi-Newton method was discussed in \citep{Minka03}.
\citet{Komarek05making,Lin2008trust} have proposed truncated Newton method.
And coordinate descent method was proposed in \citep{Huang09iterative}.

% dual for and optimization
There are also efforts being made to solve the logistic regression from its dual, of which the first algorithm was proposed in \citep{Jaakkola99probabilistic}.
The key is to introduce a set of tighter upper bound on (\ref{lr_posteriori_log}) that are parameterized by $\valpha$.
The bound should have a simple form such that the maximizing (\ref{lr_posteriori_log}) over $\vw$ can be solved analytically with $\alpha$.
The solution to the original problem is transferred as finding the tighter upper bound for  $\vw$, which is to minimize with respect to $\alpha$.
The dual form is given by 
\begin{align*}
	\underset{\valpha}{\minimize} & \quad\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\valpha_iy_ix_ix_jy_j\valpha_j + \sum_{i=1}^{m}\left[\valpha_i\log\alpha_i+(\sigma^2-\valpha_i)\log(\sigma^2-\valpha_i)\right] \\
	\st &\quad 0\le\alpha\le\sigma^2, \, \forall i=\{1,\cdots,m\}.
\end{align*}
Later on, optimization algorithms that are based on dual form have been developed, for example iterative optimization method \citep{Keerthi05a} and the dual coordinate descent method \citep{Yu11dual}.

% TODO: add detail deviation of dual form of logistic regression
% TODO: explain the loss and gain of different optimzation algorithm


%
% svm
\subsection{Support Vector Machine (\svm)}
\label{sc_svm}

% some history
Support Vector Machine (\svm) is probably the most widely used classification algorithm in various real world applications.
It is also among the best plug-and-play machine learning algorithms that usually achieves good classification performance.
In this section we will begin the story of \svm\ by first introducing the maximum margin principle of separating training data.
Then we will discuss the primal-dual optimization strategy and the kernel methods that allows \svm\ to deal with high dimensionality.
In the end we will try to cover the optimization strategy developed for \svm.

The algorithm framework of \svm\ was firstly introduced in \citep{Boser92,Cortes95support}.
Theory and algorithm are detailed in book chapters e.g. \citep{Scholkopf02learning,taylor04,Bishop07}.
% hard margin svm
We begin our discussion by considering a very simple case where we assume the data are linearly separable, that is there is a hyperplane in the feature space which separates the training data into two classes.
In addition we assume the \textit{separating hyperplane} has a simple form
\begin{align*}
	f(\vx)=\ip{\vw}{\vx} + b = 0,
\end{align*}
such that for training data we have $y_i=-1$ if $f(\vx_i)<0$, and $y_i=+1$ if $f(\vx_i)>0$.
Given $\vw$ that achieves correct separation on the training data, we can decide the label of an arbitrary test example $x_{ts}$ by the decision rule $y_{ts}=\sign(f(\vx_{ts}))$.

However, there can be infinite number of separating hyperplane that solves the separation problem on the training data.
We wish to find the one that also generalize well on the test data.
A good strategy is to look for a hyperplane that keeps the maximum distance from examples of two classes.
The strategy is often known as \textit{maximum-margin principal}.
To see this, imagining putting a separating hyperplane close to one class of examples which will achieve better classification on test examples from the other class.

Based on the maximum-margin principal, we further denote by $\gamma_i$ the \textit{margin} for the $i$'th example which is defined as the geometric distance from the data point to the separating hyperplane
\begin{align*}
	\gamma_i = \frac{y_i(\ip{\vw}{\vx_i}+b)}{\norm{\vw}}.
\end{align*}
We notice if we scale $\vw$ and $b$ with any constant (e.g. $\vw\leftarrow\kappa\vw, b\leftarrow\kappa b, \kappa\in\RR$, ) the margin $\gamma_i$ stays unchanged. 
We still achieve the same classification and generalization performance.
As parameters are invariance to scaling, we set $\norm{\vw}=1$.
Given a set of training example $D$, we also define the margin with respect to $D$ as the minimum margin of individual training example
\begin{align*}
	\gamma = \underset{i\in\{1,\cdots,m\}}{\minimize}\gamma_i.
\end{align*}

Our goal is to find the separating hyperplane such that it maximizes the margin with respect to all training data while separating the training examples into two classes.
Naturally, this corresponds to finding a ''big-gap'' between two classes.
The max-margin solution is given as \citep{Bishop07}
\begin{align*}
	\underset{\vw,b,\gamma}{\maximize} &\quad \gamma\\
	\st &\quad y_i(\ip{\vw}{\vx_i}+b) \ge \gamma, \, \norm{\vw}=1,\\
	 &\quad \forall i\in\{1,\cdots,m\}.
\end{align*}
The objective maximizes the minimum margin over all training examples such that all examples are correctly separated with margin at least $\gamma$.
However, this is very difficult to optimize not only because the constraint $\norm{\vw}=1$ is non-convex, also the optimization is not like any standard form.
By replacing $\vw$ with $\frac{\vw}{\gamma}$, we obtain 
\begin{align*}
	\underset{\vw,b}{\maximize} &\quad \frac{1}{\norm{\vw}}\\
	\st &\quad y_i(\ip{\vw}{\vx_i}+b) \ge 1, \, \forall i\in\{1,\cdots,m\},
\end{align*}
which is equivalent to optimize
\begin{definition}{Hard-Margin \svm\ Optimization Problem in Primal}\label{hardsvmprimal}
	\begin{align*}
		\underset{\vw}{\minimize} & \quad\frac{1}{2}\norm{\vw}^2\\
		\st & \quad y_i(\ip{\vw}{\vx_i} + b) \ge 1, \forall i\in\{1,\cdots,m\},
	\end{align*}
\end{definition}
where the objective is to find the weight vector of minimum norm that corresponds to maximize the margin between two class of examples.
The constraints states that the training data should be well separated.

% soft margin svm
We do not use (Definition~\ref{hardsvmprimal}) in practice for two reasons. 
First, many real world data are not separable, so that solution to (Definition~\ref{hardsvmprimal}) does not always exist.
Second, the data usually come with noise and error and we do not want resulting classifier over-fit the training data.
Therefore, we relax the constraints by introducing for each training example $x_i$ a \textit{margin slack} parameter $\xi_i$ and rewrite the constraints as 
\begin{align}
	y_i(\ip{\vw}{\vx_i} + b) \ge 1-\xi_i, \xi_i\ge 0, \forall i\in\{1,\cdots,m\}. \label{softsvmconstraint}
\end{align}
The margin slack parameter $\xi_i$ will allow example with margin less than $1$ (the violation of the constraints).
In particular, with $\xi_i=0$ the data point $x_i$ is correctly classified, and lies either on the margin or on the correct side.
With $0<\xi_i\le 1$ the data point is correctly classified and lies between margin and separating hyperplane.
With $\xi_i>1$ the data point is misclassified locating on the other side of the hyperplane.
With margin slack parameter, the new goal is to maximize the margin while penalizing the data points that either lies on the wrong side of the hyperplane or has margin less than one
\begin{definition}{Soft-Margin \svm\ Optimization Problem in primal}\label{softsvmprimal}
	\begin{align*}
		\underset{\vw,\xi}{\minimize} & \quad\frac{1}{2}\norm{\vw}^2 + C\sum_{i=1}^{m}\xi_i\\
		\st & \quad y_i(\ip{\vw}{\vx_i} + b) \ge 1-\xi_i, \xi_i\ge 0, \forall i\in\{1,\cdots,m\}.
	\end{align*}
\end{definition}

% dual, kernel svm
Optimization problem is usually transformed into dual form by introducing for each constraint a Lagrangian multiplier (dual variable) $\alpha$.
We obtain the following dual optimization problem
\begin{definition}{Soft-Margin \svm\ Optimization Problem in dual}\label{softsvmdual}
	\begin{align*}
		\underset{\valpha}{\maximize} & \quad\sum_{i=1}^{m}\alpha_i -\frac{1}{2}\sum_{i=1}^{m}\sum_{i=1}^{m}\alpha_i\alpha_jy_iy_j\ip{\vx_i}{\vx_j}\\
		\st & \quad \sum_{i=1}^{m}\alpha_iy_i=0, \, 0\le\alpha_i\le C, \, \forall i\in\{1,\cdots,m\}.
	\end{align*}
\end{definition}
It is not difficult to verify that according to KKT condition only examples with both $\xi_i=0$ and satisfying equality constraints in (\ref{softsvmconstraint}) will be ''active'' and have dual variable $\alpha_i>0$.
These training examples lie on the margin and have $\gamma_i=1$.
They are called \textit{support vectors} in \svm\ optimization problem.
In fact the number of support vectors is usually smaller than the size of the training set.
The property is extreme useful, as the weight vector can be expressed as the linear combination of the training examples
\begin{align*}
	\vw = \sum_{i=1}^{m}\alpha_iy_i\vx_i.
\end{align*}
As most of the dual variables are zero, the evaluation can be done efficiently by maintaining a small set of non-zero $\alpha$.

In addition, we notice that to work out (Definition~\ref{softsvmdual}) it is not necessary to work on the explicit representation of $\vx$, as only the value of the inner product $\ip{\vx_i}{\vx_j}$ is need.
As the algorithm can be represented entirely in terms of the inner product, we can also replace the inner product with $\ip{\phi(\vx_i)}{\phi(\vx_j)}$.
In particular, we defined the \textit{kernel}
\begin{align}
	K(\vx_i,\vx_j) = \ip{\phi(\vx_i)}{\phi(\vx_j)}, \label{kernel}
\end{align}
where $\phi(\cdot)$ is some high-dimensional feature map.
Whenever we need $\ip{\vx_i}{\vx_j}$, we instead use $K(\vx_i,\vx_j)$.
Besides, the property of kernel function \citep{Scholkopf02learning} allows us to compute (\ref{kernel}) without explicitly accessing the high dimensional feature map.
Thus the algorithm can tackle high dimensionality by learning from $\phi(\cdot)$ at a low computational cost.

In addition, according to \textit{Mercer's Theorem} \citep{taylor04}, every positive semidefinite symmetric function is a kernel.
Kernel functions that heavily used in practice include the \textit{linear kernel}
\begin{align*}
	K(\vx_i,\vx_j) = \ip{\vx_i}{\vx_j},
\end{align*}
the \textit{polynomial kernel}
\begin{align*}
	K(\vx_i,\vx_j) = (\ip{\vx_i}{\vx_j} + b)^k,
\end{align*}
where $b$ is the bias term and $d$ is the degree of polynomial, and the \textit{Gaussian kernel} (RBF)
\begin{align*}
	K(\vx_i,\vx_j) = \exp{\left(\frac{\norm{{\vx_i}-{\vx_j}}^2}{2\sigma^2}\right)},
\end{align*}
where $\sigma$ is the Gaussian width parameter.

% optimization of support vector machine,, original approach
The optimization techniques for solving \svm\ have been intensively studied.
A ''chunking'' method was developed in \citep{Vapnik82estimation} which takes the advantage that the \svm\ solution only depends on the nonzero Lagrangian multipliers.
The method breaks down the original optimization problem into a set of smaller problems, and solves in each iteration a small problem that contains the nonzero Lagrangian multipliers from last step and a fixed number of worst examples that violate the constraints.
Following the similar idea, \citet{Osuna97an} proposed a decomposition strategy to solve \svm\ on very large dataset.
The algorithm decomposes the dual variables into an active part (working set) and an inactive part.
In each iteration, the algorithm updates the variables in working set while keeps the rest unchanged by solving the smaller optimization problem arising from the working set.
It adding and deleting the same amount of examples in the working set to maintain a constant memory.
It gains advantage of linear memory requirement with respect to the number of support vectors and the size of the working set by compromising on the convergence time.
\citep{Joachims98making} has improved this decomposition strategy by applying a shrinking technique which will identify the dual variables that is less likely to change their values.
Thus, the algorithm effectively maintains a good working set that makes much progress towards the object.
The implementation of the algorithm is well-known as \svmlight.
The decomposition strategy was further developed by solving in each iteration a smallest as possible optimization that only contains two Lagrangian multipliers, which is known as \textit{Sequential Minimum Optimization} (SMO) \citep{Platt98sequential,Platt99fast}.
\smo\ is the basis of another powerful software package \libsvm\ developed by \citet{Chang11libsvm}.
\citet{Perezcruz04double} proposed a double-chunking methods to further speed up the decomposition type of training. 
Similarly yet another approach ''digesting'' \citep{Decoste02support} optimizes subsets close to completion before adding new data, which save huge amount of memory.

Another line of research for \svm\ optimization is to divide the original problem into subproblems of smaller size, optimize \svm\ on each subproblem, and combine the results.
In \citep{Tresp00a,Schwaighofer01the}, the authors proposed a Bayesian scheme to combine the models from subproblems.
\citet{Collobert02a} proposed a partition scheme.
\citet{Graf05parallel} developed the \svmcascade\ that builds a partition tree of training examples, trains \svm\ on each partition, and returns only support vector from each partition.
Recently, \citet{Hsieh13a} proposed a divide and conquer solver that solves the original \svm\ problem with theoretical guarantee.

Other \svm\ approximation approaches for \svm\ include representing the training data with a small set of landmark points \citep{Pavlov00towards,Boley04training,Yu05making,Zhang08improved}, greedy method for basis selection \citep{Keerthi06building}, online \svm\ solver \citep{Bordes05fast}, and kernel approximation methods \citep{Zhang12scaling,Le13fast}.



% dual

% kernel

\section{Structured Output Prediction}

%
% notations
\subsection{Preliminary and Notation}


%
% sub section: perceptron
\subsection{Maximum Entropy Markov Network (\memm)}
% TODO


%
% sub section: perceptron
\subsection{Perceptron for Structured Output}

The perceptron algorithm, dated back to \citep{Rosenblatt58}, is one of the oldest learning algorithm in machine learning.
As it is extremely easy to implement and usually achieves good performance, the perceptron algorithm is still actively studied in the field.
Structured perceptron, as suggested by its name, can be seen as the generalization of the perceptron algorithm to structured output space.
It was firstly proposed in \citep{collins02a, collins02b} with the formalism incredibly similar to multiclass perceptron. 
The model assumes a score function $\ip{\vw}{\vPhi(x,\vy)}$ as the inner product between a joint feature map $\vPhi(x,\vy)$ and some feature weight parameter $\vw$.
After weight parameter $\vw$ is obtained, one need to solve the \textit{argmax problem} to get the best output for a given input
\begin{align}
	\hat{\vy} = \underset{\vy\in\Ycal}{\argmax} &\quad \ip{\vw}{\vPhi(x_i,\vy)}.  \label{perceptroninference}
\end{align}

The weight parameter $\vw$ is learned via standard perceptron iterative update by solving argmax operation (\ref{perceptroninference}) in each iteration.
In particular, the algorithm loops through training examples and updates $\vw$ whenever the predicted label $\hat{\vy}$ is different from true label $\vy$, according to 
\begin{align*}
	\vw \leftarrow \vw + \left(\vPhi(x_i,\vy_i) - \vPhi(x_i,\hat{\vy}_i)\right).
\end{align*}
The update usually leads to over-fitting and a simple refinement, called "average parameter" similar to \citep{freund99}, is proposed as shown in (Algorithm~\ref{algorithm_structured_perceptron}).
\begin{algorithm}
\caption{Structured Perceptron with Parameter Averaging}
\label{algorithm_structured_perceptron}
\begin{algorithmic}[1]
	\REQUIRE Training sample $\lbrace (x_i,\vy_i)\rbrace_{i=1}^m$
	\ENSURE Weight parameter $\vw$
	\STATE $\vw^{t,i}=[0,\cdots,0],\,\forall t\in\{1,\cdots,T\},\,\forall i\in\{1,\cdots,m\}$
	\STATE $c = 1$
	\FOR{$t=1\cdots T$}
		\FOR{$i=1\cdots m$}
			\STATE $\hat{\vy} = \underset{\vy\in\Ycal}{\argmax} \quad \ip{\vw^{t-1,i-1}}{\vPhi(x_i,\vy)}$
			\IF{$\hat{\vy}\neq\vy$}
				\STATE $\vw^{t,i} = \vw^{t-1,i-1} + \vPhi(x_i,\vy_i) - \vPhi(x_i,\hat{\vy}_i)$
			\ENDIF
			\STATE $c=c+1$
		\ENDFOR
	\ENDFOR
	\RETURN $\vw = \frac{1}{Tm}\sum_{t=1}^{T}\sum_{i=1}^{m}\vw^{t,i}$
\end{algorithmic}
\end{algorithm}

In fact, structural perceptron makes two strong assumptions that limit its power and applicability.
First, the framework assumes 0/1 loss over output variables, that is $\vell(\vy,\hat{\vy})=\vone_{\ind{\vy\neq\hat{\vy}}}$, such that the nearly correct and the completely incorrect output labels will lead to the same update of the weight parameter during the training.
Secondly, it tacitly assumes that the argmax problem can be solved efficiently, which is generally not true on many structured output space.
In the original work \citep{collins02a}, the algorithm relied on Viterbi decoding, which works on sequence tagging problem.

% TODO: incremental perceptron training


%
% sub section: crf
\subsection{Conditional Random Field (\crf)}
Condition Random Field (\crf), pioneered in \citep{lafferty01} and later in \citep{taskar02}, is a discriminative framework that constructs a conditional model $P(\vy|\vx)$ from paired input variable $x\in X$ and output variables $\vy\in Y$.
It optimizes a log-loss which is analogue to 0/1 loss over the whole structured output space.
\begin{definition}{Conditional Random Field (\crf)}\\
	Denote by $Y=\{\vy_1,\cdots,\vy_m\}$ a set of output random variables, and by $X=\{x_1,\cdots,x_m\}$ a set of input random variables to condition on.
	Let $G=(E,V)$ be a graph such that $Y = (Y_v)_{v\in V}$.
	A Conditional Random Field (\crf) defines the conditional probability distribution
	\begin{align*}
		P(\vy|\vx) &= \frac{1}{Z_{x,\vw}}\exp{\ip{\vw}{\vPhi(x,\vy)}},
	\end{align*}
	where $Z_{x,\vw}$ is the partition function dependent on $\vx$ that sums over all possible output variables 
	\begin{align}
		Z_{x,\vw} &= \sum_{\vy'\in\Ycal}\exp{\ip{\vw}{\vPhi(x,\vy')}}. \label{crf_partition_function}
	\end{align}
	Thus, when conditioned on $\vx$, the random variables $y_v$ obey the Markov property with respect to graph $G$.
\end{definition}

The parameter $\vw$ is solved by introducing parameter prior and maximizing the log of the resulting MAP problem as described in \citep{taskar02}
\begin{align}
	L(\vw) = \sum_{i=1}^{m}\left[\ip{\vw}{\vPhi(x_i,\vy_i)}-\log{\sum_{\vy'\in\Ycal}{\exp{\ip{\vw}{\vPhi(x_i,\vy')}}}}\right] - \frac{1}{\sigma^2}\norm{\vw}^2 + C. \label{crf_inference}
\end{align}
The parameter learning problem, as (\ref{crf_inference}), is solved in \citep{lafferty01} via improved iterative scaling algorithm (\iis) from \citep{Pietra97i}.
In order to make \crf\ work in practice, one have to make sure that the log normalization function (\ref{crf_partition_function}) and the argmax inference can be solve efficiently.


 


%
% sub section: m3n
\subsection{Max-Margin Markov Network (\mmmn)}

The Max-Margin Markov Network (\mmmn) unifies the frameworks of kernel based discriminative learning and probabilistic probabilistic graphical model \citep{taskar04}.
The model defines a log-linear Markov network over a set of output variables, thus, it is capable of modelling the correlation between these output variables.
Recall that Support Vector Machine (\svm) searches for a feature weight vector of smaller L$_2$ norm, which achieves a margin of at least one on training examples for the purpose of good generalization power.
Following the same formalism, \mmmn\ also defines a margin-based quadratic programming optimization problem for the weight vector of the model.
In particular, it extends \svm\ to structured output space via a loss function $\ell$ that requires the score difference between true output label $\vy$ and any incorrect label $\hat{\vy}$ be at least $\vell(\vy,\hat{\vy})$.
The primal optimization problem of \mmmn\ is given as
\begin{definition}{\mmmn\ optimization problem in primal form}\label{def_mmmn}
	\begin{align*}
		\underset{\vw,\vxi}{\minimize} & \quad\frac{1}{2}\norm{\vw}^2 + C\sum_{i=1}^{m}\xi_i \\
		\st & \quad\ip{\vw}{\vPhi(x_i,\vy_i)} - \ip{\vw}{\vPhi(x_i,\vy)} \ge \vell(\vy_i,\vy)-\xi_i,\\
		& \quad \forall \xi_i\ge0, \forall \vy\in\Ycal/\vy_i, \forall i\in\{1,\cdots,m\},
	\end{align*}
\end{definition}
where $\xi_i$ denotes the slack allotted to each example to make sure solution can always be found, $\vell(\vy_i,\vy)$ is the loss function between pseudo-label and the correct label, and $C$ is the slack parameter that controls the amount of regularization in the model.
The intuition behind the optimization is to maximizing the margin between the correct output label $\vy$ and any incorrect label $\hat{\vy}$.
In particular, the margin is scaled by the loss function $\vell(\vy,\hat{\vy})$ such that the incorrect label with bigger loss will require larger margin. 

It is immediately clear that the primal optimization problem of \mmmn\ (Definition~\ref{def_mmmn}) is difficult to solve as there are exponential numbers of constraints, one being instantiated for each pair of training example and pseudo label $(x,\vy)$.
The corresponding dual form is also difficult as there are exponential number of dual variables \cite[p.~4]{taskar04}.
Fortunately, by exploring the Markov network structure defined on the output labels, the original problem can be formulated into factorized dual quadratic programming, as long as the loss function $\vell$ and joint feature map $\vPhi$ are decomposable over the Markov network.

As the number of parameters (weight vector) is quadratic in the number of training examples and edges in the network, it still cannot fit into standard QP solver. 
In the original work \citep{taskar04}, the authors developed a coordinate descent method analogous to the sequential minimal optimization (\smo) used for SVM \citep{Platt98sequential,Platt99fast}.
Subsequently, there are many efficient optimization algorithms being proposed, including exponential gradient optimization methods developed in \citep{bartlett04}, extra-gradient methods described in \citep{taskar06}, sub-gradient methods proposed in \citep{Ratliff07}, and conditional gradient methods in \citep{rousu06, rousu07}.

In order to apply the optimization methods in practice, one have to solve the \textit{loss augmented} inference problem defined as
\begin{align}
	\hat{\vy} = \underset{\vy\in\Ycal/\vy_i}{\argmax} &\quad \ip{\vw}{\vPhi(x_i,\vy)} + \vell(\vy_i,\vy) \label{mmmninference}.
\end{align} 
To efficiently compute (\ref{mmmninference}), the loss function need to be decomposable over the Markov network.
Nevertheless, \mmmn\ is more flexible compared to \crf\ due to the following two reasons.
First, \mmmn\ model does not require partition function to be calculated. 
Thus, it is more feasible in terms of computation and can be applied to problems that is \nphard\ for \crf.
Secondly, one can define more complex loss function on \mmmn\ other than just 0/1 loss for \crf, as long as the function is decomposable.





%
% sub section: svm struct
\subsection{Support Vector Machine for Interdependent and Structured Outputs (\svmstruct)}

The formulation of Support Vector Machine for interdependent and structured output space (\svmstruct) described in \citep{THJA04,TJTA05} is amazingly similar to \mmmn.
Compared to \mmmn\ framework which scales the margin by the loss function, \svmstruct\ formulation scales the margin errors ({\em slack variables}) by the loss function.
The primal optimization problem of \svmstruct\ is given as
\begin{definition}{\svmstruct\ optimization problem in primal form}\label{def_svmstruct}
	\begin{align*}
		\underset{\vw,\vxi}{\minimize} & \quad\frac{1}{2}\norm{\vw}^2 + \frac{C}{m}\sum_{i=1}^{m}\xi_i \\
		\st & \quad\ip{\vw}{\vPhi(x_i,\vy_i)} - \ip{\vw}{\vPhi(x_i,\vy)} \ge 1-\frac{\xi_i}{\vell(\vy_i,\vy)},\\
		& \quad \forall \xi_i\ge0, \forall \vy\in\Ycal/\vy_i, \forall i\in\{1,\cdots,m\},
	\end{align*}
\end{definition}
where $\xi_i$ denotes the slack allotted to each example, $\vell(\vy_i,\vy)$ is the loss function between pseudo-label and the correct label, and $C$ is the slack parameter that controls the amount of regularization in the model.
The primal form can be interpreted as maximizing the the margin between the correct training example and the pseudo-example.
Allowing margin error is to make sure solution can always be found.

It seems intuitive to scale the margin error by the loss function, the advantage of which is also claimed the authors in \citep[p.3]{THJA04}.
They suggest that under \mmmn\ framework the learning system will work hard on the examples that incur big loss thought they may not even close to be confusable compared to true target value $\vy_i$.

In addition to the slight difference in loss scaling, the optimization technique employed by \svmstruct\ differ significantly compared to \mmmn.
Since the loss function in \svmstruct\ is not decomposable, the model cannot remove exponential number of constraints during optimization.
On the other hand, \citet{THJA04} developed an iterative optimization approach that creates a nested sequence of successively tighter relaxation of the original problem via a cutting-plane methods \citep{Bishop07,JFY09}.
In particular, constraints are added as necessary.
It is also shown that the iterative optimization algorithm will converge to some optimal solution of $\epsilon$ precision in polynomial number of steps.

The major drawback of \svmstruct\ is the inference is usually intractable.
To find the most violating constraint, the model will compute the loss-augmented inference problem as in \citep{TJTA05}
\begin{align}
	\hat{\vy} = \underset{\vy\in\Ycal/\vy_i}{\argmax} &\quad [1-\ip{\vw}{\vPhi(x_i,\vy)}]\,\vell(\vy_i,\vy) \label{svmstructinference},
\end{align}
where the loss function appears as a multiplicative term.
As the model does not assume any property of decomposition on the loss function over output space, the loss-augmented inference problem (\ref{svmstructinference}) of \svmstruct\ is in practice intractable. 
The intractability of the inference problem can be seen as an exchange of the generality of the loss function which allows more complicated loss to be defined. 



%
% ensemble methdos
\section{Ensemble Methods}

Ensemble methods are one of the prominent classification techniques widely used in Machine Learning.
In general, the methods train multiple base learners and combine them in order to achiever better classification performance.
Most importantly, there is no requirement for base learners to be accuracy as long as they perform better than random guessing.
%To trace the exact originality of the ensemble methods seems impossible, as the methods involve along well with human society by which it is wiser to take in to account advises from multiple human experts.
There are several different types of ensemble algorithm, to name a few, bagging \citep{Breiman96bagging}, boosting \citep{Freund97a,Schapire99improved}, stacking \citep{Smyth99linearly}, Bayesian averaging \citep{Freund04generalization}.
Most of the algorithms have improved the classification performance when compared to individual counterpart, some of them also enjoyed theoretical guarantee \citep{Schapire97boosting,Koltchinskii00empirical,Cortes14deep,Cortes14semble}.
For our purposes, the section will start with and focus on bagging and boosting ensemble since both are best known approaches and most related to this thesis.
In the later part of the section, we will extend the ensemble methods to structured output space by reviewing several recently established algorithms.

%
% preliminary notations
\subsection{Preliminary and Notations}



%
% boosting
\subsection{Boosting}
\textit{Boosting} corresponds to a learning framework or a family of algorithms that take in a weak classifier and turning it into a strong one.
We begin our discussion from the notion of  \textit{concept class}.
A \textit{concept} is a boolean function over domain $\Xcal$, and a \textit{concept class} is a class of concepts.
% TODO: an illustrative example
A concept class is \textit{strongly learnable} if there exists a polynomial learning algorithm that achieves high accuracy with high probability for all concepts in the class.
On the other hand, a concept class is \textit{weak learnable} if the learning algorithm achieves arbitrary high accuracy where the only requirement is that the learning algorithm finds a function that performs better than coin flip.
The concept of learnability were proposed in \citep{Kearns94cryptographic} together with the question whether the strong learnability and the weak learnability are equivalent which is known as \textit{hypothesis boosting problem}.
Finding a weak learner which performs better than random guessing is easy in practice, but finding a strong learner is usually difficult.
\citet{Schapire90the} has proved that two classes of learnability are equal which lays the foundation of the boosting algorithm that aim to convert a weak learning algorithm into a strong one that achieves high performance.

Adaptive Boosting (\adaboost) developed by \citet{Freund97a} is the very first practical boosting algorithm and is the most influential one.
In addition, \citet{Schapire99improved} proposed a variant of the algorithm which updates the adaptive parameters in order to minimize the exponential loss of the weak learners.
The exact algorithm is shown in (Algorithm~\ref{adaboost}).
The fundamental idea of \adaboost\ is to maintain a distribution $D$ over all training examples, and update the distribution in each iteration such that difficult-to-classify examples will get more probability mass for the next iteration (line~\ref{update_d}).
In particular, the algorithm computes in each iteration a weak learner $f^t(\vx)$ base on all training examples and the current distribution $D^{t}$ (line~\ref{compuet_f}), calculates the weighted training error $\epsilon^k$ for the current weak learner (line~\ref{compute_error}), and computes the adaptive parameter $\alpha^t$ for the current weak learner (line~\ref{compute_alpha}).
The ensemble is the weighted combination of all weak learners.
\begin{align*}
	H(\vx) = \sign(\sum_{t=1}^T\alpha^tf^t(\vx)).
\end{align*}
\begin{algorithm}
\caption{\adaboost}
\label{adaboost}
\begin{algorithmic}[1]
	\REQUIRE Training sample $\Scal = \{(x_i,\vy_i)\}_{i=1}^m$, learning function $\Wcal$, number of weak learners $T$
	\ENSURE Boosting ensemble $H(\vx)$
	\STATE Initialize $D^0(i)=\frac{1}{m},i\in\{1,\cdots,m\}$
	\FOR{$t=1\cdots T$}
		\STATE $f^t(\vx)\leftarrow\Wcal(\Scal,D^{i-1})$ \label{compuet_f}
		\STATE $y_i = f^t(\vx_i),\forall i\in\{1,\cdots,m\}$
		\STATE $\epsilon^t = \sum_{i=1}^{m}D^t(i)\ind{y_i\neq\hat{y}_i}$ \label{compute_error}
		\STATE $\alpha^{t} = \frac{1}{2}\ln\left(\frac{1-\epsilon^t}{\epsilon^t}\right)$ \label{compute_alpha}
		\STATE $Z=\sum_{i=1}^{m}D^{t-1}(i)\exp(-\alpha^ty_i\hat{y}_i)$
		\STATE $D^t(i) = \frac{1}{Z}D^{t-1}(i)\exp(-\alpha^ty_i\hat{y}_i),\forall i\in\{1,\cdots,m\}$ \label{update_d}
	\ENDFOR
	\RETURN $H(\vx) = \sign(\sum_{t=1}^T\alpha^tf^t(\vx))$
\end{algorithmic}
\end{algorithm}

For each weak learner $f^t(\vx)$, the strategy of updating adaptive parameter $\alpha^t$ is to ensure that the exponential loss of $\alpha^tf^t(\vx)$ is minimized.
To see this, we first compute the exponential loss for $\alpha^tf^t(\vx)$ given current distribution $D^t$ over training example and adaptive parameter for $f^t(\vx)$
\begin{align*}
	\ell_{exp}&(\alpha^t,f^t(\vx),D^t) \\
	&= \sum_{i=1}^mD^t(i)\exp(-\hatf(\vx_i)\alpha^tf^t(\vx_i))\\
	&=\exp(-\alpha^t)\sum_{i=1}^mD^t(i)\ind{\hatf(\vx_i)=f(\vx_i)} + \exp(\alpha^t)\sum_{i=1}^mD^t(i)\ind{\hatf(\vx_i)\neq f(\vx_i)}\\
	&=\exp(-\alpha^t)(1-\epsilon^t) + \exp(\alpha^t)\epsilon^t.
\end{align*}
To minimize $\ell_{exp}(\alpha^t,f^t(\vx),D^t)$, we take partial derivative with respect to $\alpha^t$ and set it to zero
\begin{align*}
	\frac{\partial\ell_{\exp}(\alpha^t,f^t(\vx),D^t)}{\partial\alpha^t} = -\exp(-\alpha^t)(1-\epsilon^t) + \exp(\alpha^t)\epsilon^t=0
\end{align*}
Solve it for $\alpha^t$, we get
\begin{align*}
	\alpha^{t} = \frac{1}{2}\ln\left(\frac{1-\epsilon^t}{\epsilon^t}\right).
\end{align*}
Thus the rule for updating $\alpha^t$ will make sure the exponential loss of $\alpha^tf^t(\vx)$ will be minimized.

It is worth noticing that the \adaboost\ algorithm describe in (algorithm~\ref{adaboost}) requires the learning algorithm $\Wcal$ capable of learning from some specific distribution.
The distribution will reflect the difficulty of the training examples during the learning.
It is usually achieved by \textit{reweighing} which initials a uniform distribution over training examples and updates the distribution after each iteration.
For learning algorithms that cannot handle training data with specific distribution, we can apply \textit{resampling} which samples training examples in each iteration according to some desired distribution.

Most recently, a new boosting algorithm \deepboosting\ was developed in \citep{Cortes14deep}.
\deepboosting\ allows the learning algorithm to use complex hypothesis class contain very deep decision trees, by which it extends \adaboost\ that only uses simple hypothesis class of decision tree with depth one.
In addition, \citet{Cortes14deep} also developed rich learning bounds for \deepboosting\ optimization problem in terms of the Rademacher complexity theory.
The new theory advances the previous performance guarantee of \adaboost\ which is build in terms of margins of the training examples \citep{Schapire97boosting,Koltchinskii00empirical}.


% TODO: adaboost loss bound



%
% bagging
\subsection{Bootstrap Aggregating}
\textit{Bootstrap Aggregating} (\bagging), initially developed by \citet{Breiman96bagging}, is an ensemble methods that exploit the independency between the weak learners.
The algorithm is based on the fact that the error can be dramatically re duced by combining independent base learners.
Denote by $f_i$ the $i$'th weak learner.
The ensemble prediction $H(\vx)$ is the averaged prediction over all weak learners
\begin{align}
	H(\vx) = \sign\left(\sum_{i=1}^{T}f_i(\vx)\right). \label{bagging_ensemble}
\end{align}
Denote by $\hatf$ the ground true function.
Suppose base learner has probability $\epsilon$ of making independent mistake
\begin{align*}
	P(f_i(\vx) \ne \hatf(\vx)) = \epsilon.
\end{align*}
As the bagging ensemble (\ref{bagging_ensemble}) makes a mistake when at least half of the weak learners make mistakes, we can compute the probability of bagging ensemble making mistake by
\begin{align*}
	P(H(\vx)\neq\hatf(\vx)) = \sum_{i=0}^{T/2}\left(\begin{tabular}{cc}T\\k\end{tabular}\right)(1-\epsilon)^k\epsilon^{T-k} \le\exp\left(-\frac{1}{2}T(2\epsilon-1)^2\right).
\end{align*}
It is clear that the probability decreases exponentially with respect to the number of the weak learners.
In extreme case, the probability will approach zero when $T$ approaches infinity.
Though it is impractical as the base learners are hardly independent since they are generated from the same training data, we still try to exploit the independency by adding randomness in the algorithm framework.

\bagging\ uses bootstrap sampling \citep{Efron1994introduction} to generate subsets of training examples.
Given a training set of $m$ training examples, a subset of same size is generated by sampling with replacement $m$ time from original training set.
The sampling procedure is then repeated $T$ time in order to generate $T$ subsets for constructing base learners.
Note that the sampled subsets will be similar as they are sampled from the same training set.
However, they will not be too similar which each subset will only cover around $63\%$ of the origin training data under the condition that $m$ is large.
To see this, consider the probability that the $i$'th training examples is not sampled once is $(1-\frac{1}{m})$, and the probability that the training example is not sampled at all is $(1-\frac{1}{m})^m$.
When $m$ is large, the probability will approach $37\%$. 
That is around $37\%$ of the training examples will not be represented in any given subset.
The property of the bootstrap sampling allows us to efficiently estimate the generalization error of the base learner known as \textit{out-of-bag estimation} \citep{Breiman96out,Tibshirani1996bias,Wolpert99an}.

Random Forest developed in \citep{Breiman01random} is an important extension of \bagging.
The major different from \bagging\ is that randomized feature selection is incorporated in random forest.
In practice, the random forest algorithm selects in each step a subset of features and constructs conventional random tree classifier within the selected subset of features.
The feature split process of decision tree is deterministic given a set of features.
\citet{Liu08spectrum} proposed \vrtree\ which further extend random forest algorithm by introducing randomness in both feature selection and split processes.


\subsection{Ensemble Methods in Structured Output Space}

The ensemble methods have been initially developed for single task classification \citep{Breiman96bagging,Freund97a} or regression tasks \citep{Breiman96bagging}, as it is straightforward to combine multiple scalar output variables.
However, it is not immediately clear how to combine vector valued outputs in multilabel structured output space.

\adaboostmh\ is the multilabel variance of original \adaboost\ algorithm proposed by \citet{Schapire99improved}, which is further developed by \citet{Esuli2008boosting}.
The core idea of \adaboostmh\ is to adopt hamming loss instead of 0/1 loss. 
In particular, the algorithm reduces from a multilabel classification problem to a set of single label classification problems by replacing each training example $(\vx_i,\vy_i)$ with $k$ examples $\{(\vx_i,\vy_i[l])\}_{l=1}^k$.
The algorithm is detailed in (Algorithm~\ref{adaboostmh}).
In particular, it maintains a distribution over all examples and labels.
In each iteration, the algorithm takes in the distribution and all training examples, generates a weak learner $f^k(\vx)$ (line~\ref{mh_compuet_f}), computes hamming loss (line~\ref{mh_compute_error}), computes the adaptive parameter $\alpha^t$ (line~\ref{mh_compute_alpha}), and update the distribution (line~\ref{mh_update_d}).
The returned prediction $H(x)$ is the combination of base learners $f^t(x)$ weighted by $\alpha^t$.
\begin{algorithm}
\caption{\adaboostmh}
\label{adaboostmh}
\begin{algorithmic}[1]
	\REQUIRE Training sample $\Scal = \{(x_i,\vy_i)\}_{i=1}^m$, learning function $\Wcal$, number of weak learners $T$
	\ENSURE Boosting ensemble $H(\vx)$
	\STATE Initialize $D^0(i,l)=\frac{1}{mk},i\in\{1,\cdots,m\}$
	\FOR{$t=1\cdots T$}
		\STATE $f^t(\vx)\leftarrow\Wcal(\Scal,D^{i-1})$ \label{mh_compuet_f}
		\STATE $\vy_i = f^t(\vx_i),\forall i\in\{1,\cdots,m\}$
		\STATE $\epsilon^t =\sum_{l=1}^k\sum_{i=1}^{m}D^t(i,l)\ind{\vy_i[l]\neq\hat{\vy}_i[l]}$ \label{mh_compute_error}
		\STATE $\alpha^{t} = \frac{1}{2}\ln\left(\frac{1-\epsilon^t}{\epsilon^t}\right)$ \label{mh_compute_alpha}
		\STATE $Z=\sum_{i=1}^{m}\sum_{l=1}^{k}D^{t-1}(i,l)\exp(-\alpha^t\vy_i[l]\hat{\vy}_i[l])$
		\STATE $D^t(i,l) = \frac{1}{Z}D^{t-1}(i,l)\exp(-\alpha^t\vy_i[l]\hat{\vy}_i[l]),\forall i,\forall l$ \label{mh_update_d}
	\ENDFOR
	\RETURN $H(\vx) = \sign(\sum_{t=1}^T\alpha^tf^t(\vx))$
\end{algorithmic}
\end{algorithm}

Furthermore, many other ensemble approaches have been proposed and applied in specific areas, including, in natural language processing applications \citep{Collins05distrimnative,Zeman05improving,Sagae06parsing,Zhang09kbest} and 
in text and speech recognition applications \citep{Fiscus97a,Benesty08speech,Petrov10products}.
Limiting the applicability of these methods is the fact that they are mostly developed from the ensemble approaches for single task problem, and are specially tailored for the applications.
It is not immediately clear how to apply to other machine learning tasks.
In addition, other methods that are based on boosting or bagging are also developed in \citep{Wang07simple,Kocev13tree}.
Most recently, a series of learning algorithms with performance guarantee are proposed in \citep{Cortes14semble}.




\section{Evaluation}

\chapter{Methods}


\chapter{Predicting Influence Network}
\section{Problem Definition}
\section{Prior Work}
\section{Methods}



\chapter{Theory}


\chapter{Applications}


\chapter{Future Work}



%% Examples of article references, remove these from your manuscript!
% Uncomment them, if you want to see the results of these commands in this example document
% Refer to the Journal paper 1 of this example document
%\citepub{su14b} \& \cpub{su14b} \& \cp{su14b} \& \pageref{su14b} \& \ref{su14b}
% Refer to the Conference paper of this example document
%\citepub[p.~2]{su14a} \& \cpub[Sec.~ 1]{su14a} \&  \cp[pp.~1--2]{su14a} \& \pageref{su14a} \& \ref{su14a} 


%\section{Section Heading}
%\lipsum[5]

\bibliographystyle{apalike} 
\bibliography{dissertation}
