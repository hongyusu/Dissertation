





\chapter{Introduction}\label{ch_introduction}

\section{Scope of the Thesis}

In this thesis, we study the \textit{supervised learning} in structured output space, also known as \textit{structured output prediction}.

\textit{Machine learning}, an active research field in information and computer science, has drawn enormous attentions during the last few decades, not only because it develops intelligent systems that generalize well from previous observed examples; but also because it is grounded from rich statistical learning theories that prove the feasibility and guarantee the performance.
Machine learning has in practice provided lots of inventions, which people may use many times a day without knowing it, to name but a few, from web searching engine to self-driving car, from optical image recognition to email spam filter, from automatic stock exchange to online recommender system.

\textit{Supervised learning}, one of the most important fields in {machine learning}, is usually instantiated as learning a function that is capable of predicting the best value for the output variable $\vy$ given an observed input variable $\vx$.
The function is learned by utilizing a set of input and output pairs known as training examples.
In the classical supervised learning setting, the value that the output variable $\vy$ can take is often no more than one discrete or continuous number.
This is often referred as single-task classification in machine learning community.
Many excellent learning algorithms have been designed for the problem and achieved remarkable success in many applications.

However, many real world problems require multiple interdependent output variables defined in structured output space to be predicted at the same time, known as \textit{multi-task structured output prediction}.
Otherwise, the problem can be summarized as learning a function for mapping from arbitrary input to arbitrary output.
In this case, single task classification approaches render inefficiency and incapability of modeling the interdependency.
In addition, the interdependency can be given either explicitly as the output graph that connecting multiple output variables (e.g. sequence parsing, network influence prediction, hierarchical document classification), or implicitly with the only assumption that the output variable are correlated (e.g. molecular classification, computer vision).
Nevertheless, it is crucial to capture the common properties shared between similar output variables in order to achieve good performance and to generalize well.

Most existing methods for structured output prediction do not tackle the situation when the output graph structure is not observed.
Especially, the learning models built on Markov network over multiple output variables will break down without the observed output structure.
On the other hand, methods that do not explicitly assume the output structure usually have less power of modeling the interdependency and often struggle in optimization due to the exponential sized searching space.

In this thesis, we investigate the potentials of developing novel structured output prediction models to improve the prediction performance in real world multi-task classification problems (e.g. molecular activity prediction, network influence prediction).
In particular, we present the novel structured output prediction models that are capable of learning without predefined structure.
We focus on the efficiency and the scalability of the proposed structured output prediction models, as the inference problem is often \nphard\ to solve.
Additionally, the new learning models are well motivated.
The theoretical study is able to explain the behaviour and to guarantee the performance of the models.
In particular, our research questions can be casted as followings:
\begin{enumerate}[label=\textbf{Q \Roman*}:]
\item Is the structured output prediction models more suitable for predicting multiple interdependent variables in structured output space?
\item How to tackle the problem of structured output learning without observed output structure?
\item What is the motivation of the proposed structured output learning framework? Can we explain the improvement and guarantee the empirical risk of the proposed models?
\item Can we efficiently optimize the proposed structured output prediction models?
\end{enumerate}





\section{Contribution and Outline of the Thesis}

The main contribution of the thesis is to present a learning framework for structured output prediction with both empirical and theoretical guarantee.
Under the framework, several learning algorithms are proposed.
Unlike previous structured output learning algorithms, the proposed models do not assume any observed structured on the output variables, and thus largely widen the applicability of the structured output learning.
Meanwhile, the thesis presents several attractive theoretical properties which support the new learning framework and guarantee the generalization error.
In addition to the main contribution, the thesis also proposes a new learning algorithm that is capable of predicting a directed acyclic graph and demonstrating the performance on influence network prediction problem.
Besides, the thesis also focuses on elegant optimization strategy with which the newly proposed models often avoid the exponential parameter space and \nphard\ inference problem.

The thesis is structured as follows.
In Chapter~\ref{ch_introduction}, the scope of the research is briefly explained with several research questions being proposed which will be answered in this thesis.
Chapter~\ref{ch_background} provides the introduction to the problem settings, learning algorithms, and statistical models that are necessary to understand the rest part of the thesis.
The chapter starts by introducing in Section~\ref{sc_single} several single task classification algorithms, including 
perceptron (Section~\ref{sc_perceptron}), 
logistic regression (Section~\ref{sc_lr}), 
and \svm\ (Section~\ref{sc_svm}).
Section~\ref{sc_multi} extends the previous section by introducing the learning algorithms designed for multiple interdependent variables in structured output space.
Ensemble models for both single task and multi-task classification will be briefly covered in Section~\ref{sc_ensemble}.
Section~\ref{sc_statistics} describes the performance measures and the statistical models that are used for model evaluation and comparison.

Chapter~\ref{ch_models} introduces the novel statistical models developed by the author.
In particular, Section~\ref{sc_su10} explains the structured output learning model for molecular activity prediction.
Section~\ref{sc_su11} and Section~\ref{sc_su14a} discuss three ensemble models (\mve, \amm, \mam) that tackle the problem in structured output prediction when the output structure is unknown.
Section~\ref{sc_su14c} presents \rta\ model that is a major step forward of \mam\ by performing joint learning and inference over a random sample of spanning trees.
Section~\ref{sc_su14b} introduces network influence prediction problem and presents \spin\ that is capable of predicting a directed acyclic graph (DAG).
Finally, Chapter~\ref{ch_conclusion} concludes the thesis by answering the research questions and by pointing out future directions.

In this thesis, the ideas and the formalisms of the proposed statistical models for structured output learning are explained.
However, the technical details from the original publications are usually not repeated.
Furthermore, the notations and the presentations of some proposed models are slightly improved to be better incorporated into a unified framework.
The background information and the related work of some proposed models are sometimes elaborated in the thesis to help better understand their contributions to the community.
Empirical evaluations of the proposed models are in nowhere restated, rather, they can be found from the original publications. 





\chapter{Background}\label{ch_background}


\section{Regularized Empirical Risk Minimization}

In this section, the author will introduce two fundamental concepts in statistical machine learning, known as empirical risk minimization and regularized learning.
The previous model and newly proposed algorithms are all built based on the concept.

\subsection{Empirical Risk Minimization}

Without lose of generality, we assume two random variable $\vx\in\vXcal$ and $\vy\in\vYcal$ are related according to some unknown probability distribution $P(\vx,\vy)$.
In addition, we are provided with examples in pair $(\vx,\vy)\in\vXcal\times\vYcal$ generated by sampling according to the distribution $P(\vx,\vy)$.
The problem for \textit{statistical learning} is to provide an \textit{estimator} $f:\vXcal\rightarrow\vYcal$ that predict the value of $\vy$ given an arbitrary input $\vx$.

We define a risk function (loss function) $\ell(\vy,f(\vx))$ between the true output $\vy$ and the predicted output $f(\vx)$.
The average error, known as \textit{true risk}, made by $f$ over domain $\vXcal\times\vYcal$ can be computed from
\begin{align}
	\Rcal(f) = \int_{\vXcal,\vYcal}\ell(\vy,f(\vx))P(\vx,\vy)d_{\vx}d_{\vy} \label{true_risk}.
\end{align}
As a results, we should search for an estimator $f$ that minimizes the true risk (\ref{true_risk}).
In practice, the distribution that generate the examples is often known.
Instead, we are given a random sample of $m$ examples, denoted by $S=\{(\vx_1,\vy_1),\cdots,(\vx_m,\vy_m)\}$.
$S$ is often called \textit{training data}.
We define the \textit{empirical risk} as the average error made on the training data of limit size
\begin{align}
	\Rcal_{emp}(f) = \frac{1}{m}\sum_{i=1}^{m}\ell(\vy_i,f(\vx_i)) \label{true_risk},
\end{align}
and search for an estimator $f$ to minimize the (\ref{true_risk}).
The strategy is known in machine learning community as \textit{empirical risk minimization}.

\subsection{Regularized Learning}

The empirical risk minimization strategy is ill-posed which will generates infinity number of solutions, all of which have the same empirical error.
Besides, it naturally leads to \textit{overfitting} as the underlying true distribution $P(\vx,\vy)$ is difficult to be estimate from a finite sample of training examples.
In other words, the test examples can be generated from a different distribution from the one estimated based on the training examples.
The \textit{regularization theory} provides a framework for the two problems which aims to minimize 
 \begin{align}
	\Jcal(f) = \frac{1}{m}\sum_{i=1}^{m}\ell(\vy_i,f(\vx_i)) + \Omega(f),
\end{align}
where $\Omega(f)$ is the regularization function that controls the complexity of the estimator $f$.
In terms of the linear function class, there are varying way to defined $\Omega(f)$, of which two regularization terms are most popular, namely $L_1$ norm and $L_2$ norm regularization.
The $L_2$ norm regularization controls the complex of $f$ and provides a smooth solution.
The $L_1$ norm regularization will provides sparse estimation which means that we obtain  a high dimensional feature weight vector with many $0$ entries.
This is a attractive property as feature selection is incorporated in the learning and the interpretability of the resulting model is usually hight.
Besides, other regularization functions are also studies, to name a few, $L_{1,2}$ norm \citep{Argyriou07multitask} and elastic net regularization \citep{}.

%
\section{Single Label Classification}\label{sc_single}

In this section, the author will introduce the basic classification problem known as single task classification.
The author will explain three prominent algorithms in the area: Perceptron, Logistic Regression and the Support Vector Machines.
The optimization techniques and the latest advances of these algorithms will also be briefly discussed.
The goal is to cover the background that is necessary to understand the algorithms presented in the later part of the dissertation. 

%
%
\subsection{Preliminary and Notation}\label{singlelabel_preliminary}
We consider supervised learning problem which assumes an arbitrary input space $\vXcal$, an output space $\Ycal$, and the training samples coming in pairs $(\vx_i,y_i)\in\vXcal\times\Ycal$.
As we focus in this chapter standard supervised learning problem also known as binary classification, we explicitly assume the output space $\Ycal=\{-1,+1\}$ (instead of $\Ycal=\{0,+1\}$).
We point out that other kind of supervised learning problem can be formulated by altering the definition of the output space.
For example, by setting $\Ycal=\{1,\cdots,K\}$ we have multiclass classification problem, and by setting $\Ycal=\RR$ we have regression problem, where $\RR$ is the set of real numbers.
Additionally, we assume a feature map $\phib:\vXcal\rightarrow\Fcal$, which embeds the input in some high dimensional feature space $\Fcal=\RR^d$.
In particular, $\phib(\vx)$ is a real value vector of $d$ dimension.
Given a training set of $m$ training examples $\Scal=\{(\vx_1,y_1),\cdots,(\vx_m,y_m)\}$, the goal is to learn from a \textit{hypothesis class} $\Hcal$ a mapping function $f\in\Hcal:\vXcal\rightarrow\Ycal$  that maps an input $\vx\in\vXcal$ to an output $y\in\Ycal$. 

The hypothesis class we consider is a set of \textit{linear classifiers} that are parameterized by the weight vector $\vw$ and the bias term $b$ that takes the form
\begin{align}
	f(x;\vw,b) = \ip{\vw}{\vx} + b, \label{linearclassifier}
\end{align}
where by $\ip{\dot\,}{\dot\,}$ we denote the inner product of two vectors
\begin{align*}
	\ip{\vw}{\vx} = \sum_{i=1}^{d}\vw[i]\,\vx[i].
\end{align*}

In addition, for any $1\le\rho\in\RR$ we defined the {$L_{\rho}$-norm of vector $\vw$ 
\begin{align*}
	\norm{\vw}_{\rho} = \left(\sum_{i=1}^{d}|\vw[i]|^{\rho}\right)^{\frac{1}{\rho}}.
\end{align*}
Without loss of generality, we denote by $||\vw||$ the $L_2$-norm of vector $\vw$ in the following sections.


%
%
\subsection{Perceptron}\label{sc_perceptron}

The Rosenblatt's Perceptron algorithm \citep{Rosenblatt58,Rosenblatt62} is one type of linear classifiers that is parameterized by a weight vector $\vw$ and a bias term $b$. 
The decision boundary given by 
\begin{align*}
	f(x;\vw,b) = \ip{\vw}{\vx} + b =0
\end{align*}
will separate the data into two classes.
The goal of learning is to obtain $\vw$ and $b$ by minimizing the distance of misclassified training examples to the decision boundary.

Assume data point $(\vx_i,y_i)$ is misclassified, the signed geometric distance from $\vx_i$ to decision boundary can be computed according to
\begin{align*}
	D(\vx_i;\vw,b) = -\frac{y_i(\ip{\vw}{\vx_i}+b)}{\norm{\vw}}.
\end{align*}
Denote by $\vXcal^m$ the set of misclassified examples, the goal of perceptron is to minimize the distance over all misclassified examples defined as
\begin{definition}{Perceptron Objective Function.}\label{perceptron}
	\begin{align*}
		\underset{\vw,b}{\minimize} \, D(\vw,b) = \underset{\vw,b}{\minimize} \left( -\sum_{\vx_i\in\vXcal^{m}}y_i[\ip{\vw}{\vx_i}+b]\right).
	\end{align*}
\end{definition}
To optimize (Definition~\ref{perceptron}), we first assume $\vXcal^{m}$ is fixed and compute the partial gradient with respect to parameters $\vw$ and $b$
\begin{align*}
	\frac{\partial D(\vw,b)}{\partial\vw} = -\sum_{\vx_i\in\vXcal^m}{y_i\vx_i},\quad
	\frac{\partial D(\vw,b)}{\partial b} = -\sum_{\vx_i\in\vXcal^m}{y_i}.
\end{align*}
In practice, the perceptron algorithm uses stochastic gradient descent that processes iteratively from training data one example at a time.
On each step, the algorithm makes sure the current $\vw$ and $b$ will correctly separate the current training example. 
Once a misclassified example is visited, it adjusts the parameter according to the update rule
\begin{align*}
	\vw &\leftarrow \vw + \rho y_i \vx_i,\\
	b &\leftarrow b + \rho y_i,
\end{align*}
where $\rho$ is the perceptron learning rate.
\begin{theory}[Perceptron Convergence Theorem \citep{Block62the,Novikoff62}]\label{perceptron_theory}
	Given a sequence of training examples in pairs $\Scal=\{(\vx_1,y_1),\cdots,(\vx_m,y_m)\}$. 
	For simplicity, we consider a perceptron without bias term $b$.
	Assume $\norm{\vx_i}\le R$ for all $i\in\{1,\cdots,m\}$.
	Suppose for some $\gamma>0$ there exists a unit length vector $\norm{\hat{\vw}}=1$ such that $y_i(\ip{\hat{\vw}}{\vx_i})\ge\gamma$ for $i$ (training data $\Scal$ is linearly separable).
	Then the number of mistakes the perceptron algorithm makes on the sequence of training data $\Scal$ is at most ${R^2}/{\gamma^2}$.
\end{theory}
\begin{proof}
	Suppose the $k$'th mistake is made on the $i$'th training example, and current weight vector is $\vw^k$.
	In addition, we set $\vw^0=\vzero$.
	As the Perceptron makes a mistake on $(\vx_i,y_i)$, we immediately have
	\begin{align*}
		y_i\ip{\vw_k}{\vx_i} \le 0.
	\end{align*}
	According to update rule, we have
	\begin{align*}
		\vw^{k+1} = \vw^{k} + \rho y_i\vx_i.
	\end{align*}
	Then, the following holds
	\begin{align*}
		\ip{\vw^{k+1}}{\hat{\vw}} = \ip{\vw^k}{\hat{\vw}} + \rho y_i\ip{\vx_i}{\hat{\vw}} \ge \ip{\vw^k}{\hat{\vw}} + \rho\gamma.
	\end{align*}
	Straight forward induction gives us
	\begin{align}
		\ip{\vw^{k+1}}{\hat{\vw}}\ge k\rho\gamma. \label{induction1}
	\end{align}
	We also have
	\begin{align*}
		\norm{\vw^{k+1}}^2 
		= \norm{\vw^{k} + \rho y_ix_i}^2
		= \norm{\vw^k}^2 + \rho^2\norm{\vx_i}^2 + 2\rho y_i\ip{\vx_i}{\vw^k}
		\le \norm{\vw^k}^2 + \rho^2R^2
	\end{align*}
	Straight forward induction gives us
	\begin{align}
		\norm{\vw^{k+1}}^2\le k\rho^2R^2. \label{induction2}
	\end{align}
	Together with (\ref{induction1}) and (\ref{induction2}) we have
	\begin{align*}
		k\le{R^2}/{\gamma^2}.
	\end{align*}
\end{proof}
Theory~\ref{perceptron_theory} shows that if the data are linearly separable, the perceptron algorithm will make a finite number of mistakes. 
In particular, the perceptron algorithm will iterate through the training set and will converge to a vector that well separates all training examples.
Moreover, the number of mistakes will be bounded by the minimum gap between the positive and negative examples.
That is the algorithm converges quickly when the gap $\gamma$ is big.
We have to keep in mind that the convergence theorem does not say anything about the quality of the solution, though it is built based on the assumption that there is a ``gap'' between positive and negative examples.

The standard Perceptron algorithm supposes that the data are linearly separable, iterates over the training set, and eventually converges.
However, the setting is less interesting for most applications where the data might not be linearly separable or it takes too long for the algorithm to converge.
Therefore, we need to find out the best decision rule given a set of updates.

\textit{Voted Perceptron} developed in \citep{Freund99large} is a straight forward modification of the standard perceptron.
As described in (Algorithm~\ref{voted_perceptron}), the voted perceptron algorithm keeps track all weight vectors that appears during training phase (line~\ref{perceptron_algorithm_weight_vector}) and their weights (line~\ref{perceptron_algorithm_weight}).
The weight can be seen as the ``survival time'' of the weight vector during Perceptron training.
The prediction for a new test example $\vx_{ts}$ is computed by 
\begin{align}
	y_{ts} = \sign\left({\sum_{l=1}^{k}c^l(\sign{\ip{\vw^l}{\vx_{ts}}+b^{l}}})\right) \label{voted_perceptron_prediction}
\end{align}
using the weight vectors and the corresponding weights output from the algorithm (line~\ref{perceptron_algorithm_average}).
\begin{algorithm}
\caption{Voted Perceptron Learning Algorithm}
\label{voted_perceptron}
\begin{algorithmic}[1]
	\REQUIRE Training sample $\lbrace (x_i,\vy_i)\rbrace_{i=1}^m$, iteration limit $T$
	\ENSURE Weight parameter $\vw$
	\STATE $\vw^{1}=\vzero,c^1=0$
	\STATE $k=1$
	\FOR{$t=1\cdots T$}
		\FOR{$i=1\cdots m$}
			\STATE Compute $\hat{y} = \sign(\ip{\vw}{\vx_i})$
			\IF{$\hat{y} = y_i$}
				\STATE $c^k = c^k + 1$
			\ELSE
				\STATE $\vw^{k+1} = \vw^{k} + \rho y_i\vx_i$ \label{perceptron_algorithm_weight_vector}
				\STATE $c^{k+1} =1$ \label{perceptron_algorithm_weight}
				\STATE $k=k+1$
			\ENDIF
		\ENDFOR
	\ENDFOR
	\RETURN $\vw = \{\vw^{k}\}_{l=1}^{k},\,c = \{c^{k}\}_{l=1}^{k}$ \label{perceptron_algorithm_average}
\end{algorithmic}
\end{algorithm}

% TODO: vote perceptron error bound, add reference 
It is proved that the voted perceptron guarantees to achieve better generalization error than original perceptron \citep{Rosenblatt58,Rosenblatt62}.
However, voted perceptron is not practical as it has to store all weight vectors encountered during the training.
The \textit{averaged perceptron} is a simple alternative, where in stead of storing all weight vectors the algorithm only keeps the weighted sum $\sum_{l=1}^{k}c^k\vw^l$. 
The prediction for a new test example $\vx_{ts}$ is computed by 
\begin{align*}
	y_{ts} = \sign\left(c^l({\sum_{l=1}^{k}{\ip{\vw^l}{\vx_{ts}}+b^{l}}})\right) = \sign\left({{\ip{c^l\sum_{l=1}^{k}\vw^l}{\vx_{ts}}+c^l\sum_{l=1}^{k}b^{l}}}\right),
\end{align*}
which is extremely similar to (\ref{voted_perceptron_prediction}) except for the inside $\sign$ operation, and is more efficient to compute.

%
% logit
\subsection{Logistic Regression (\lr)}\label{sc_lr}

% general introduction
Logistic regression is another important method in statistical machine learning and is closely related to the Perceptron (Section~\ref{sc_perceptron}) and the Support Vector Machines (Section~\ref{sc_svm}).
It is in fact a classification model rather than regression \citep{Bishop07}.
It models the conditional probability $p(y=+1|\vx)$ for a binary output variable $y\in\Ycal$.
%Without loss of generality, in the rest part of this section we use $p(\vx)$ to denote above conditional probability of $p(y|\vx)$ when fixing $y=1$.
To model the probability, we do not restrict to any particular form, as any unknown parameters can be estimated by \textit{maximum likelihood estimation} (MLE).
However, we are most interested in the simple linear model as described in (\ref{linearclassifier}).

% detailed model, decision boundary
To use linear model in Logistic Regression, the first choice is to let $p(y=+1|\vx)$ be a linear function of $\vx$, while the problem is the linear function is unbounded but the probability $p(\vx)\in[0,1]$.
Another choice is to let $\log p(y=+1|\vx)$ be a linear function of $\vx$. 
However, the problem is the log-likelihood ranges from zero to infinite but the linear function is unbounded.
The choice in Logistic Regression is to use logistic transformation of the original probability function
\begin{align*}
	\log\frac{p(y=+1|\vx)}{1-p(y=+1|\vx)} = \ip{\vw}{\vx} + b,
\end{align*}
which by solving for $p(y=+1|\vx)$ results in 
\begin{align}
	p(y=+1|\vx;\vw,b) = \frac{1}{1+\exponential{-\ip{\vw}{\vx}-b}}. \label{lr_1}
\end{align}
We can also compute
\begin{align}
	p(y=-1|\vx;\vw,b)=1-p(y=+1|\vx;\vw,b)=\frac{1}{1+\exponential{\ip{\vw}{\vx}+b}}. \label{lr_2}
\end{align}
Putting together (\ref{lr_1}) and (\ref{lr_2}), we define the Logistic Regression as
\begin{definition}{Logistic Regression.}\label{logistic_regression}
	\begin{align*}
		p(y=\pm1|\vx;\vw,b) = \frac{1}{1+\exponential{-y(\ip{\vw}{\vx}-b)}}.
	\end{align*}
\end{definition}
Naturally, we expect the prediction $y=+1$ when $p(y=+1|\vx;\vw,b)\ge0.5$ and $y=-1$, when $p(y=+1|\vx;\vw,b)<0.5$.
The decision rule is similar to perceptron where we predict $y=+1$ when $\ip{\vw}{x}+b\ge0$, and $y=-1$ otherwise.
Besides decision boundary, Logistic Regression can output the class probability from (Definition~\ref{logistic_regression}) as the the ``distance'' of the data point to the decision boundary.
It is the probabilistic output that makes Logistic Regression no more than a classifier, as it outputs detailed predictions (class probability).

% optimization
As the model is capable of outputting class probability not just class category, to learn a Logistic Regression model we attempt to learn parameters $\vw$ and $b$ by maximizing the probability (likelihood) of the training data.
In particular, the probability of training data $\vx$ with class label $y$ is $p(y|\vx)$.
The likelihood of parameters given data can be computed from
\begin{align}
	L(\vw,b;D) = \prod_{i=1}^mp(y_i|\vx_i). \label{lr_likelihood}
\end{align}
To apply MLE, it is easier if, instead of maximizing the likelihood, we maximize the log-likelihood, which turns the product (\ref{lr_likelihood}) into sum
\begin{align}
	\log L(\vw,b|D) = \sum_{i=1}^{m}\log p(y_i|\vx_i) = -\sum_{i=1}^{m}\log (1+\exponential{-y_i(\ip{\vx_i}{\vw}+b)}) \label{lr_likelihood_sum}.
\end{align}

% logistic regression of other type
Though MLE training for Logistic Regression model meets our need of fitting training data, it does not guarantee to generalize well on test data.
To achieve better generalization power, various smoothing techniques have been proposed \citep{Chen99,Chen00,Goodman03}, among which adding Gaussian prior on weight parameter $\vw$ is one of the standard treatment.
In practice, a zero-mean spherical Gaussian with variance $\sigma^2$ is assumed on $\vw$.
Thus, the maximum likelihood problem (\ref{lr_likelihood}) is transformed into \textit{Maximum A-Posteriori} (MAP) problem of the following form
\begin{align}
	p(\vw,b|D;\sigma^2) = p(\vw|\sigma^2)\prod_{i=1}^mp(y_i|\vx_i)=\exponential{-\frac{\norm{\vw}^2}{\sigma^2}}\prod_{i=1}^{m}{\frac{1}{1+\exponential{-y_i(\ip{\vx}{\vw}+b)}}}. \label{lr_posteriori}
\end{align}
Instead of maximizing the posteriori (\ref{lr_posteriori}), it is easier to maximize the log-posteriori
\begin{align}
	\log p(\vw,b|D;\sigma^2) = -{\frac{\norm{\vw}^2}{\sigma^2}} - \sum_{i=1}^{m}\log{(1+\exponential{-y_i(\ip{\vx}{\vw}+b)})}. \label{lr_posteriori_log}
\end{align}

Numbers of optimization techniques have been proposed to solve the maximization problem (\ref{lr_posteriori_log}) in Logistic Regression, many of which are covered in the survey \citep{Minka03}.
For example, \textit{Itrative Scaling} methods were proposed and continuously developed in \citep{Darroch72,Pietra97inducing,Berger97,Goodman02Sequential,Jin03a}.
A quasi-Newton method was discussed in \citep{Minka03}.
\citet{Komarek05making,Lin2008trust} have proposed truncated Newton method.
And coordinate descent method was proposed in \citep{Huang09iterative}.

% dual for and optimization
There are also efforts being made to solve the logistic regression from its dual, of which the first algorithm was proposed in \citep{Jaakkola99probabilistic}.
The key is to introduce a set of tighter upper bounds on (\ref{lr_posteriori_log}) that are parameterized by $\valpha$.
The bounds should have a simple form such that the maximizing (\ref{lr_posteriori_log}) over $\vw$ can be solved analytically with $\alpha$.
The solution to the original problem is transferred as finding the tighter upper bounds for  $\vw$, which is to minimize with respect to $\alpha$.
\iffalse
The dual form is given by 
\begin{align*}
	\underset{\valpha}{\minimize} & \quad\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\valpha_iy_ix_ix_jy_j\valpha_j + \sum_{i=1}^{m}\left[\valpha_i\log\alpha_i+(\sigma^2-\valpha_i)\log(\sigma^2-\valpha_i)\right] \\
	\st &\quad 0\le\alpha\le\sigma^2, \, \forall i=\{1,\cdots,m\}.
\end{align*}
\fi
Later on, optimization algorithms that are based on dual form have been developed, for example iterative optimization method \citep{Keerthi05a} and the dual coordinate descent method \citep{Yu11dual}.

% TODO: add detail deviation of dual form of logistic regression
% TODO: explain the loss and gain of different optimzation algorithm


%
% svm
\subsection{Support Vector Machines (\svm)}\label{sc_svm}
\label{sc_svm}

% some history
Support Vector Machines (\svm) is probably the most widely used classification algorithm in various real world applications.
It is also among the best plug-and-play machine learning algorithms that usually achieves good classification performance.
In this section we will begin the story of \svm\ by first introducing the maximum margin principle of separating training data.
Then we will discuss the primal-dual optimization strategy and the kernel methods that allows \svm\ to deal with high dimensionality.
In the end we will try to cover the optimization strategy developed for \svm.

The algorithm framework of \svm\ was firstly introduced in \citep{Boser92,Cortes95support}.
Theory and algorithm are detailed in book chapters e.g. \citep{Scholkopf02learning,taylor04,Bishop07}.
% hard margin svm
We begin our discussion by considering a very simple case where we assume the data are linearly separable, that is there is a hyperplane in the feature space which separates the training data into two classes.
In addition we assume the \textit{separating hyperplane} has a simple form
\begin{align*}
	f(\vx)=\ip{\vw}{\vx} + b = 0,
\end{align*}
such that for training data we have $y_i=-1$ if $f(\vx_i)<0$, and $y_i=+1$ if $f(\vx_i)>0$.
Given $\vw$ that achieves correct separation on the training data, we can decide the label of an arbitrary test example $x_{ts}$ by the decision rule $y_{ts}=\sign(f(\vx_{ts}))$.

However, there can be infinite number of separating hyperplane that solves the separation problem on the training data.
We wish to find the one that also generalize well on the test data.
A good strategy is to look for a hyperplane that keeps the maximum distance from examples of two classes.
The strategy is often known as \textit{maximum-margin principal}.
To see this, imagining putting a separating hyperplane close to one class of examples which will achieve better classification on test examples from the other class.

Based on the maximum-margin principal, we further denote by $\gamma_i$ the \textit{margin} for the $i$'th example which is defined as the geometric distance from the data point to the separating hyperplane
\begin{align*}
	\gamma_i = \frac{y_i(\ip{\vw}{\vx_i}+b)}{\norm{\vw}}.
\end{align*}
We notice if we scale $\vw$ and $b$ with any constant (e.g. $\vw\leftarrow\kappa\vw, b\leftarrow\kappa b, \kappa\in\RR$, ) the margin $\gamma_i$ stays unchanged. 
We still achieve the same classification and generalization performance.
As parameters are invariance to scaling, we set $\norm{\vw}=1$.
Given a set of training example $D$, we also define the margin with respect to $D$ as the minimum margin of individual training example
\begin{align*}
	\gamma = \underset{i\in\{1,\cdots,m\}}{\minimize}\gamma_i.
\end{align*}

Our goal is to find the separating hyperplane such that it maximizes the margin with respect to all training data while separating the training examples into two classes.
Naturally, this corresponds to finding a ``big-gap'' between two classes.
The max-margin solution is given as \citep{Bishop07}
\begin{align*}
	\underset{\vw,b,\gamma}{\maximize} &\quad \gamma\\
	\st &\quad y_i(\ip{\vw}{\vx_i}+b) \ge \gamma, \, \norm{\vw}=1,\\
	 &\quad \forall i\in\{1,\cdots,m\}.
\end{align*}
The objective maximizes the minimum margin over all training examples such that all examples are correctly separated with margin at least $\gamma$.
However, this is very difficult to optimize not only because the constraint $\norm{\vw}=1$ is non-convex, also the optimization is not like any standard form.
By replacing $\vw$ with $\frac{\vw}{\gamma}$, we obtain 
\begin{align*}
	\underset{\vw,b}{\maximize} &\quad \frac{1}{\norm{\vw}}\\
	\st &\quad y_i(\ip{\vw}{\vx_i}+b) \ge 1, \, \forall i\in\{1,\cdots,m\},
\end{align*}
which is equivalent to optimize
\begin{definition}{Primal Hard-Margin \svm\ Optimization Problem.}\label{hardsvmprimal}
	\begin{align*}
		\underset{\vw}{\minimize} & \quad\frac{1}{2}\norm{\vw}^2\\
		\st & \quad y_i(\ip{\vw}{\vx_i} + b) \ge 1, \forall i\in\{1,\cdots,m\},
	\end{align*}
\end{definition}
where the objective is to find the weight vector of minimum norm that corresponds to maximize the margin between two class of examples.
The constraints states that the training data should be well separated.

% soft margin svm
We do not use (Definition~\ref{hardsvmprimal}) in practice for two reasons. 
First, many real world data are not separable, so that solution to (Definition~\ref{hardsvmprimal}) does not always exist.
Second, the data usually come with noise and error and we do not want resulting classifier over-fit the training data.
Therefore, we relax the constraints by introducing for each training example $x_i$ a \textit{margin slack} parameter $\xi_i$ and rewrite the constraints as 
\begin{align}
	y_i(\ip{\vw}{\vx_i} + b) \ge 1-\xi_i, \xi_i\ge 0, \forall i\in\{1,\cdots,m\}. \label{softsvmconstraint}
\end{align}
The margin slack parameter $\xi_i$ will allow example with margin less than $1$ (the violation of the constraints).
In particular, with $\xi_i=0$ the data point $x_i$ is correctly classified, and lies either on the margin or on the correct side.
With $0<\xi_i\le 1$ the data point is correctly classified and lies between margin and separating hyperplane.
With $\xi_i>1$ the data point is misclassified locating on the other side of the hyperplane.
With margin slack parameter, the new goal is to maximize the margin while penalizing the data points that either lies on the wrong side of the hyperplane or has margin less than one as defined by
\begin{definition}{Primal Soft-Margin \svm\ Optimization Problem.}\label{softsvmprimal}
	\begin{align*}
		\underset{\vw,\xi}{\minimize} & \quad\frac{1}{2}\norm{\vw}^2 + C\sum_{i=1}^{m}\xi_i\\
		\st & \quad y_i(\ip{\vw}{\vx_i} + b) \ge 1-\xi_i, \xi_i\ge 0, \forall i\in\{1,\cdots,m\}.
	\end{align*}
\end{definition}

% dual, kernel svm
Optimization problem is usually transformed into dual form by introducing for each constraint a Lagrangian multiplier (dual variable) $\alpha$.
We obtain the following dual optimization problem
\begin{definition}{Dual Soft-Margin \svm\ Optimization Problem.}\label{softsvmdual}
	\begin{align*}
		\underset{\valpha}{\maximize} & \quad\sum_{i=1}^{m}\alpha_i -\frac{1}{2}\sum_{i=1}^{m}\sum_{i=1}^{m}\alpha_i\alpha_jy_iy_j\ip{\vx_i}{\vx_j}\\
		\st & \quad \sum_{i=1}^{m}\alpha_iy_i=0, \, 0\le\alpha_i\le C, \, \forall i\in\{1,\cdots,m\}.
	\end{align*}
\end{definition}
It is not difficult to verify that according to KKT condition only examples with both $\xi_i=0$ and satisfying equality constraints in (\ref{softsvmconstraint}) will be ``active'' and have dual variable $\alpha_i>0$.
These training examples lie on the margin and have $\gamma_i=1$.
They are called \textit{support vectors} in \svm\ optimization problem.
In fact the number of support vectors is usually smaller than the size of the training set.
The property is extremely useful, as the weight vector can be expressed as the linear combination of the training examples
\begin{align*}
	\vw = \sum_{i=1}^{m}\alpha_iy_i\vx_i.
\end{align*}
As most of the dual variables are zero, the evaluation can be done efficiently by maintaining a small set of non-zero $\alpha$.

In addition, we notice that to work out (Definition~\ref{softsvmdual}) it is not necessary to work on the explicit representation of $\vx$, as only the value of the inner product $\ip{\vx_i}{\vx_j}$ is needed.
As the algorithm can be represented entirely in terms of the inner product, we can also replace the inner product with $\ip{\phib(\vx_i)}{\phib(\vx_j)}$.
In particular, we define the \textit{kernel}
\begin{align}
	K(\vx_i,\vx_j) = \ip{\phib(\vx_i)}{\phib(\vx_j)}, \label{kernel}
\end{align}
where $\phib(\cdot)$ is some high-dimensional feature map.
Whenever we need $\ip{\vx_i}{\vx_j}$, we instead use $K(\vx_i,\vx_j)$.
Besides, the property of kernel function \citep{Scholkopf02learning} allows us to compute (\ref{kernel}) without explicitly accessing the high dimensional feature map.
Thus the algorithm can tackle high dimensionality by learning from $\phib(\cdot)$ at a low computational cost.

In addition, according to \textit{Mercer's Theorem} \citep{taylor04}, every positive semidefinite symmetric function is a kernel.
Kernel functions that heavily used in practice include the \textit{linear kernel}
\begin{align*}
	K(\vx_i,\vx_j) = \ip{\vx_i}{\vx_j},
\end{align*}
the \textit{polynomial kernel}
\begin{align*}
	K(\vx_i,\vx_j) = (\ip{\vx_i}{\vx_j} + b)^k,
\end{align*}
where $b$ is the bias term and $d$ is the degree of polynomial, and the \textit{Gaussian kernel} (RBF)
\begin{align*}
	K(\vx_i,\vx_j) = \exp{\left(\frac{\norm{{\vx_i}-{\vx_j}}^2}{2\sigma^2}\right)},
\end{align*}
where $\sigma$ is the Gaussian width parameter.

% optimization of support vector machine,, original approach
The optimization techniques for solving \svm\ have been intensively studied.
A ``chunking'' method was developed in \citep{Vapnik82estimation} which takes the advantage that the \svm\ solution only depends on the nonzero Lagrangian multipliers.
The method breaks down the original optimization problem into a set of smaller problems, and solves in each iteration a small problem that contains the nonzero Lagrangian multipliers from last step and a fixed number of worst examples that violate the constraints.
Following the similar idea, \citet{Osuna97an} proposed a decomposition strategy to solve \svm\ on very large dataset.
The algorithm decomposes the dual variables into an active part (working set) and an inactive part.
In each iteration, the algorithm updates the variables in working set while keeps the rest unchanged by solving the smaller optimization problem arising from the working set.
It adding and deleting the same amount of examples in the working set to maintain a constant memory.
It gains advantage of linear memory requirement with respect to the number of support vectors and the size of the working set by compromising on the convergence time.
\citep{Joachims98making} has improved this decomposition strategy by applying a shrinking technique which will identify the dual variables that is less likely to change their values.
Thus, the algorithm effectively maintains a good working set that makes much progress towards the object.
The implementation of the algorithm is well-known as \svmlight.
The decomposition strategy was further developed by solving in each iteration a smallest as possible optimization that only contains two Lagrangian multipliers, which is known as \textit{Sequential Minimum Optimization} (SMO) \citep{Platt98sequential,Platt99fast}.
\smo\ is the basis of another powerful software package \libsvm\ developed by \citet{Chang11libsvm}.
\citet{Perezcruz04double} proposed a double-chunking method to further speed up the decomposition type of training. 
Similarly yet another approach ``digesting'' \citep{Decoste02support} optimizes subsets close to completion before adding new data, which saves huge amount of memory.

Another line of research to speed up \svm\ training is to divide the original problem into subproblems of smaller size, optimize \svm\ on each subproblem, and combine the results from each subproblem.
In \citep{Tresp00a,Schwaighofer01the}, the authors proposed a Bayesian scheme to combine the models from subproblems.
\citet{Collobert02a} proposed a partition scheme.
\citet{Graf05parallel} developed the \svmcascade\ that builds a partition tree of training examples, trains \svm\ on each partition, and returns only support vector from each partition.
Quite recently, \citet{Hsieh14a} proposed a divide and conquer solver that solves the original \svm\ problem with theoretical guarantee.

There are many other active researches on \svm\ optimization that aim to make kernel support vector machine scalable on very large scale dataset in terms of both computational time and memory space, to name but a few, representing the training data with a small set of landmark points \citep{Pavlov00towards,Boley04training,Yu05making,Zhang08improved}, greedy method for basis selection \citep{Keerthi06building}, online \svm\ solver \citep{Bordes05fast}, approximating kernel \svm\ objective function \citep{Zhang12scaling, Le13fast} which avoids high computational and memory cost but struggles in prediction performance, and approximating kernel matrix with low-rank matrix \citep{Smola00sparse,Fine02efficient,Drineas05on,Si14memory}.




%
%
% multilabel classification problem
\section{Structured Output Prediction}\label{sc_multi}

As the learning algorithms for multi-task classification in structured output space have evolved rapidly during the last decade, the author wish to briefly review in this section several prominent structured output learning algorithms, namely,  Structured Perceptron, Conditional Random Field, Maximum-Margin Markov Networks, Structured SVM, and Mutltask Feature Learning.
These algorithms are also relevant to this thesis.
These algorithms are mostly from 2010, the period when this field of researches has begun to emerge.


%
% notations
\subsection{Preliminary and Notation}\label{multilabel_preliminary}

We will keep most notations from (Section~\ref{singlelabel_preliminary}).
In particular, we examine the following multilabel classification setting.
We assume data from a domain $\vXcal\times\vYcal$, where $\vXcal$ is a set and $\vYcal$ is the set of multilabels.
$\vYcal = \Ycal_1\times\cdots\times\Ycal_k$ is composed by a Cartesian product of $k$ sets $\Ycal_i=\{-1,+1\}$.
We retain a single-label classification setting by letting $k=1$.
A vector $\vy=(y_1,\cdots,y_k)\in\vYcal$ is called \textit{multilabel} and each element $y_j$ is called \textit{microlabel}.
In addition, we are given a set of labeled training examples $\Scal=\{(\vx_i,\vy_i)\}_{i=1}^{m}\in\vXcal\times\vYcal$. 
A pair $(\vx_i,\vy)$, where $\vx_i$ is the training input and $\vy\in\vYcal$ is an arbitrary output, is call \textit{pseudo-example}.
The goal is to learn a mapping $f\in\Hcal:\vXcal\rightarrow\vYcal$ from input to output such that the loss $\ell$ for the future unseen examples is minimized, where the loss function is usually tailored for the structured output learning algorithm.

Multilabel classification deals with multiple interdependent output variables (e.g. $\vy\in\vYcal$), which locates in structured output space.
As graph is the natural way of modeling the multiple interdependencies, we assume a graph $G=(E,V)$ with node set $V=\{1,\cdots,k\}$ corresponding to the microlabels $\{y_1,\cdots,y_k\}$ and edge set $E=V\times V$ representing the dependency between pair of microlabels.
For each edge $e=(j,j')\in E$, we denote by $\vy_e$ the edge label of edge $\ve$ with respect to multilabel $\vy$ by concatenating the head label $y_i$ and the tail label $y_i$, with the edge label domain $\vy_e\in\vYcal_e=\Ycal_i\times\Ycal_{j'}$.
We denote by $\vy_{i,e}$ the edge label of example pair $(\vx_i,\vy_i)$ on edge $\ve$.
Thus, given a pair of training example $(\vx_i,\vy_i)$ and the output graph $G$, we can uniquely identify the node label $y_i$ and the edge label $\vy_{i,e}$.
In addition, we denote by $u_i$ the possible label of node $i$ and by $\vu_{e}$ the possible label of edge $\ve$ where $u_i$ and $\vu_{e}$ do not constraint to any multilabel $\vy$.
Naturally, $u_i\in\Ycal_i$ and $\vu_e\in\vYcal_e$.


%
% sub section: perceptron
%\subsection{Maximum Entropy Markov Network (\memm)}
% TODO


%
% sub section: perceptron
\subsection{Perceptron for Structured Output}

The perceptron algorithm, dated back to \citep{Rosenblatt58}, is one of the oldest learning algorithm in machine learning.
As it is extremely easy to implement and usually achieves good performance, the perceptron algorithm is still actively studied in the field.
Structured perceptron, as suggested by its name, can be seen as the generalization of the perceptron algorithm to structured output space.
It was firstly proposed in \citep{collins02a, collins02b} with the formalism incredibly similar to multiclass perceptron. 
The model assumes a score function $\ip{\vw}{\phib(\vx,\vy)}$ as the inner product between a joint feature map $\phib(\vx,\vy)$ and some feature weight parameter $\vw$.
After weight parameter $\vw$ is obtained, one need to solve the \textit{argmax} problem to get the best output for a given input
\begin{align}
	\hat{\vy} = \underset{\vy\in\vYcal}{\argmax} &\, \ip{\vw}{\phib(\vx_i,\vy)}.  \label{perceptroninference}
\end{align}

The weight parameter $\vw$ is learned via standard perceptron iterative update by solving argmax operation (\ref{perceptroninference}) in each iteration.
In particular, the algorithm loops through training examples and updates $\vw$ whenever the predicted label $\hat{\vy}$ is different from true label $\vy$, according to 
\begin{align*}
	\vw \leftarrow \vw + \left(\phib(\vx_i,\vy_i) - \phib(\vx_i,\hat{\vy}_i)\right).
\end{align*}
The update usually leads to over-fitting and a simple refinement, called "average parameter" similar to \citep{Freund99large}, is shown in (Algorithm~\ref{algorithm_structured_perceptron}).
\begin{algorithm}
\caption{Structured Perceptron with Parameter Averaging}
\label{algorithm_structured_perceptron}
\begin{algorithmic}[1]
	\REQUIRE Training sample $\lbrace (\vx_i,\vy_i)\rbrace_{i=1}^m$
	\ENSURE Weight parameter $\vw$
	\STATE $\vw^{t,i}=[0,\cdots,0],\,\forall t\in\{1,\cdots,T\},\,\forall i\in\{1,\cdots,m\}$
	%\STATE $c = 1$
	\FOR{$t=1\cdots T$}
		\FOR{$i=1\cdots m$}
			\STATE $\hat{\vy} = \underset{\vy\in\vYcal}{\argmax} \quad \ip{\vw^{t-1,i-1}}{\phib(\vx_i,\vy)}$
			\IF{$\hat{\vy}\neq\vy$}
				\STATE $\vw^{t,i} = \vw^{t-1,i-1} + \phib(\vx_i,\vy_i) - \phib(\vx_i,\hat{\vy}_i)$
			\ENDIF
			%\STATE $c=c+1$
		\ENDFOR
	\ENDFOR
	\RETURN $\vw = \frac{1}{Tm}\sum_{t=1}^{T}\sum_{i=1}^{m}\vw^{t,i}$
\end{algorithmic}
\end{algorithm}

In fact, structured perceptron makes two strong assumptions that restrict its power and applicability.
First, the framework assumes 0/1 loss over output variables, that is $\vell(\vy,\hat{\vy})=\ind{\vy\neq\hat{\vy}}$, such that the nearly correct and the completely incorrect output labels will lead to the same update of the weight parameter during the training.
Secondly, it tacitly assumes that the argmax problem can be solved efficiently, which is generally not true in many structured output space.
In the original work, \citet{collins02a} only demonstrated the algorithm on sequence tagging problem with Viterbi decoding.

% TODO: incremental perceptron training


%
% sub section: crf
\subsection{Conditional Random Field (\crf)}
Condition Random Field (\crf), pioneered in \citep{lafferty01} and later in \citep{taskar02}, is a discriminative framework that constructs a conditional model $P(\vy|\vx)$ from paired input variable $\vx\in\vXcal$ and output variables $\vy\in\vYcal$.
It optimizes a log-loss which is analogue to 0/1 loss over the whole structured output space.
\begin{definition}{Conditional Random Field (\crf).}\\
	Denote by $Y=\{\vy_1,\cdots,\vy_m\}$ a set of output random variables, and by $X=\{\vx_1,\cdots,\vx_m\}$ a set of input random variables to condition on.
	Let $G=(E,V)$ be a graph such that $\vy = (y_v)_{v\in V}$.
	A Conditional Random Field (\crf) defines the conditional probability distribution
	\begin{align*}
		P(\vy|\vx) &= \frac{1}{Z_{x,\vw}}\exp{\ip{\vw}{\phib(x,\vy)}},
	\end{align*}
	where $Z_{x,\vw}$ is the partition function dependent on $\vx$ that sums over all possible output variables 
	\begin{align}
		Z_{x,\vw} &= \sum_{\vy'\in\Ycal}\exp{\ip{\vw}{\phib(x,\vy')}}. \label{crf_partition_function}
	\end{align}
	Thus, when conditioned on $\vx$, the random variables $y_v$ obey the Markov property with respect to graph $G$.
\end{definition}

The parameter $\vw$ is solved by introducing parameter prior and maximizing the log of the resulting \textit{maximum a posteriori} (MAP) problem as described in \citep{taskar02}
\begin{align}
	L(\vw) = \sum_{i=1}^{m}\left[\ip{\vw}{\phib(\vx_i,\vy_i)}-\log{\sum_{\vy'\in\vYcal}{\exp{\ip{\vw}{\phib(\vx_i,\vy')}}}}\right] - \frac{1}{\sigma^2}\norm{\vw}^2 + C. \label{crf_inference}
\end{align}
The parameter learning problem, as (\ref{crf_inference}), is solved in \citep{lafferty01} via improved iterative scaling algorithm (\iis) \citep{Pietra97inducing}.
In order to make \crf\ work in practice, one has to make sure that the log normalization function (\ref{crf_partition_function}) and the argmax inference can be solve efficiently.


 


%
% sub section: m3n
\subsection{Max-Margin Markov Network (\mmmn)}

The Max-Margin Markov Network (\mmmn) unifies the frameworks of kernel based discriminative learning and probabilistic graphical model \citep{Taskar04max}.
The model defines a log-linear Markov network over a set of output variables, thus, it is capable of modeling the correlation between these output variables.
Recall that Support Vector Machine (\svm) searches for a feature weight vector of smaller $L_2$ norm, which achieves a margin of at least one on training examples for the purpose of good generalization power.
Following the same principal, \mmmn\ defines a margin-based quadratic programming optimization problem for the weight vector of the model.
In particular, it extends \svm\ to structured output space via a loss function $\ell$ that requires the score difference between true output label $\vy_i$ and any incorrect label ${\vy}$ be at least $\vell(\vy_i,{\vy})$.
The primal optimization problem of \mmmn\ is given as
\begin{definition}{\mmmn\ Optimization Problem in Primal.}\label{def_mmmn}
	\begin{align*}
		\underset{\vw,\vxi_i}{\minimize} & \quad\frac{1}{2}\norm{\vw}^2 + C\sum_{i=1}^{m}\xi_i \\
		\st & \quad\ip{\vw}{\phib(\vx_i,\vy_i)} - \ip{\vw}{\phib(\vx_i,\vy)} \ge \vell(\vy_i,\vy)-\xi_i,\\
		& \quad \forall \xi_i\ge0,\, \forall \vy\in\vYcal/\vy_i,\, \forall i\in\{1,\cdots,m\},
	\end{align*}
\end{definition}
where $\xi_i$ denotes the slack allotted to each example to make sure solution can always be found, $\vell(\vy_i,\vy)$ is the loss function between pseudo-label and the correct label, and $C$ is the slack parameter that controls the amount of regularization in the model.
The intuition behind the optimization is to maximizing the margin from each training example between the correct output label $\vy_i$ and any incorrect label ${\vy}$.
In particular, the margin is scaled by the loss function $\vell(\vy_i,{\vy})$ such that the incorrect label with bigger loss will require larger margin. 

It is immediately apparent that the primal optimization problem of \mmmn\ (Definition~\ref{def_mmmn}) is difficult to solve as there are exponential numbers of constraints, one being instantiated for each pair of training example and pseudo label $(x,\vy)$.
The corresponding dual form is also difficult as there are exponential number of dual variables \cite[p.~4]{Taskar04max}.
Fortunately, by exploring the Markov network structure defined on the output labels, the original problem can be formulated into factorized dual quadratic programming, as long as the loss function $\vell$ and joint feature map $\phib$ are decomposable over the Markov network.

As the number of parameters (weight vector) is quadratic in the number of training examples and edges in the network, it still cannot fit into standard QP solver. 
In the original work \citep{Taskar04max}, the authors developed a coordinate descent method analogous to the sequential minimal optimization (\smo) used for SVM \citep{Platt98sequential,Platt99fast}.
Subsequently, there are many efficient optimization algorithms being proposed, including exponential gradient optimization methods developed in \citep{bartlett04}, extra-gradient methods described in \citep{taskar06}, sub-gradient methods proposed in \citep{Ratliff07}, and conditional gradient methods in \citep{rousu06, rousu07}.

In order to apply the optimization methods in practice, one have to solve the \textit{loss augmented} inference problem defined as
\begin{align}
	\hat{\vy} = \underset{\vy\in\vYcal/\vy_i}{\argmax} &\quad \ip{\vw}{\phib(\vx_i,\vy)} + \vell(\vy_i,\vy) \label{mmmninference}.
\end{align} 
To efficiently compute (\ref{mmmninference}), the loss function need to be decomposable over the Markov network.
Nevertheless, \mmmn\ is more flexible compared to \crf\ due to the following two reasons.
First, \mmmn\ model does not require partition function to be calculated. 
Thus, it is more feasible in terms of computation and can be applied to problems that is \nphard\ for \crf.
Secondly, one can define more complex loss function on \mmmn\ other than just 0/1 loss for \crf, as long as the function is decomposable.





%
% sub section: svm struct
\subsection{Support Vector Machine for Interdependent and Structured Outputs (\svmstruct)}

The formulation of Support Vector Machine for interdependent and structured output space (\svmstruct) described in \citep{THJA04,TJTA05} is amazingly similar to \mmmn.
Compared to \mmmn\ framework which scales the margin by the loss function, \svmstruct\ formulation scales the margin errors ({\em slack variables}) by the loss function.
The primal optimization problem of \svmstruct\ is given as
\begin{definition}{\svmstruct\ Optimization Problem in Primal.}\label{def_svmstruct}
	\begin{align*}
		\underset{\vw,\xi_i}{\minimize} & \quad\frac{1}{2}\norm{\vw}^2 + \frac{C}{m}\sum_{i=1}^{m}\xi_i \\
		\st & \quad\ip{\vw}{\phib(x_i,\vy_i)} - \ip{\vw}{\phib(x_i,\vy)} \ge 1-\frac{\xi_i}{\vell(\vy_i,\vy)},\\
		& \quad \forall \xi_i\ge0,\, \forall \vy\in\vYcal/\vy_i,\, \forall i\in\{1,\cdots,m\},
	\end{align*}
\end{definition}
where $\xi_i$ denotes the slack allotted to each example, $\vell(\vy_i,\vy)$ is the loss function between pseudo-label and the correct label, and $C$ is the slack parameter that controls the amount of regularization in the model.
The primal form can be interpreted as maximizing the margin between the correct training example and the pseudo-example.
Allowing margin error is to make sure solution can always be found.

It seems intuitive to scale the margin error by the loss function, the advantage of which is also claimed the authors in \citep[p.3]{THJA04}.
They suggest that under \mmmn\ framework the learning system will work hard on the examples that incur big loss thought they may not even close to be confusable compared to true target value $\vy_i$.

In addition to the slight difference in loss scaling, the optimization technique employed by \svmstruct\ differ significantly compared to \mmmn.
Since the part of the \svmstruct optimization problem containing loss function is not decomposable, the model cannot remove exponential number of constraints during optimization.
On the other hand, \citet{THJA04} developed an iterative optimization approach that creates a nested sequence of successively tighter relaxation of the original problem via a cutting-plane methods \citep{Bishop07,JFY09}.
In particular, constraints are added as necessary.
It is also shown that the iterative optimization algorithm will converge to some optimal solution of $\epsilon$ precision in polynomial number of steps.

Besides the optimization issue, the major drawback of \svmstruct\ is the intractability of the the inference problem.
In particular, to find the most violating constraint, the model will compute the loss-augmented inference problem as in \citep{TJTA05}
\begin{align}
	\hat{\vy} = \underset{\vy\in\vYcal/\vy_i}{\argmax} &\, [1-\ip{\vw}{\phib(\vx_i,\vy)}]\,\vell(\vy_i,\vy) \label{svmstructinference},
\end{align}
where the loss function $\ell$ appears as a multiplicative term.
As the model does not assume any property of decomposition on the loss function over output space, the loss-augmented inference problem (\ref{svmstructinference}) of \svmstruct\ is in practice intractable. 
The intractability of the inference problem can be seen as an exchange of the generality of the loss function which allows more complicated loss to be defined. 


%
% mtl
\subsection{Multitask Feature Learning (\mtl)}

Multi-task Feature Learning (\mtl) developed in \citep{Argyriou07multitask} is another algorithm designed for multiple interdependent structured output space.
\mtl\ is quite different from other structured output learning algorithms discussed in the previous sections.
In particular, the \mtl\ approach is based on the assumption that different tasks are related such that they share a common underlying feature representations.
Similar assumption is also applied in \citep{Caruana97multitask,Baxter00a,BenDavide03exploiting}.

Denote by $f_t(\vx)$ the task specific function for the $t$'th task.
$f_t(\vx)$ can be expressed as
\begin{align*}
	f_t(\vx) = \ip{\va_t}{h(\phib(\vx))} = \sum_{i=1}^{d}\va_{t}[i]h(\phib(\vx))[i],
\end{align*}
where $\va_t\in\RR^d$ is the weight vector parameters for the $t$'th task.
$h(\phib(\vx))$ is a linear feature mapping composed by
\begin{align*}
	h(\phib(\vx))[i] = \ip{\vu_i}{\phib(\vx)},
\end{align*}
where $\phib(\vx)\in\RR^d$ is in original feature space and $\vu_i\in\RR^{d}$ is a linear feature map.
We further denote by $A$ the matrix composed by $\va_{t}$, and denote by $U$ the matrix composed by $\vu_i$.
\mtl\ assumes that tasks share a small set of features where $A$ is supposed to be sparse with many entries equal to zero.
The optimization problem of \mtl\ is defined as
\begin{definition}{\mtl\ Optimization Problem in Primal}\label{mtl_opt}
	\begin{align*}
		\underset{\substack{\vu\in\RR^{d\times d}\\A\in\RR^{d\times T}}}{\minimize} \left\{\sum_{t=1}^{T}\sum_{i=1}^{m}\ell(\vy_{i,t},\ip{\va_t}{\ip{U}{\phib(\vx_i)}})+C\norm{A}_{2,1}^2\right\},
	\end{align*}
\end{definition}
where the objective is to minimize the empirical error $\ell(\vy_{i,t},\ip{\va_t}{\ip{U}{\phib(\vx_i)}})$ and to keep a small proportion of the non-zero element in $\va_t$.
$C$ is the parameter to balance these two aspects.
As (Definition~\ref{mtl_opt}) is non-convex and the second term is non-smooth, it is transformed into an equivalent form and solved by an alternative optimization approach.

An extension of \mtl\ algorithm was proposed in \citep{Argyriou08convex} which introduced a nonlinear generalization using kernel methods.
In addition, similar but not identical algorithms \citep{Argyriou08an,Jacob09cluster} are also proposed which assume that the tasks form clusters such that task specific weight vectors should be similar within the clusters.
Recently, \citet{Paredes12exploit} proposed a method which exploits the information between unrelated tasks, based on a similar assumption that tasks of different groups tend not to share any features.

%
% ensemble methdos
\section{Ensemble Methods}\label{sc_ensemble}

Ensemble methods are one of the prominent classification techniques widely used in machine learning community.
In general, the methods train multiple base classifiers and combine them in order to achiever better classification performance.
Most importantly, there is no requirement for base classifiers to be accuracy as long as they perform better than random guessing.
%To trace the exact originality of the ensemble methods seems impossible, as the methods involve along well with human society by which it is wiser to take in to account advises from multiple human experts.
There are several different types of ensemble algorithms, to name a few, bagging \citep{Breiman96bagging}, boosting \citep{Freund97a,Schapire99improved}, stacking \citep{Smyth99linearly}, Bayesian averaging \citep{Freund04generalization}.
Most of the algorithms have improved the classification performance when compared to their base classifier counterpart, some of them also enjoyed theoretical guarantee \citep{Schapire97boosting,Koltchinskii00empirical,Cortes14semble,Cortes14deep}.
For our purposes, the section will start with and focus on bagging and boosting ensemble since both methods are best known approaches and are most related to this thesis.
In the later part of the section, we will extend the ensemble methods to structured output space by reviewing several recently established algorithms.

%
% preliminary notations
\subsection{Preliminary and Notations}

In addition to the notations introduced in (Section~\ref{singlelabel_preliminary}, \ref{multilabel_preliminary}), we assume there is a hypothesis class $\Fcal$ where we generate weak/base hypothesis $f^t(x)\in\Fcal$.
By $t$ we intend to index the $t$'th weak hypothesis.
We denote by $H(x)$ the ensemble framework that is able to combine multiple weak hypotheses and generate a stronger one.
In many cases, no other information about $f^t(x)$ is available to $H(x)$ except that each weak hypothesis will take in as parameter $x$ and output $y\in\Ycal$ for single label classification ($\vy\in\vYcal$ in multilabel classification respectively).

%
% boosting
\subsection{Boosting}
\textit{Boosting} corresponds to a learning framework or a family of algorithms that take in a weak classifier and tuning it into a strong one.
We begin our discussion from the notion of  \textit{concept class}.
A \textit{concept} is a boolean function over domain $\vXcal$, and a \textit{concept class} is a class of concepts.
% TODO: an illustrative example
A concept class is \textit{strongly learnable} if there exists a polynomial learning algorithm that achieves high accuracy with high probability for all concepts in the class.
On the other hand, a concept class is \textit{weak learnable} if the learning algorithm achieves arbitrary high accuracy where the only requirement is that the learning algorithm finds a function that performs better than coin flip.
The concept of learnability were proposed in \citep{Kearns94cryptographic} together with the question whether the strong learnability and the weak learnability are equivalent which is known as \textit{hypothesis boosting problem}.
Finding a weak learner which performs better than random guessing is easy in practice, but finding a strong learner is usually difficult.
\citet{Schapire90the} has proved that two classes of learnability are equal which lays the foundation of the boosting algorithm that aim to convert a weak learning algorithm into a strong one that achieves high performance.

Adaptive Boosting (\adaboost) developed by \citet{Freund97a} is the very first practical boosting algorithm and is the most influential one.
In addition, \citet{Schapire99improved} proposed a variant of the algorithm which updates the adaptive parameters in order to minimize the exponential loss of the weak learners.
The exact algorithm is shown in (Algorithm~\ref{adaboost}).
The fundamental idea of \adaboost\ is to maintain a distribution $D$ over all training examples, and update the distribution in each iteration such that difficult-to-classify examples will get more probability mass for the next iteration (line~\ref{update_d}).
In particular, the algorithm computes in each iteration a weak learner $f^t(\vx)$ base on all training examples and the current distribution $D^{t}$ (line~\ref{compuet_f}), calculates the weighted training error $\epsilon^k$ for the current weak learner (line~\ref{compute_error}), and computes the adaptive parameter $\alpha^t$ for the current weak learner (line~\ref{compute_alpha}).
The ensemble is the weighted combination of all weak learners.
\begin{align*}
	H(\vx) = \sign(\sum_{t=1}^T\alpha^tf^t(\vx)).
\end{align*}
\begin{algorithm}
\caption{\adaboost}
\label{adaboost}
\begin{algorithmic}[1]
	\REQUIRE Training sample $\Scal = \{(x_i,\vy_i)\}_{i=1}^m$, learning function $\Wcal$, number of weak learners $T$
	\ENSURE Boosting ensemble $H(\vx)$
	\STATE Initialize $D^0(i)=\frac{1}{m},i\in\{1,\cdots,m\}$
	\FOR{$t=1\cdots T$}
		\STATE $f^t(\vx)\leftarrow\Wcal(\Scal,D^{i-1})$ \label{compuet_f}
		\STATE $y_i = f^t(\vx_i),\forall i\in\{1,\cdots,m\}$
		\STATE $\epsilon^t = \sum_{i=1}^{m}D^t(i)\ind{y_i\neq\hat{y}_i}$ \label{compute_error}
		\STATE $\alpha^{t} = \frac{1}{2}\ln\left(\frac{1-\epsilon^t}{\epsilon^t}\right)$ \label{compute_alpha}
		\STATE $Z=\sum_{i=1}^{m}D^{t-1}(i)\exp(-\alpha^ty_i\hat{y}_i)$
		\STATE $D^t(i) = \frac{1}{Z}D^{t-1}(i)\exp(-\alpha^ty_i\hat{y}_i),\forall i\in\{1,\cdots,m\}$ \label{update_d}
	\ENDFOR
	\RETURN $H(\vx) = \sign(\sum_{t=1}^T\alpha^tf^t(\vx))$
\end{algorithmic}
\end{algorithm}

For each weak learner $f^t(\vx)$, the strategy of updating adaptive parameter $\alpha^t$ is to ensure that the exponential loss of $\alpha^tf^t(\vx)$ is minimized.
To see this, we first compute the exponential loss for $\alpha^tf^t(\vx)$ given current distribution $D^t$ over training example and adaptive parameter for $f^t(\vx)$
\begin{align*}
	\ell_{exp}&(\alpha^t,f^t(\vx),D^t) \\
	&= \sum_{i=1}^mD^t(i)\exp(-\hatf(\vx_i)\alpha^tf^t(\vx_i))\\
	&=\exp(-\alpha^t)\sum_{i=1}^mD^t(i)\ind{\hatf(\vx_i)=f(\vx_i)} + \exp(\alpha^t)\sum_{i=1}^mD^t(i)\ind{\hatf(\vx_i)\neq f(\vx_i)}\\
	&=\exp(-\alpha^t)(1-\epsilon^t) + \exp(\alpha^t)\epsilon^t.
\end{align*}
To minimize $\ell_{exp}(\alpha^t,f^t(\vx),D^t)$, we take partial derivative with respect to $\alpha^t$ and set it to zero
\begin{align*}
	\frac{\partial\ell_{\exp}(\alpha^t,f^t(\vx),D^t)}{\partial\alpha^t} = -\exp(-\alpha^t)(1-\epsilon^t) + \exp(\alpha^t)\epsilon^t=0
\end{align*}
Solve it for $\alpha^t$, we get
\begin{align*}
	\alpha^{t} = \frac{1}{2}\ln\left(\frac{1-\epsilon^t}{\epsilon^t}\right).
\end{align*}
Thus the rule for updating $\alpha^t$ will make sure the exponential loss of $\alpha^tf^t(\vx)$ will be minimized.

It is worth noticing that the \adaboost\ algorithm describe in (algorithm~\ref{adaboost}) requires the learning algorithm $\Wcal$ capable of learning from some specific distribution.
The distribution will reflect the difficulty of the training examples during the learning.
It is usually achieved by \textit{reweighing} which initials a uniform distribution over training examples and updates the distribution after each iteration.
For learning algorithms that cannot handle training data with specific distribution, we can apply \textit{resampling} which samples training examples in each iteration according to some desired distribution.

Most recently, a new boosting algorithm \deepboosting\ was developed in \citep{Cortes14deep}.
\deepboosting\ allows the learning algorithm to use complex hypothesis class contain very deep decision trees, by which it extends \adaboost\ that only uses simple hypothesis class of decision tree with depth one.
In addition, \citet{Cortes14deep} also developed rich learning bounds for \deepboosting\ optimization problem in terms of the Rademacher complexity theory.
The new theory advances the previous performance guarantee of \adaboost\ which is build in terms of margins of the training examples \citep{Schapire97boosting,Koltchinskii00empirical}.


%
% bagging
\subsection{Bootstrap Aggregating}
\textit{Bootstrap Aggregating} (\bagging), initially developed by \citet{Breiman96bagging}, is an ensemble method that exploits the independency between the weak learners.
The algorithm is based on the fact that the error can be dramatically reduced by combining independent base learners.
Denote by $f_i$ the $i$'th weak learner.
The ensemble prediction $H(\vx)$ is the averaged prediction over all weak learners
\begin{align}
	H(\vx) = \sign\left(\sum_{i=1}^{T}f_i(\vx)\right). \label{bagging_ensemble}
\end{align}
Denote by $\hatf$ the ground true function.
Suppose base learner has probability $\epsilon$ of making independent mistake
\begin{align*}
	P(f_i(\vx) \ne \hatf(\vx)) = \epsilon.
\end{align*}
As the bagging ensemble (\ref{bagging_ensemble}) makes a mistake when at least half of the weak learners make mistakes, we can compute the probability of bagging ensemble making mistake by
\begin{align*}
	P(H(\vx)\neq\hatf(\vx)) = \sum_{i=0}^{T/2}\left(\begin{tabular}{cc}T\\k\end{tabular}\right)(1-\epsilon)^k\epsilon^{T-k} \le\exp\left(-\frac{1}{2}T(2\epsilon-1)^2\right).
\end{align*}
It is clear that the probability decreases exponentially with respect to the number of the weak learners.
In extreme case, the probability will approach zero when $T$ approaches infinity.
Though it is impractical as the base learners are hardly independent since they are generated from the same training data, we still try to exploit the independency by adding randomness in the algorithm framework.

\bagging\ uses bootstrap sampling \citep{Efron1994introduction} to generate subsets of training examples.
Given a training set of $m$ training examples, a subset of same size is generated by sampling with replacement $m$ time from original training set.
The sampling procedure is then repeated $T$ time in order to generate $T$ subsets for constructing base learners.
Note that the sampled subsets will be similar as they are sampled from the same training set.
However, they will not be too similar which each subset will only cover around $63\%$ of the origin training data under the condition that $m$ is large.
To see this, consider the probability that the $i$'th training examples is not sampled once is $(1-\frac{1}{m})$, and the probability that the training example is not sampled at all is $(1-\frac{1}{m})^m$.
When $m$ is large, the probability will approach $37\%$. 
That is around $37\%$ of the training examples will not be represented in any given subset.
The property of the bootstrap sampling allows us to efficiently estimate the generalization error of the base learner known as \textit{out-of-bag estimation} \citep{Breiman96out,Tibshirani1996bias,Wolpert99an}.

Random Forest developed in \citep{Breiman01random} is an important extension of \bagging.
The major different from \bagging\ is that randomized feature selection is incorporated in random forest.
In practice, the random forest algorithm selects in each step a subset of features and constructs conventional random tree classifier within the selected subset of features.
The feature split process of decision tree is deterministic given a set of features.
\citet{Liu08spectrum} proposed \vrtree\ which further extend random forest algorithm by introducing randomness in both feature selection and split processes.


\subsection{Ensemble Methods in Structured Output Space}

The ensemble methods have been initially developed for single task classification \citep{Breiman96bagging,Freund97a} or regression tasks \citep{Breiman96bagging}, as it is straightforward to combine multiple scalar output variables.
However, it is not immediately apparent how to combine vector valued outputs in multilabel structured output space.

\adaboostmh\ is the multilabel variance of original \adaboost\ algorithm proposed by \citet{Schapire99improved}, which is further developed by \citet{Esuli2008boosting}.
The core idea of \adaboostmh\ is to adopt hamming loss instead of 0/1 loss. 
In particular, the algorithm reduces from a multilabel classification problem to a set of single label classification problems by replacing each training example $(\vx_i,\vy_i)$ with $k$ examples $\{(\vx_i,\vy_i[l])\}_{l=1}^k$.
The algorithm is detailed in (Algorithm~\ref{adaboostmh}).
In particular, it maintains a distribution over all examples and labels.
In each iteration, the algorithm takes in the distribution and all training examples, generates a weak learner $f^k(\vx)$ (line~\ref{mh_compuet_f}), computes hamming loss (line~\ref{mh_compute_error}), computes the adaptive parameter $\alpha^t$ (line~\ref{mh_compute_alpha}), and update the distribution (line~\ref{mh_update_d}).
The returned prediction $H(x)$ is the combination of base learners $f^t(x)$ weighted by $\alpha^t$.
\begin{algorithm}
\caption{\adaboostmh}
\label{adaboostmh}
\begin{algorithmic}[1]
	\REQUIRE Training sample $\Scal = \{(x_i,\vy_i)\}_{i=1}^m$, learning function $\Wcal$, number of weak learners $T$
	\ENSURE Boosting ensemble $H(\vx)$
	\STATE Initialize $D^0(i,l)=\frac{1}{mk},i\in\{1,\cdots,m\}$
	\FOR{$t=1\cdots T$}
		\STATE $f^t(\vx)\leftarrow\Wcal(\Scal,D^{i-1})$ \label{mh_compuet_f}
		\STATE $\vy_i = f^t(\vx_i),\forall i\in\{1,\cdots,m\}$
		\STATE $\epsilon^t =\sum_{l=1}^k\sum_{i=1}^{m}D^t(i,l)\ind{\vy_i[l]\neq\hat{\vy}_i[l]}$ \label{mh_compute_error}
		\STATE $\alpha^{t} = \frac{1}{2}\ln\left(\frac{1-\epsilon^t}{\epsilon^t}\right)$ \label{mh_compute_alpha}
		\STATE $Z=\sum_{i=1}^{m}\sum_{l=1}^{k}D^{t-1}(i,l)\exp(-\alpha^t\vy_i[l]\hat{\vy}_i[l])$
		\STATE $D^t(i,l) = \frac{1}{Z}D^{t-1}(i,l)\exp(-\alpha^t\vy_i[l]\hat{\vy}_i[l]),\forall i,\forall l$ \label{mh_update_d}
	\ENDFOR
	\RETURN $H(\vx) = \sign(\sum_{t=1}^T\alpha^tf^t(\vx))$
\end{algorithmic}
\end{algorithm}

Furthermore, many other ensemble approaches have been proposed and applied in specific areas, including, in natural language processing applications \citep{Collins05distrimnative,Zeman05improving,Sagae06parsing,Zhang09kbest} and 
in text and speech recognition applications \citep{Fiscus97a,Benesty08speech,Petrov10products}.
Limiting the applicability of these methods is the fact that they are mostly extensions of the ensemble approaches for single task problem, and are specially tailored for the applications.
It is not immediately apparent how to apply these methods to other machine learning tasks.
In addition, other methods that are based on boosting or bagging are also developed in \citep{Wang07simple,Kocev13tree}.
Most recently, three other learning algorithms with performance guarantee are proposed in \citep{Cortes14semble}.

%\marginpar{TODO: extend the section by adding algorithm details in \citep{Cortes14semble}.}




%
%
%
\section{Evaluation and Comparison}\label{sc_statistics}

In this section, we will briefly introduce the performance measures and statistical test methods that are involved in the publications as well as in the later part of the dissertation.
These measures will enable us to quantitatively evaluate the performance of the proposed models and measure the significancy compared to other well established methods.

%
%
\subsection{Performance Measures}

It is often quite difficult and expensive to get labeled data. 
In order to maximize the outcomes of the valuable labeled data, the common approach adopted in practice is known as \textit{$K$-fold cross-validation}.
In particular, the origin set of labeled examples are partitioned into $K$ disjoint subsets of same size, denoted by $\{S_1,\cdots,S_K\}$.
The cross-validation framework will repeat training and test procedures $K$ times.
In the $i$'th iteration, the validation scheme uses $K-1$ subsets of labeled examples $\cup_{j\in\{1,\cdots,K\}/i}S_j$ to build a training set for learning a classifier.
The examples in the $i$'th subset is then taken as test data of which the label is assumed to be unknown to the classifier.
The resulting classifier is used to predict the examples in the $i$'th subset, and is evaluated according to the true labels in the testing set.
In extreme case where we have $K$ equal to the number of original training examples, there will be only one example in each test set.
This is typically known as \textit{leave-one-out cross-validation} (\loo).

As most machine learning algorithms explicitly assume that the training data and the testing data are drawn from the same distribution,
it is worth noticing that we should make sure that the property of original labeled data is well preserved when partitioning the original labeled set into subsets.
In single-label classification problem we can easily preserve the label distribution if we randomly sample labeled examples without replacement from the origin experimental dataset.
However, it is not clear how to preserve label distribution under multilabel classification setting.
To this end, we use \textit{stratification} or \textit{stratified sampling} method where we first group training examples into equivalent classes based on the number of positive labels they have. 
Each class is then randomly split into $K$ local fold, after that local folds are merged to create $K$ global fold.
The stratification scheme ensures that also smaller classes have representations in all folds. 

After partition the original data into training and test sets, we adopt several well known performance measures in order to quantitively evaluate the performance of difference classifiers.
We report \textit{multilabel accuracy} (\textit{0/1 accuracy}) which counts the percentage of multilabel predictions that have all of the microlabel being correct.
The counterpart is often known as \textit{multilabel loss} (\textit{0/1 loss})
\begin{align*}
	\ell_{0/1} = \frac{1}{m}\sum_{i=1}^{m}\ind{\vy_i \neq \hat{\vy}_i},
\end{align*}
where by $\vy_i$ we denote the predicted multilabel for example $\vx_i$, and by $\hat{\vy}_i$ we denote ground true multilabel.
We also report \textit{microlabel accuracy} as the proportion of microlabel being correctly predicted, of which the counterpart is often known as \textit{hamming loss}
\begin{align*}
	\ell_{h} = \frac{1}{mk}\sum_{i=1}^{m}\sum_{j=1}^{k}\ind{\vy_i[j] \neq \hat{\vy}_i[j]}.
\end{align*}
In addition, we report \textit{$F_1$ score} as harmonic mean of \textit{microlabel precision} and \textit{microlabel recall}
\begin{align*}
	F_1 = 2\cdot\frac{Pre\times Rec}{Pre+Rec}.
\end{align*}
Microlabel precision and microlabel recall are defined by
\begin{align*}
	Pre &= \frac{TP}{TP+FP}\\
	Rec &= \frac{TP}{TP+FN},
\end{align*}
where by $TP$, $FP$, and $FN$ we denote \textit{true positive}, \textit{false positive}, and \textit{false negative} predictions respectively.

%
%
\subsection{Statistical Test}

Once we compute the performance measures of multiple machine learning algorithms in terms of 0/1 loss, hamming loss and $F_1$ score, we want to compare the performance in order to decide the best model under consideration. 
To serve as a starting point, we assume a very basic setting where we already have the performance of multiple machine learning algorithms on one selected dataset.
It is straight forward to compare the performance of the algorithms in terms of the performance measures and choose the best based on the value of the measure without relying any statistical methods.
The only assumption made is that the measures accurately reveal the performance of the algorithms.
However, in practice, when a new learning algorithm is developed, it is usually evaluated over multiple dataset to rule out the probability that it performs well on one particular dataset by chance.
Statistical evaluation of experimental results have been considered as an essential part of comparison steps in, for example, such complicate scenarios.
We will base our discuss on \citep{Demsar06statistical,Garcia08an} in the following part of the section.

We first consider the case where we want to compare two learning algorithms over multiple datasets.
In order to decide the best performing method, one way is to average the performance over all testing datasets.
However, it is debatable that the error rate from different domains (testing datasets) are commensurable.
Beside, the averaging is susceptible to the outliers.
Another approach is to use paired t-test to check if the average difference of two learning algorithms over datasets is significantly different from zero.
Paired t-test is problematic due to commensurability.
Besides, the result will not be significant unless the sample size is larger enough (over 30 datasets).
\citet{Demsar06statistical} proposed Wilcoxon signed-ranks test which is a non-parametric alternative to the paired t-test to compared two algorithms over multiple datasets.
Meanwhile, sign-test is also proposed in \citep{Demsar06statistical}.
As the setting is not very interesting for our purpose, we will not expand our discussion on the details.

Second, we will consider a more common situation where we want to compare multiple learning algorithms over many datasets.
In practice, we want to answer the question, for example, whether algorithm A is significantly better than algorithm C, and algorithm C outperformances algorithm B in a significantly manner.
In statistics, this is equivalent to multiple hypothesis test.
The common way is to use repeat-measures ANOVA \citep{Fisher59statistical} with post-hoc Tukey test \citep{Turkey49comparing}. The test procedure is based on two assumptions: the first is known as \textit{normality} meaning samples are drawn from normal distributions; second is known as \textit{sphericity} meaning samples should have equal variant.
It is not difficult to see that the ANOVA is ill-posed as none the assumptions are met within the context of comparing the performance of learning algorithms over datasets.

% to test if pair of algorithms are significantly different from each other 
% to test if algorithms are significantly different from control
To overcome the problem, Friedman test \citep{Friedman37the,Friedman40Comparison} with post-hoc Nemenyi test \citep{Nemenyi63distribution} are recommended by \citet{Demsar06statistical}.
The goal of the test procedure is to examine under multiple hypothesis test setting whether a pair of algorithms are significantly different from each other.
To perform Friedman test, we first compute the average rank $R_j$ of the $j$'th algorithms.
The Friedman test then compares the average ranks and under null-hypothesis states that the average ranks are equal.
The test statistics can be computed according to the procedure discussed in \citep{Demsar06statistical}.
If null-hypothesis is rejected, we can proceed with post-hoc Nemenyi test to compare algorithms with each other.
In particular, we can compute the \textit{critical different} ($CD$) and state that the performance of two classifiers are significantly different if the corresponding average rank differs by at least $CD$.
As the post-hoc Nemenyi test is usually conservative and has little statistical power, we sometimes use Bonferroni-Dunn test \citep{Dunn61mulitple} to compare the learning algorithms against the selected control algorithm, which is usually the newly established or proposed algorithm.
The test results are often visualized as \textit{$CD$ diagram}.














% --------------------------
%
% 		chapter
%
% --------------------------
\chapter{Novel Statistical Models}\label{ch_models}

In this chapter, the author aims to build the connection of the publications as well as to discuss the corresponding contributions.
Each section will mostly focus on one publication, and will start with introduction and background knowledge that are necessary to understand the content within the section.
An overview of the theoretical end technical novelties will also be briefly presented in each section.

\section{Structured Output Prediction for Molecular Activity Classification}\label{sc_su10}

The molecular activity classification problem has long been tackled under the single task classification setting.
On the other hand, multiple interdependent molecular activities are often screened simultaneously.
This makes single task classification approaches suffer from two formidable issues.
The first issue is scalability which in order to predict multiple activities for one molecule a set of single task classifiers need to be built separately, which soon becomes infeasible when large amount of activities need to be predicted.
The second issue lies in the fact the single task model discards the dependency information among multiple output targets.
The dependency structure can be further utilized to improve the performance.
Therefore, in \citepub{su10} we investigate the potential of using structured output learning framework to tackle the molecular activity classification problem.


\subsection{Background and Introduction}

Molecular classification, the goal of which is to predict the anticancer potentials of the drug-like molecules, is one crucial step in drug discovery and has draw enormous attention from machine learning community.
Viable molecular structures are scanned, searched, or designed for therapeutic efficacy.
In particular, expensive preclinical \textit{in vitro} and \textit{in vivo} drug tests can be largely avoided and special efforts can be devoted to few promising candidate molecules, once accurate \textit{in silico} models are available \citep{Burbidg01drug}.

Various machine learning methods have been developed for the task, to name but a few, inductive logic programming \citep{King96structure}, artificial neural network \citep{Bernazzani06predicting}, kernel methods for nonlinear molecular properties \citep{Trotter01drug,Ralaivola05graph,Swamidass05kernel,Ceroni07classification}, and \svm\ based methods for reliable predictions \citep{Trotter01drug,Byvatov03comparison,Xue04effect}.
Albeit with the large quantity of the computational models, they only focus on predicting single target variable (e.g. inhibition potential in one cell line). 
On the other hand, relative larger number of cell line targets are often screened at the same time.
For example in the recent \textit{NCI-60} {Human Tumor Cell Line Screen} project \citep{Shoemaker06the}, thousands of molecular structures are tested agains hundreds of cell line targets.


\subsection{Methods}

To efficiently and accurately predict molecular activities in multiple cell lines at the same time, we proposed a structured output learning algorithm in \citepub{su10}, which is to our knowledge the first multi-task learning approach for molecular classification problem.
The algorithm can be seen as an instantiation of the structured output learning method \mmcrf\ \citep{rousu06}.
In particular, the model defines a compatibility score through inner product for a molecular structure $\vx$ and the activities in multiple cell lines $\vy$
\begin{align*}
	F(\vx,\vy;\vw) = \ip{\vw}{\phib(\vx,\vy)},
\end{align*}
where $\vw$ are the feature weight parameters that ensures the molecular structure with correct activities will be scored higher than with incorrect activities.
$\vw$ are learned by maximizing the minimum loss-scaled margin between correct examples $(\vx_i,\vy_i)$ and incorrect examples $(\vx_i,\vy)$ over the whole training set
\begin{align*}
	\underset{\vw,\vxi_i}{\minimize} & \quad \frac{1}{2}\norm{\vw}^2 + C\sum_{i=1}^{m}\xi_i\\
	\st &\quad \ip{\vw}{\phib(\vx_i,\vy_i)} - \ip{\vw}{\phib(\vx_i,\vy)} \ge \ell(\vy_i,\vy)-\xi_i,\\
	&\quad \xi_i>0, \forall i\in\{1,\cdots,m\}, \forall \vy\in\Ycal,
\end{align*}
where $\ell(\vy_i,\vy)$ is the loss function defined as
\begin{align*}
	\ell(\vy_i,\vy) = \sum_{j=1}^{k}\ind{\vy_i[j]\neq \vy[j]}.
\end{align*}
The loss scaled margin optimization will push high-loss pseudo-examples further away from the correct example than the low-loss pseudo-examples.
The model is optimized by conditional gradient optimization \citep{Bertsekas95nonlinear} in marginalized dual space \citep{Taskar04max}, which not only enjoys a polynomial-size parameter space in marginalized dual representation but also enables the kernel function that is capable of dealing with non-linearity of the complex molecular structures.

Kernel is the classical way of defining the similarity between complex objects.
In \citepub{su10}, We used graph kernel to measure the similarity of pair of molecular structures.
The common way to represent the structure of a molecule is to use an undirected labeled graph $G=(V,E)$, where vertices $V=\{v_1,\cdots,v_n\}$ corresponds to atoms and edge $E=\{e_1,\cdots,e_m\}$ corresponds to the covalent bonds.
The adjacency matrix $A$ of graph $G$ is defined sush that its $(i,j)$'th entry $A_{i,j}$ equals to one if there is an edge between atom $i$ and atom $j$.

% walk kernel
\textit{Walk Kernel} \citep{Kashima03marginalized,Gartner03a} computes the sum of matching walks in a pair of graphs. 
The contribution of each matching walk is down scaled exponentially according to its length.  
We denote by $w_m$ a walk of length $m$ in graph $G$ such that there exists a edge for each pair of vertices $(v_i,v_{i+1})$ for all $i\in\{1,\cdots,m-1\}$.
In addition, we denote by $G_{\times}(G_1,G_2)$ the direct product graph of two graph $G_1$ and $G_2$, of which the vertices of a product graph are computed from 
\begin{align*}
	V_{\times}(G_1,G_2) = \{(v_1,v_2)\in V_1\times V_2, label(v_1)=label(v_2)\},
\end{align*}
and edges are computed from
\begin{align*}
	E_{\times}(G_1,G_2) = \{((v_1,v_2),(u_1,u_2))\in V_{\times}\times V_{\times},(v_1,u_i)\in E_1\wedge (v_2,u_2)\in E_2\}.
\end{align*}
The walk kernel can be defined on the adjacency matrix of $G_{\times}$
\begin{align*}
	K_{wk}(G_1,G_2) = \sum_{i,j=1}^{|V_{\times}|}\left[\sum_{n=0}^{\infty}\lambda^{n}A_{\times}^n\right]_{i,j},
\end{align*}
where $0<\lambda<1$.
Using exponential series or geometric series the walk kernel can be compute in cubic time \citep{Gartner03a}
\begin{align*}
	K_{wk}(G_1,G_2) = \ve\tp(\vI-\lambda A_{\times})^{-1}\ve,
\end{align*}
where we denote by $\vI$ the identity matrix and by $\ve$ the matrix of ones.

% weighted decomposition kernel and others
\textit{Weighted Decomposition Kernel} in \citep{Menchetti05weighted,Ceroni08classification} is an extension of substructure kernel \citep{Haussler99convolution} by weighting identical part in two graph with contextual information.
The kernel will evaluate the matching subgraphs (\textit{contextor}) in the neighborhood of an atom (\textit{selector}).
We also used \textit{Tanimoto Kernel} \citep{Ralaivola05graph} on a finite set of molecular fingerprints \citep{Wang09pubchem}.
See \citep{Vishwanathan10graph} for a more comprehensive survey on graph kernels.

%\subsubsection{Generation of Markov Network}
To apply structured output learning algorithm described above, we assume the underlying Markov network structure is fully observed.
In other words, we need to build a network connecting cell lines used as the output graph, with nodes correspond to cell lines and edges correspond to potential statistical dependencies.
Auxiliary datasets are available for this purpose \citep{Shoemaker06the}  which implicitly describes the relationship between cell lines.
In practice, we first computed the covariance matrix for cell lines based on auxiliary datasets, then extracted the network structure by applying the following two methods.
In maximum spanning tree approach, we take the minimum number of edges that make a connected network whilst maximizing the edge weights.
In correlation thresholding approach, we take all edges that exceed a fixed threshold, which typically generates a non-tree graph.


%
%
\section{Graph Labeling Ensemble (\mve)}\label{sc_su11}

Multi-task molecular activities classification with structured output learning that relies on the representation of the cell-lines structures allows us to model the dependencies betweem different output variables.
When apply structured output learning, it is explicitly assumed the structure of the output network is fully observed.
In \citepub{su10}, we supposed the feasibility of extracting the structures assuming it is given implicitly in the auxiliary datasets.
For many real world structured output learning problems, we cannot take the structure as granted as it is often difficult to get the underlying dependency structure.
Therefore, we explored in \citepub{su11} the possibility of constructing an simple ensemble of multiple structured output learning models built on random output graphs and applied the technique on molecular activity classification task.


\subsection{Methods}

We use \mmcrf\ as base classifier trained on a set of random output graphs.
Particularly, for each base model, a random graph $G_t$ is generated to couple the output vector variables $\vy$ which are the activities of molecular structure $\vx$ over all cell lines.
The base model \mmcrf\ is then learned based on training data $S=\{(\vx_i,\vy_i)\}_{i=1}^{m}$ and the output graph $G_t$.
After ensemble has been generated, the predictions are extracted from each base classifier and are collected for a post-processing step: we compute a majority vote over the graph labeling from the sign on the means of the base classifier's prediction
\begin{definition}{Majority Voting Ensemble (\mve).}
\begin{align*}
	F_j^{\text{\mve}}=\underset{y_j\in\Ycal_j}{\argmax}\,\left(\frac{1}{T}\sum_{i=1}^{T}\ind{F_j^{(t)}(x)=y_j}\right), \forall j\in\{1,\cdots,k\},
\end{align*}
\end{definition}
where by $T$ we denote the size of the ensemble, and by $F^{(t)}(\vx)=\{F^{(t)}_j(x)\}_{j=1}^{k}$ we denote the predicted multilabel in the $t$'th base classifier.
In words, the ensemble prediction on each microlabel is the most frequently appearing prediction among the base classifiers.
It is also worthy of noticing that the \mve\ framework can be extended with any structured output learning models as long as they incorporate the output structure into learning process and make predictions based on the structure.

We designed two approaches to generate random output graphs.
In random spanning tree approach, we first generate a random correlation matrix and extract the spanning tree out from the matrix.
The approach will eventual output a tree structure connecting all vertices.
In random pairing approach, we randomly draw two vertices at a time and couple the two with an edge.
The approach will output disconnected pairs.


\section{Random Graph Ensemble (\amm, \mam)}\label{sc_su14a}

Through extensive experiments in \citepub{su11}, we have validated the feasibility of constructing a majority voting ensemble (\mve) of structured output learner built on a set of random output graphs.
The proposed method also improved the performance on molecular activity classification problem.
In \citepub{su14b}, we aim to develop elegant aggregation techniques (\amm\ and \mam\ respectively) that will perform inference before or after forming ensemble.
We tested the proposed techniques on a set of heterogeneous multilabel datasets from various domains to verify the wide applicability.
In addition, we have brought forward a theory that explains the improvement of \mam\ framework.
The theory is based on the reconstruction error of the compatibility score.


\subsection{Background and Introduction}

% compatibility score
Besides the assumption made for \mve\ which the base classifier will devise the prediction according to the structure of the output graph, we also assume that the base classifier for \amm\ and \mam\ is modelled with a Markov network structure $G=(E,V)$.
That is the base classifier defines a compatibility score $\psi(\vx,\vy)$ for pairs $(\vx,\vy)\in\vXcal\times\Ycal$ according to the output graph $G$, indicating how well the input and the output goes together
\begin{align*}
	\psi(\vx,\vy) = \ip{\vw}{\phib(\vx,\vy)} = \sum_{e\in E}\ip{\vw}{\phib_e(\vx,\vy_e)}=\sum_{e\in E}\psi_e(\vx,\vy_e),
\end{align*}
where by $\psi_e(\vx,\vy_e)$ we denote edge compatibility score, or \textit{edge potential}, between input $\vx$ and edge label $\vy_e$ on edge $e$.
$\vw$ is the feature weight parameters that naturally ensures input $\vx$ with correct output $\vy$ achieves higher compatibility score than with incorrect output.

% edge potential
In addition, we assume we have access to the edge potentials between input and edge labels for each base classifiers
\begin{align*}
	\psib_E^{(t)} = (\psi_e^{(t)}(\vx,\vu_e))_{e\in E^{(t)},\vu_e\in\Ycal_e}.
\end{align*}
With edge compatibility scores, we can infer the max-marginal of label $u_i$ on node $y_i$ \citep{Wainwright05map}.
\begin{align*}
	\tilde{\psi}_j(\vx,u_j) = \underset{\vy\in\Ycal,y_j=u_j}{\maximize}\sum_{e\in E}\psi_e(\vx,\vy_e).
\end{align*}
That is the maximum score of the multilabel consistent with $y_i=u_i$.
We denote the collection of max-marginals by $\tilde{\psib} = (\tilde{\psi}_j(\vx,u_j))_{j\in V,u_j\in\Ycal_j}$.

\subsection{Methods}

% amm
Denote by  $\Gcal=\{G^{(1)},\cdots,G^{(T)}\}$ a set of random output graphs, and by $\{\tilde{\psib}^{(1)},\cdots,\tilde{\psib}^{(T)}\}$ the max-marginal vectors from base classifiers that is built on each output graph.
The \amm\ prediction on each node is obtained by averaging the max-marginals of base classifiers and choose the maximizing microlabel for the node
\begin{definition}{Average of Max-Marginal Aggregation (\amm).}
	\begin{align*}
	F_j^{\text{\amm}}=\underset{u_j\in\Ycal_j}{\argmax}\,\frac{1}{T}\sum_{t=1}^{T}\tilde{\psi}_{j,u_j}^{(t)}(\vx),
	\end{align*}
\end{definition}
and the predicted multilabel is composted by the predicted microlabels
\begin{align*}
	F^{\text{\amm}}=\left(F_j^{\text{\amm}}\right)_{j\in V}.
\end{align*}

% mam
\amm\ performs the inference to find the max-marginals before forming ensemble. 
On the other hand, the maximum of average marginals aggregation (\mam) will first collect the local edge potentials $\psib_E^{(t)}$ from each base classifier, average them and finally performs inference on the global consensus graph $\hat{G}=(\hat{E},V)$ with averaged edge potentials
\begin{definition}{Maximum of Average marginals Aggregation (\mam).}
	\begin{align*}
		F^{\text{\tiny MAM}}(x) &= \underset{\vy \in \Ycal }{\argmax}\,\sum_{e\in \hat{E}}\frac{1}{T}\sum_{t=1}^{T}\ \psi^{(t)}_e(x,\vy_e)
		%\vy^* &= \underset{\vy}{\argmax}\, \frac{1}{T}\sum_{t=1}^{T}\sum_{e\in E_t}{w^t_e}^T\varphib_e(x,\vy)
		= \underset{\vy \in \Ycal}{\argmax}\, \frac{1}{T} \sum_{t=1}^T \sum_{e\in \hat{E}} \ip{\vw_e^{(t)}}{\phib_e(x,\vy_e)}. %\label{ensemble3}
	\end{align*}
\end{definition}
\citepub[Lemma 1]{su14b} allows us to simplify the computation of \mam\ in terms of dual variables and kernels.
As a results, the \mam\ ensemble can be equivalently stated as
\begin{align*}
F^{\text{\tiny MAM}}(x) 
 & =\underset{\vy \in \Ycal}{\argmax}\, \frac{1}{T} \sum_{t=1}^T \sum_{i,e,\vu_e} \mu^{(t)}(i,e,\vu_e) \cdot H_e(i,\vu_e;x,\vy_e)\nonumber \\
 & =\underset{\vy \in \Ycal}{\argmax}\,   \sum_{i,e,\vu_e}  \bar\mu(i,e,\vu_e) H_e(i,\vu_e;x,\vy_e),
\end{align*}
where by $\bar\mu(i,e,\vu_e)=\frac{1}{T}\sum_{t=1}^T\mu^{(t)}(i,e,\vu_e)$ we denote the marginalized dual variable averaged over ensemble, and
\begin{align*}
	H_e(x_i,\vu_e;x,\vy_e)  = K_{\phib}(x,x_i)\cdot\left(K_{\Upsilon,e}(\vy_{ie},\vy_e)-K_{\Upsilon,e}(\vu_e,\vy_e)\right).
\end{align*}
	
Besides the algorithm frameworks for random graph ensemble, we also developed theoretical analysis to explain the improvement of \mam\ ensemble.
The analysis extends the theory on single task ensemble developed by \citet{Brown10good}.
In general, \citepub[Theory 1]{su14b} states that the reconstructive error from \mam\ ensemble is guaranteed to be less than or equal to the average reconstruction error from the base classifiers.
The improvement can be decomposed into two terms, namely \textit{diversity} and \textit{coherence}.
The former measures the variability of individual classifiers learned from different perspectives which shares the same argument as the analysis of single task ensemble model \citep{Brown10good}, the later measures the correlation of microlabel predictions which higher correlation gives better improvements.

%
% RTA
%
\section{Random Spanning Tree Approximation (\rta)}\label{sc_su14c}

Random Spanning Tree Approximation (\rta) model developed in \citepub{su14c} is a major step forward of \mam.
The goal of \rta\ is to bring in joint learning and inference framework as well as to provide extensive theory to guarantee the performance and the tractability of the inference.
Especially, \rta\ explores the potential of tackling the intractable graph inference problem with a set of random spanning trees.
Thus, it lays the foundation of performing inference on unknown structure to achieve accurate optimization with attainable computational efforts.

\subsection{Background and Introduction}

Limiting the applicability of structured output learning methods is the fact that the output structure is assume to be observed.
However, it is often prohibitive to obtain the correct output structure, sometimes it is arguably a even harder problem that the structured output learning itself \citep{Chickering94learning}.
In stead, one can opt to the \textit{complete graph}, in which there is an edge connecting each pair of vertices, as the output graph structure and rely on the learning algorithm to discover the proper 'parameters' defined for the edges (e.g. edge potentials).
To understand this, think the edges that are not from the correct output graph will have zero 'parameters' in the complete graph.

Structured output learning with complete graph is by no mean a easier problem as the inference is \nphard\ in nature, which is often instantiated as finding the \textit{maximum a posteriori} (MAP) configuration on a graph-structured probability distribution.
In terms of the intractability issue of graph inference problem, many techniques have been proposed but with important differences.
\citet{Jordan04semiefinite} developed semi-definite programming convex relaxation for inference on graph with cycles.
\citet{Wainwright05map} proposed MAP inference with tree-based and \textit{linear programming} (LP) relaxation.
Efficient inference on special case of graph has also been extensively studied in \citep{Globerson07approximate}.

\citepub{su14c} is motivated by well-established max-margin assumption (Section~\ref{}) and investigate whether the problem of inference over a complete graph in structured output learning can be avoided in a analogous way.
Start from a sampling results, \citepub[Lemma 3]{su14c} shows that with high probability a big fraction of margin from the complete graph can be obtained by a sample of spanning trees of small size.
Besides, \citepub[Theorem 5]{su14c} shows the good generalization can also be guaranteed when learning with in stead of complete graph a sample of spanning trees.

Thus, in addition to \citepub[Theory 1]{su14b}, we further provide the theoretical justification of \mam\ that constructs a set of base classifiers trained on random spanning trees. 
Besides, \citepub[Theorem 5]{su14c} suggests we should optimize the joint margin from all spanning trees, instead of optimizing them separately as \mam, which leads to the \rta\ model described in the following section.

\subsection{Methods}

Denote by $\Tcal=\{T_1,\cdots,T_n\}$ a sample of $n$ random spanning trees, and by $\{\vw_{T_t}|T_t\in\Tcal\}$ the feature weights to be learner on each tree.
The goal of optimization is to maximize the joint margin from all spanning trees between correct training examples and other examples.
\begin{definition}{\bf Primal $L_2$-norm Random Tree Approximation (\rta).}\label{primalrta}
	\begin{align*}
		\underset{\vw_{T_t},\xi_i}{\minimize} & \quad \frac{1}{2}\sum_{t=1}^{n}\norm{\vw_{T_t}}_2^2 + C\sum_{i=1}^{m}\xi_i\\
		\st & \quad \sum_{t=1}^{n}{ \langle \vw_{T_t}, \phib_{T_t}(\vx_i,\vy_i) \rangle} - \underset{\vy \neq \vy_i}{\maximize\ } \sum_{t=1}^{n}{\langle \vw_{T_t}, \phib_{T_t}(\vx_i,\vy) \rangle } \geq 1 -  \xi_i, \\
		& \quad \xi_i\ge0\, , \forall\ i \in \set{1,\dots,m},
	\end{align*}
\end{definition}
where $\phib_{T_t}(\vx,\vy)$ is the feature map that is local on each tree $T_t$, $\xi_i$ is the margin slack allocated for each $\vx_i$, and $C$ is the slack parameter that controls the amount of regularization.

The key for the optimization is to solve the '$\argmax$' problem efficiently.
This is a \nphard\ problem in practice, as the size of the multilabel space is exponential in terms of the number of microlabels.
For the optimization problem, we proposed in \citepub{su14c} a $K$-best inference algorithm working in $\Theta(Knk)$ time per data point, where $k$ is the number of microlabels in the multilabel vector and $K$ is the number of best multilabels we compute from each random spanning tree.

In particular, it is known that the exact solution for the inference problem on individual tree $T_t$ is tractable \citep{Koller09probabilistic}, for which 
\begin{align}\label{rta:maximizer_onetree}
	\hat{\vy}_{T_t}(x) = \underset{\vy\in\Ycal}{\argmax}\, F_{\vw_{T_t}}(x,\vy)\ =\ \underset{\vy\in\Ycal}{\argmax}\, \langle \vw_{T_t}, \phib_{T_t}(x,\vy)\rangle,
\end{align}
can be solved in $\Theta(k)$ time by \textit{dynamic programming} also known as \textit{max-product} or \textit{min-sum}.
However, there is no guarantee the the maximizer of (\ref{rta:maximizer_onetree}) is also the global maximizer of (Definition~\ref{primalrta}) over all spanning trees.
Therefore, we compute for each random spanning tree $K$-best multilabels, which in total costs $\Theta(Knk)$ time for all spanning trees.
\citepub[Lemma 7]{su14c} provides a method to retrieve the best multilabel from $K$-best list in linear time.
Now the question is to make sure the global violation multilabel is within the list.
\citepub[Lemma 8]{su14c} guarantees that with high probability the global violation multilabel is in the list and $K$ does not need to be large.

In addition, we derived the marginalized dual representation of (Definition~\ref{primalrta})
\iffalse
\begin{definition}{\bf $L_2$-norm RTA Marginalized Dual.}
	\begin{align*}
		%\underset{\mub\in\Mcal^m}{\maximize} &\quad \frac{1}{|E_\Tcal|}\sum_{k,e,\vu_e}\mu(k,e,\vu_e)   -\frac{1}{2}\sum_{k,e,\vu_e} \mu(k,e,\vu_e) K_\Tcal^e(k,\vu_e;k',\vu'_e) \mu(k',e,\vu'_e)\, ,
		\underset{\vmu\in\Mcal^m}{\maximize} &\quad \frac{1}{|E_\Tcal|}\sum_{e,k,\vu_e}\mu(k,e,\vu_e)   -\frac{1}{2}\sum_{\substack{e,k,\vu_e,\\k',\vu_e'}} \mu(k,e,\vu_e) K_\Tcal^e(x_k,\vu_e;x_k',\vu'_e) \mu(k',e,\vu'_e)\, ,
	\end{align*}
\end{definition}
\fi
which not only enjoys a polynomial sized parameter space but also enable kernel function to deal with complex input space.


%
%
\section{\spin\ for Network Response Prediction}\label{sc_su14b}

In \citepub{su14a}, a structured output learning model for network response prediction problem is developed.
The proposed model, \spin, is \textit{context-sensitive} and is different from the previous the state-of-the-art methods which only model the influence in terms of the network connectivity.
The inference problem of \spin\ is \nphard\ in general which was solved by a semidefinite programming algorithm \sdp\ with approximation guarantee and by a fast \greedy\ heuristics.

\subsection{Background and Introduction}

With the extensive availability of the large scale networks, there are tremendous interest in studying the phenomena of the network influence, in particular, the structure, the function and the influence dynamics. 
The outcome of the network influence research have been widely applied in many areas, to name but a few, spreadness of pathogens or infectious diseases \citep{Hethcote00the,Anderson02infectious}, diffusion of medical and technology innovation \citep{Strang98diffusion,Rogers03the}, opinion and news formation \citep{Adar04implicit,Gruhl04information,Adar05tracking,Leskovec07cascading,Nowell08tracing,Leskovec09meme}, viral market \citep{Domingos01mining,Kempe03maximizing,Liben-Nowell03the}.

In the field of studying network influence, one primary interest is to discover the latent structures that reveal the dynamics of influences.
In general, the problem can be defined into two different ways depending on the availability of the underlying network structure.
On one hand, one would assume that the underlying structure is hidden or incomplete and the only observation is a cascade of actions.
The instantiation of the setting can be, for example, online news agents sharing information but not physically connected, or in epidemiological study where people are affected by pathogens through various ways.
In this case, the task is to infer network structure in terms of edges connecting nodes given a set of action cascade.
%With early work on independent cascade and linear threshold model \citep{Kempe03maximizing},
Many algorithms are designed to solve the problem in this setting from \netinf\ \citep{GomezRodriguez10inferring}, \netrate\ \citep{Rodrigues11unconvering}, \kernelcascade\ \citep{Du12learning} to the most recent work about two stage model for inferring influence \citep{Du14influence}, inference algorithm using cascade without timestamp \citep{Amin14learning}, and general framework of inferring diffusion structure \citep{Daneshmand14estimating}.
On the other hand, we argue the problem is unnecessarily hard to solve as in many cases the structure of the network is given (e.g. friendship network).
There are also related researches aim to discover the hidden variables in the network \citep{Lovrek08prediction,Goyal10learning}

However, none of them consider the property of the action.
In particular, our network influence problem is motivated by the following observation: for a given action $\va$ performed on the network $G$, the influence from node $u$ to $u'$ not only depends on their connections but also depend on the action under consideration.
For example, $u'$ follows $u'$ in twitter network, $u'$ will retweet the message from $u$ if the message is related to \textit{science} but not related to \textit{politics}.
Therefore, we propose the following definition of the network influence problem
\begin{definition}{Network Influence Problem.}\\
	Given a complex network and an action performed on the network, predict the subnetwork that responses to the action. In particular, which nodes will perform the action and which directed edges relay the action from one node to its neighbors.
\end{definition}

%
\subsection{Methods}
We approach the problem by structured output learning, where we define a computability score as a inner product of the action $\va$ and the response network $\Gva$
\begin{align*}
	F(\va,\Gva;\vw) = \ip{\va}{\phib(\va,\Gva)}.
\end{align*}
It is easy to understand that the action $\va$ with correct response network $\Gva$ will achieve high score than with any incorrect response network $\Gva'$.
As introduced in previous section, the joint feature map $\phib(\va,\Gva)$ is composed by the tensor product between action feature $\varphib(\va)$ and the features of the response network $\Upsilonb(\Gva)$.
In particular, $\varphib(\va)$ can be a bag-of-words features of the action (e.g. blog post) and $\Upsilonb(\Gva)$ is a vector of edges and labels of the response network $\Gva$.

% optimization
The feature weight vector $\vw$ is learned through maximum-margin structured output learning by solving the following optimization problem
\begin{definition}{Primal \spin\ Optimization Problem.}
\begin{align*}
	\underset{\vw,\vxi_i}{\minimize} &\quad \frac{1}{2}\norm{\vw}^2+C\sum_{i=1}^{m}\xi_i\\
	\st &\quad F(\va_i,G_{\va_i};\vw) > \underset{\substack{G_{\va_i}'\in\Hcal(G)/G_{\va_i}}}{\maximize}(F(\va_i,G_{\va_i}';\vw)+\vell_{G}(G_{\va_i},G_{\va_i}')) - \xi_i,\\
	&\quad \xi_i\ge0, \forall i\in\{1,\cdots,m\},
\end{align*}
\end{definition}
where by $\Hcal(G)$ we denote the set of directed acyclic graphs of $G$.

% inference
To solve the above optimization problem, we have to compute both in training and in prediction the highest-scoring subgraph for an action. 
In particular, during the training the goal is to find the worst margin violating subgraph which corresponds to solving the loss-augmented maximization problem
\begin{align*}
	H^*(\va_i) = \underset{G_{\va_i}'\in\Hcal(G)/G_{\va_i}}{\argmax}\, (F(\va_i,G_{\va_i}';\vw)+\vell_{G}(G_{\va_i},G_{\va_i}')).
\end{align*}
During the prediction, the goal is to find the subgraph with maximum compatibility given the action
\begin{align}
	H^*(\va_i) = \underset{H\in\Hcal(G)}{\argmax}\, F(\va_i,H;\vw) \label{spin_inference}.
\end{align}
As the two problems are different only in terms of the definition of the score, we explain our inference algorithm based on (\ref{spin_inference}) by writing the problem explicitly in terms of weight vectors and the feature maps
\begin{align}
	H^*(\va_i) &= \underset{H\in\Hcal(G)}{\argmax}\, \ip{\vw}{\varphib(\va)\otimes\Upsilonb(H)} \nonumber \\
	&= \underset{H\in\Hcal(G)}{\argmax}\, \sum_{e\in E^H}s_{\vy_e}(e,\va), \label{spin_np}
\end{align}
where we denote by $s_{\vy_e}(e,\va) = \sum_{i}\vw_{i,e,\vy_e}\varphib(\va)$ the score of edge $e$ with edge label $\vy_e$.

\begin{lemma}
	Finding the graph that maximizes (\ref{spin_np}) is an \nphard\ problem.
\end{lemma}
In supplementary material of \citepub{su14a}, we have proved the \nphardness\ by forming a reduction from the \maxcut\ problem \citep{Garey90computers}.
In addition, we proposed two algorithms to solve (\ref{spin_np}).
The first is called {\sdp\ inference} which introduces for each node $u\in V$ a binary variable $x_u\in\{-1,+1\}$ and transfer the inference problem into an integer quadratic program (\iqp) problem.
The \iqp\ is then tackle by similar technique introduced in \citep{Geomans1995improved}, such that each variable $x_u$ is relaxed to a vector $\vv_u\in\RR^n$ and the relaxed problem is solved by semidefinite programming (\sdp). 
The resulting vector is rounded back into binary value by incomplete Cholesky decomposition.
The benefit from \sdp\ inference algorithm is the approximation guarantee. 
%If we denote by $Z^*$ ($Z_R$, and $Z$ respectively) the optimal value achieved by \iqp\ (the relaxed problem, and the \sdp\ inference respectively), we have $Z_R\ge Z^*$ and $E[Z]\ge(\alpha-\epsilon)Z_R$ where $\alpha\ge0.796$.
In particular, the proposed \sdp\ inference algorithm is a $0.796$ approximation of the original \iqp.

As \sdp\ inference is not scalable on large networks, we developed a \greedy\ heuristic base on observation stated as the following lemma:
\begin{lemma}
	The inference problem (\ref{spin_np}) can be stated equivalently as a function of activated vertices
	\begin{align*}
		H^{*}(\va) = \underset{{H \in {\cal H}(G)}}{\argmax} \sum_{ \substack{v_i\in V^H_{\p}}}F_m(v_i). \nonumber 
	\end{align*}
\end{lemma}
The proof is given in supplementary material of \citepub{su14a}. 
As a results, the \greedy\ algorithm starts with an empty vertex set and adds one vertex at a time such that the increment of the score is maximized over all inactivated vertices.
The iteration ends when the objective cannot be improved.  
It is worthy noticing that we are not able to give any approximation guarantee for the solutions produced by the \greedy\ algorithm.
The property of submodularity, which is often used to analyze the greedy algorithm, only holds for the special case of our inference problem.

% loss function
The decomposable loss function $\ell_G(G_{\va},G_{\va}')$ will penalize the mistakes uniformly over the network.
However, we wish the learning algorithm focus on the vicinity of the \textit{foci} based on the observation that one certain action will only affect a small part of the network.
This suggests we should penalize the mistake according to the distance to the \textit{foci}.
Therefore, we define the \textit{symmetric-difference loss}, also known as \textit{hamming loss}, applied on the nodes and edges of the network. 
In addition, we define the scaling function $\gamma_G$ according to the distance to \textit{foci}.
The resulting loss function is 
\begin{align*}
	\ell_G(G_{\va},G_{\vb}) = \sum_{u\in V}\ell_v(G_{\va},G_{\vb})\gamma_G(u_k;u) + \sum_{(u,u')\in V}\ell_e(G_{\va},G_{\vb})\gamma_G(u_k;u).
\end{align*}
We proposed two scaling function in \citepub{su14a}.
One is \textit{exponential scaling} where mistakes are penalized by $\lambda$ and $\lambda$ is weighted exponentially according to the shortest path distance to \textit{foci}.
The other is \textit{diffusion scaling} where mistakes are penalized by the values computed from diffusion kernel \citep{Kondor02diffusion}.
Diffusion scaling has the effect of shrinking the distance to the nodes that are connect to \textit{foci} by many paths.



%
%
\chapter{Conclusion}\label{ch_conclusion}

\section{Discussion}

In this thesis, we study supervised learning in structured output space of which the task is to predict the best values for multiple interdependent variables given an arbitrary input variable.
In particular, we focus on the problem of structured output learning when the output structure is unknown.
We propose a learning framework that aims to optimize on a random sample of spanning trees as output structure.
The novel learning framework is well motivated in terms of margin achieved by training examples, and the performance in terms of generalization error is also guaranteed.
In addition to the new learning framework, we also develop a new structured output prediction method that is able to predict a directed acyclic graph (DAG) and apply the methods on network influence prediction problem.
We also tackle the optimization problem of the proposed models for which the exponential sized parameter space and the \nphard\ inference problem can usually be avoided.

We have posed several research questions at the beginning of the thesis which serve as the guide through the presentation of the work.
Section~\ref{sc_single} and Section~\ref{sc_multi} introduce the backgrounds of both single-task and multi-task learnings.
The sections also present several well-established algorithms that are relevant to the thesis.
Section~\ref{sc_ensemble} discuss a few ensemble approaches that are close related to the learning framework developed in the thesis.
After the introduction, Chapter~\ref{ch_models} devotes to bringing in novel statistical models for structured output prediction that are developed by the author.
We will revisit the research questions with the insights and the results we gain from the novel statistical models that are briefly presented in the thesis as well as detailed in the publications.
In particular, we give the answer to the following questions.

\begin{enumerate}[label=\textbf{Q \Roman*}:]
\item Is the structured output prediction models more suitable for predicting multiple interdependent variables in structured output space?

Multi-task structured output prediction problem presented in Section~\ref{sc_multi} requires multiple output variables to be predicted at the same time.
The structured output prediction models are more suitable for the problem as it describes the interdependencies between the output variables and is more efficient compared to the single-task models presented in Section~\ref{sc_single}.
Besides, we have demonstrated the performance of the structured output prediction models in molecular activity prediction problem presented in \citepub{su10} and in network influence prediction problem presented in \citepub{su14a}.

\item How to tackle the problem of structured output learning without observed output structure?

As it is often difficult to get the output structure that describes the interdependencies of the output variables, we have developed several algorithms which eventually lead to a general learning framework that learns from a random sample of spanning trees.
In particular, we have developed the following learning algorithms.
\mve\ which collects and postprocesses the predictions from each spanning tree.
\amm\ and \mam\ both performs parameter learning based on individual random spanning tree.
The only difference is that the inference of \amm\ is also computed on individual tree, on the other hand, the inference of \mam\ is based on the consensus graph by pooling edges from all spanning trees.
Furthermore, \rta\ brings in the joint learning and inference such that all random spanning trees are optimized toward the same global objective.

\item What is the motivation of the proposed structured output learning framework? Can we explain the improvement and guarantee the empirical risk of the proposed models?

The proposed learning framework is designed for structured output prediction problem with unknown output structure.
Learning with any observed structure can be seen as a special case of learning with a complete graph.
The idea of learning from a random sample of spanning trees as output graph has been explained in \citepub{su14c} as an approximation to learning with a complete graph in terms of margin achieved on a set of training examples.
Thus, the proposed new learning framework is well motivated.
The improvement of the performance can be explained in terms of reconstruction error of the compatibility scores as in \citepub{su14a}.
\citepub{su14c} also provide the generalization error for the proposed model.

\item Can we efficiently optimize the proposed structured output prediction models?

The optimization problem in structured output prediction is usually difficult to optimize for two reasons, exponential sized parameter space due to the number of possible solutions and the \nphard\ inference problem instantiated as MAP configuration on the general graph.
To avoid the exponential sized parameter space, we use marginalize dual decomposition technique which reduces the parameter space to polynomial size in terms of the number of edges in the output graph.
We used different strategies to overcome the \nphard\ inference problem.
We applied loopy belief propagation for the inference problem defined on the general graph structure with approximate solution.
We used dynamic programming for the inference problem defined on a tree structure where the exact solution is guaranteed.
We developed $K$-best inference algorithm in \citepub{su14c} for the inference problem defined on a set of random spanning trees.
We have also developed \sdp\ inference algorithm for the inference problem defined on directed acyclic graph (DAG).
\end{enumerate}

\section{Future Work}

What has been shown in the thesis is a new learning framework with algorithm instantiations for structured output prediction problems where the structure of the output is not known but is expected to play an important role.
An immediate extension is to explore its enormous potentials in real world practical data analysis applications with the significant benefit that we do not need to know the output structure as \textit{prior}.
In addition, since the inference algorithm designed for optimizing over a random sample of spanning tree is tractable both in theoretic analysis and empirical results, we can apply the model on the structure prediction problems for which the output structure is rather complex.

With respect to algorithm development and theoretical study, we plan to continue with \rta\ framework that performs joint learning and inference over a random sample of spanning trees.
In particular, we will explore the possibility of applying $L_1$ norm combination of the feature weight parameter $\vw_{T_t}$ of individual random spanning tree, instead of using convex $L_2$ norm combination. 
Learning with $L_1$ norm regularization of parameter combination can be expressed into an equivalent form of learning weighted $L_2$ norm regularization \citep{Rakotomamonjy08simplemkl}, where the weight of $\{\vw_{T_t}\}_{t=1}^{T}$ is the extra parameter during the optimization.
We also gain benefit by seeing the weight as the fitness of each spanning tree to current training data.
Similar to $L_2$ norm, the studies on generalization error and the condition for the tractable inference are required.
Besides the alternative optimization for the $L_1$ norm combination in \citep{Rakotomamonjy08simplemkl}, we plan to develop more elegant optimization strategy for the problem. 








%% Examples of article references, remove these from your manuscript!
% Uncomment them, if you want to see the results of these commands in this example document
% Refer to the Journal paper 1 of this example document
%\citepub{su14b} \& \cpub{su14b} \& \cp{su14b} \& \pageref{su14b} \& \ref{su14b}
% Refer to the Conference paper of this example document
%\citepub[p.~2]{su14a} \& \cpub[Sec.~ 1]{su14a} \&  \cp[pp.~1--2]{su14a} \& \pageref{su14a} \& \ref{su14a} 


\bibliographystyle{apalike} 
\bibliography{dissertation}
