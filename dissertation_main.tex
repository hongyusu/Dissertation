





\chapter{Introduction}\label{ch_introduction}


%
%
\section{Scope of the Thesis}

In this thesis, we study the \textit{supervised learning} in structured output space, also known as \textit{structured output prediction}.

\textit{Machine learning}, an active research field in information and computer science, has drawn enormous attentions during the last few decades, not only because it develops intelligent systems that generalize well from previously observed examples; but also because it is rooted from rich statistical learning theories that prove the feasibility and guarantee the performance.
Machine learning has in practice provided lots of inventions, that people may use many times a day without noticing, from web searching engine to self-driving car, from optical image recognizer to spam filter, from automatic stock exchange to online recommender system.

\textit{Supervised learning}, one of the most important fields in {machine learning}, is usually instantiated as learning a function that is capable of predicting the best value for the output variable $\vy$ given an observed input variable $\vx$.
The function is learned by utilizing a set of observed input/output pairs known as training examples.
In the classical supervised learning setting, the value that the output variable $\vy$ can take is often no more than one discrete or continuous number.
This is often referred as \textit{single-task classification} in machine learning community.
Many well-designed learning algorithms have been delivered for the problem and achieved remarkable success in many applications.

However, many real world problems require multiple interdependent output variables defined in structured output space to be predicted at the same time, known as \textit{multi-task structured output prediction}.
Otherwise, the problem can be summarized as learning a function for mapping from arbitrary input to arbitrary output.
In this case, single-task classification approach renders inefficiency and incapability of modeling the interdependency.
In addition, the interdependency can be given either explicitly as the output graph that connecting multiple output variables (e.g. sequence parsing, network influence prediction, hierarchical document classification), or implicitly with the only assumption that the output variable are correlated (e.g. molecular classification, computer vision).
Nevertheless, it is crucial to capture the common properties shared between similar output variables in order to achieve good performance and to generalize well.

Most existing methods for structured output prediction do not tackle the situation when the output graph structure is not observed.
Especially, the learning models built on Markov network over multiple output variables will break down without the observed output structure.
On the other hand, methods that do not explicitly assume the output structure usually have less power of modeling the interdependency and often struggle in optimization due to the exponential sized searching space.

In this thesis, we investigate the potentials of developing novel structured output prediction models to improve the prediction performance in real world multi-task classification problems (e.g. molecular activity prediction, network influence prediction).
In particular, we present the novel structured output prediction models that are capable of learning without predefined structure.
We focus on the efficiency and the scalability of the proposed structured output prediction models, as the inference problem is often difficult to solve.
Additionally, our new models are well motivated.
The theoretical study is able to explain the behaviour and to guarantee the performance of the models.
In particular, our research questions can be casted as followings:
\begin{enumerate}[label=\textbf{Q \Roman*}:]
\item Is the structured output prediction model more suitable for predicting multiple interdependent variables in structured output space?
\item How to tackle the problem of structured output learning without observed output structure?
\item What is the motivation of the proposed structured output learning framework? Can we explain the improvement and guarantee the empirical risk of the proposed models?
\item Can we efficiently optimize the proposed structured output prediction models?
\end{enumerate}



%
%
\section{Contribution and Outline of the Thesis}

The main contribution of the thesis is to present a learning framework for structured output prediction with both empirical and theoretical guarantee.
Under the framework, several learning algorithms are proposed.
Unlike previous structured output learning algorithms, the proposed models do not assume any observed structured on the output variables, and thus largely widen the applicability of the structured output learning.
Meanwhile, the thesis presents several attractive theoretical properties which support the new learning framework and guarantee the generalization error.
In addition to the main contribution, the thesis also proposes a new learning algorithm that is capable of predicting a directed acyclic graph and demonstrating the performance on the influence network prediction problem.
Besides, the thesis also focuses on elegant optimization strategy with which the proposed models often avoid the exponential parameter space and the \nphard\ inference problem.

The thesis is structured as follows.
In Chapter~\ref{ch_introduction}, the scope of the research is briefly explained with several research questions being proposed which will be answered in this thesis.
Chapter~\ref{ch_background} provides the introduction to the problem settings, learning algorithms, and statistical models that are necessary to understand the rest part of the thesis.
The chapter starts by introducing in Section~\ref{sc_remp} two fundamental concepts, empirical risk minimization and regularization, in statistical machine learning.
Section~\ref{sc_single} presents several single task classification algorithms, including 
perceptron (Section~\ref{sc_perceptron}), 
logistic regression (Section~\ref{sc_lr}), 
and \svm\ (Section~\ref{sc_svm}).
Section~\ref{sc_multi} will extend the previous section by introducing the learning algorithms designed for multiple interdependent variables in structured output space.
Ensemble models for both single-task and multi-task classification will be briefly covered in Section~\ref{sc_ensemble}.
Section~\ref{sc_statistics} describes the performance measures and the statistical models that are used for model evaluation and comparison.

Chapter~\ref{ch_models} introduces the novel statistical models developed by the author.
In particular, Section~\ref{sc_su10} explains the structured output learning model for molecular activity prediction.
Section~\ref{sc_su11} and Section~\ref{sc_su14a} discuss three ensemble models (\mve, \amm, \mam) that tackle the problem in structured output prediction when the output structure is unknown.
Section~\ref{sc_su14c} presents \rta\ model that is a major step forward of \mam\ by performing joint learning and inference over a random sample of spanning trees.
Section~\ref{sc_su14b} introduces network influence prediction problem and presents \spin\ that is capable of predicting a directed acyclic graph.
Finally, Chapter~\ref{ch_conclusion} concludes the thesis by answering the research questions and pointing out future directions.

In this thesis, the ideas and the formalisms of the proposed statistical models for structured output prediction are explained.
However, the technical details from the original publications are usually not repeated.
Furthermore, the notations and the presentations of some proposed models are slightly improved to be better incorporated into a unified framework.
The background information and the related work of some proposed models are sometimes elaborated in the thesis to help better understand their contributions to the community.
Empirical evaluations of the proposed models are in nowhere restated, rather, they can be found from the original research articles. 



%------------------------------------------------
%
%
%
%------------------------------------------------
\chapter{Regularized Learning for Classification} \label{ch_rlc}



%------------------------------------------------
%
%------------------------------------------------
\section{Regularized Risk Minimization}\label{sc_rrm}

In this section, the author will introduce two fundamental concepts in statistical machine learning, known as \textit{empirical risk minimization} and \textit{regularization}.
Together they give rise to the learning algorithms that will be presented in the following part of the thesis.

\subsection{Empirical Risk Minimization}\label{sc_erm}

Without loss of generality, we assume two random variable $\vx\in\vXcal$ and $\vy\in\vYcal$ are jointly distributed according to some fixed but unknown probability distribution $P(\vx,\vy)$ over domain $\vXcal\times\vYcal$.
In addition, we are provided with examples in pair $(\vx,\vy)\in\vXcal\times\vYcal$ generated by sampling according to the distribution $P(\vx,\vy)$.
A \textit{hypothesis class} $\Hcal$ is a set of functions that the learning algorithm is allowed to search against.
The basic goal for \textit{statistical learning} is to provide from a hypothesis class $\Hcal$ an \textit{estimator} $f\in\Hcal:\vXcal\rightarrow\vYcal$ that predicts the value of $\vy$ given an arbitrary input $\vx$.

To measure the goodness of the estimator, we define a risk function (loss function) $\Lcal(\vy,f(\vx))$ between the true value $\vy$ and the predicted value $f(\vx)$.
The \textit{true risk} of the estimator $f$ over all examples from domain $\vXcal\times\vYcal$ can be computed from
\begin{align}
	\Rcal(f) = \int_{(\vx,\vy)\in\vXcal\times\vYcal}\Lcal(\vy,f(\vx))P(\vx,\vy)\,d_{\vx}d_{\vy} \label{true_risk}.
\end{align}
As a results, we should search for an estimator $f$ that minimizes the true risk defined by (\ref{true_risk}).
However, the distribution $P(\vx,\vy)$ that generates the examples is unknown by which it is impossible to compute (\ref{true_risk}) directly.
Instead, we are given a random sample of $m$ examples, denoted by $S=\{(\vx_1,\vy_1),\cdots,(\vx_m,\vy_m)\}$, called \textit{training data}.
\textit{Empirical risk} is defined as the average error made by estimator $f$ on the training data $S$ of finite size
\begin{align}
	\Rcal_{emp}(f) = \frac{1}{m}\sum_{i=1}^{m}\Lcal(\vy_i,f(\vx_i)) \label{empirical_risk}.
\end{align}
This suggests the learning algorithm should search for an estimator $f$ to minimize the empirical risk defined in (\ref{empirical_risk}).
The strategy is known in machine learning community as \textit{empirical risk minimization} \citep{Vapnik92principles}.

\subsection{Regularized Learning}\label{sc_rl}

The empirical risk minimization strategy is ill-posed which in general can provide infinity number of estimators with same empirical risk on the same training data.
Besides, it often leads to \textit{overfitting} when the dimensionality of the feature space is high and the number of training data is limited.
In other words, underlying true distribution $P(\vx,\vy)$ is difficult to to estimate from a finite sample of training examples.
Thus, the estimator will generalize poorly on unseen test examples.
The \textit{regularization theory} provides a framework to alleviate these two difficulties.
In particular, it suggests to minimize 
 \begin{align*}
	\Jcal(f) = \Rcal_{emp}(f) + \Omega(f),
\end{align*}
where $\Omega(f)$ is the regularization function that controls the complexity of the estimator $f$ by penalizing the norm of the feature weight vector.

In terms of the linear function class, there are varying way to defined the regularization term $\Omega(f)$ among which $L_1$-norm and $L_2$-norm regularization are most popular.
The $L_2$-norm regularization, defined by 
\begin{align*}
	\Omega(f) = ||\vw||_2=\left(\sum_{i=1}^d|\vw[i]|^2\right)^{\frac{1}{2}},
\end{align*} 
controls the complexity of the estimator $f$ and provides a smooth solution.
It has been applied in, for example, Ridge Regression \citep{Hoerl00ridge}, Logistic Regression \citep{Chen00}, Support Vector Machines \citep{Cortes95support}.
The $L_1$-norm regularization, defined by
\begin{align*}
	\Omega(f) = ||\vw||_1=\sum_{i=1}^d|\vw[i]|,
\end{align*}
provides sparse parameter estimation which means that we obtain a high dimensional weight vector with many zero entries.
This is a attractive property as the feature selection is incorporated into the learning and the resulting model often enjoys a high interpretability.
The $L_1$-norm regularization has been applied in, for example, \lasso\ \citep{Tibshirani94regression}.
Besides, other regularization techniques have been widely studied, to name a few, $L_{1,2}$-norm regularization \citep{Argyriou07multitask} and elastic net regularization \citep{Zou05regularizationa}.


%------------------------------------------------
%
%------------------------------------------------
\section{Single-Label Classification}\label{sc_slc}

In this section, the author will introduce the basic classification problem known as \textit{single-label classification}.
The author will explain three prominent algorithms in this area: Perceptron, Logistic Regression and Support Vector Machines.
The optimization techniques and the latest advances of these algorithms will also be briefly discussed.
The goal is to cover the background that is necessary to understand the algorithms presented in the later part of the dissertation. 

%
%
\subsection{Preliminary and Notations}\label{sc_slc_pn}
We consider supervised learning problem which assumes an arbitrary input space $\vXcal$, an output space $\Ycal$, and the training samples coming in pairs $(\vx_i,y_i)\in\vXcal\times\Ycal$.
The definition of the output space will decide the type of the learning problem.
For example, we obtain multiclass classification problem by setting $\Ycal=\{1,\cdots,K\}$ and regression problem by setting $\Ycal=\RR$ where $\RR$ is the set of real numbers.
In this chapter we focus on standard supervised learning problem also known as \textit{binary classification}, we explicitly assume the output space $\Ycal=\{-1,+1\}$ (instead of $\Ycal=\{0,+1\}$).
Additionally, we assume a feature map $\varphib:\vXcal\rightarrow\Fcal$, which embeds the input into some high dimensional feature space $\Fcal=\RR^d$.
In particular, $\varphib(\vx)$ is a real value vector of $d$ dimension.
Given a training set of $m$ training examples $\Scal=\{(\vx_1,y_1),\cdots,(\vx_m,y_m)\}$, the goal is to learn from a \textit{hypothesis class} $\Hcal$ a mapping function $f\in\Hcal:\vXcal\rightarrow\Ycal$ that maps an input $\vx\in\vXcal$ to an output $y\in\Ycal$. 

We consider the hypothesis class to be a set of \textit{linear classifiers} that are parameterized by the weight vector $\vw$ and the bias term $b$ defined as
\begin{align}
	f(\vx;\vw,b) = \ip{\vw}{\varphib(\vx)} + b, \label{linearclassifier}
\end{align}
where by $\ip{\cdot\,}{\cdot\,}$ we denote the inner product of two vectors
\begin{align*}
	\ip{\vw}{\varphib(\vx)} = \sum_{i=1}^{d}\vw[i]\,\varphib(\vx)[i].
\end{align*}
In addition, for any $1\le\rho\in\RR$ we defined the {$L_{\rho}$-norm of vector $\vw$ as
\begin{align*}
	\norm{\vw}_{\rho} = \left(\sum_{i=1}^{d}|\vw[i]|^{\rho}\right)^{\frac{1}{\rho}}.
\end{align*}
We use $||\vw||$ to denote the $L_2$-norm of vector $\vw$ in the following part of the thesis for the convenience of the presentation.


%
%
\subsection{Perceptron}\label{sc_perceptron}

The Rosenblatt's Perceptron algorithm \citep{Rosenblatt58,Rosenblatt62} is one type of linear classifiers that is parameterized by a weight vector $\vw$ and a bias term $b$. 
The decision boundary, given by 
\begin{align*}
	f(\vx;\vw,b) = \ip{\vw}{\varphib(\vx)} + b =0,
\end{align*}
will separate the data into two classes.
The goal of the Perceptron learning is to obtain $\vw$ and $b$ by minimizing the distance of misclassified training examples to the decision boundary.

Assume data point $(\vx_i,y_i)$ is misclassified, the signed geometric distance from $\vx_i$ to decision boundary can be computed according to
\begin{align*}
	D(\vx_i;\vw,b) = -\frac{y_i(\ip{\vw}{\varphib(\vx_i)}+b)}{\norm{\vw}}.
\end{align*}
Denote by $\vXcal^m$ the set of misclassified examples, the objective function of Perceptron is defined as
\begin{definition}{Perceptron Objective Function.}\label{perceptron}
	\begin{align*}
		\underset{\vw,b}{\minimize} \quad D(\vw,b) = \underset{\vw,b}{\minimize} \left( -\sum_{\vx_i\in\vXcal^{m}}y_i[\ip{\vw}{\varphib(\vx_i)}+b]\right).
	\end{align*}
\end{definition}
To optimize (Definition~\ref{perceptron}), we first assume $\vXcal^{m}$ is fixed and compute the partial gradient with respect to parameter $\vw$ and $b$
\begin{align*}
	\frac{\partial D(\vw,b)}{\partial\vw} = -\sum_{\vx_i\in\vXcal^m}{y_i\vx_i},\quad
	\frac{\partial D(\vw,b)}{\partial b} = -\sum_{\vx_i\in\vXcal^m}{y_i}.
\end{align*}
In practice, the Perceptron algorithm uses stochastic gradient descent that iteratively processes one example from training data at a time.
In each step, the algorithm makes sure the current $\vw$ and $b$ will correctly separate the current training example. 
Once a misclassified example is visited, it adjusts the parameters according to the update rule
\begin{align*}
	\vw \leftarrow \vw + \eta y_i \vx_i, \quad b \leftarrow b + \eta y_i,
\end{align*}
where $\eta$ is the perceptron learning rate.
\begin{theory}[Perceptron Convergence Theorem \citep{Block62the,Novikoff62}]\label{perceptron_theory}
	Given a sequence of training examples in pairs $\Scal=\{(\vx_1,y_1),\cdots,(\vx_m,y_m)\}$. 
	For simplicity, we consider a perceptron without bias term $b$.
	Assume $\norm{\vx_i}\le R$ for all $i\in\{1,\cdots,m\}$.
	Suppose for some $\gamma>0$ there exists a unit length vector $\norm{\hat{\vw}}=1$ such that $y_i(\ip{\hat{\vw}}{\varphib(\vx_i)})\ge\gamma$ holds for all $i$ (training data $\Scal$ is linearly separable).
	Then the number of mistakes the Perceptron algorithm makes on the sequence of training data $\Scal$ is at most ${R^2}/{\gamma^2}$.
\end{theory}
\iffalse
\begin{proof}
	Suppose the $k$'th mistake is made on the $i$'th training example, and current weight vector is $\vw^k$.
	In addition, we set $\vw^0=\vzero$.
	As the Perceptron makes a mistake on $(\vx_i,y_i)$, we immediately have
	\begin{align*}
		y_i\ip{\vw_k}{\varphib(\vx_i)} \le 0.
	\end{align*}
	According to update rule, we have
	\begin{align*}
		\vw^{k+1} = \vw^{k} + \rho y_i\varphib(\vx_i).
	\end{align*}
	Then, the following holds
	\begin{align*}
		\ip{\vw^{k+1}}{\hat{\vw}} = \ip{\vw^k}{\hat{\vw}} + \rho y_i\ip{\varphib(\vx_i)}{\hat{\vw}} \ge \ip{\vw^k}{\hat{\vw}} + \rho\gamma.
	\end{align*}
	Straight forward induction gives us
	\begin{align}
		\ip{\vw^{k+1}}{\hat{\vw}}\ge k\rho\gamma. \label{induction1}
	\end{align}
	We also have
	\begin{align*}
		\norm{\vw^{k+1}}^2 
		= \norm{\vw^{k} + \rho y_ix_i}^2
		= \norm{\vw^k}^2 + \rho^2\norm{\varphib(\vx_i)}^2 + 2\rho y_i\ip{\varphib(\vx_i)}{\vw^k}
		\le \norm{\vw^k}^2 + \rho^2R^2
	\end{align*}
	Straight forward induction gives us
	\begin{align}
		\norm{\vw^{k+1}}^2\le k\rho^2R^2. \label{induction2}
	\end{align}
	Together with (\ref{induction1}) and (\ref{induction2}) we have
	\begin{align*}
		k\le{R^2}/{\gamma^2}.
	\end{align*}
\end{proof}
\fi
Theory~\ref{perceptron_theory} shows that if the data are linearly separable, the perceptron algorithm will make a finite number of mistakes. 
In particular, the perceptron algorithm will iterate through the training set and will converge to a vector that well separates all training examples.
Moreover, the number of mistakes will be bounded by the minimum gap between the positive and negative examples.
That is the algorithm converges quickly when the gap $\gamma$ is big.
We have to keep in mind that the convergence theorem does not say anything about the quality of the solution, though it is built based on the assumption that there is a ``gap'' between positive and negative examples.

The standard Perceptron algorithm supposes that the data are linearly separable, iterates over the training set, and eventually converges.
However, the setting is less interesting for most applications where the data might not be linearly separable or it takes too long for the algorithm to converge.
Therefore, we need to find out the best decision rule given a set of updates.

\textit{Voted Perceptron} developed in \citep{Freund99large} is a straight forward modification of the standard Perceptron.
Voted Perceptron algorithm keeps track all weight vectors that appears during training phase and their weights.
The weight can be seen as the ``survival time'' of the weight vector during the Perceptron training.
The prediction for a new test example $\vx_{ts}$ is computed by 
\begin{align}
	y_{ts} = \sign\left({\sum_{l=1}^{k}c^l(\sign{\ip{\vw^l}{\varphib(\vx_{ts})}+b^{l}}})\right) \label{voted_perceptron_prediction}
\end{align}
using the weight vectors $\vw^l$ and the corresponding weights $c^l$ obtained during learning.
\iffalse
\textit{Voted Perceptron} developed in \citep{Freund99large} is a straight forward modification of the standard perceptron.
As described in (Algorithm~\ref{voted_perceptron}), the voted perceptron algorithm keeps track all weight vectors that appears during training phase (line~\ref{perceptron_algorithm_weight_vector}) and their weights (line~\ref{perceptron_algorithm_weight}).
The weight can be seen as the ``survival time'' of the weight vector during Perceptron training.
The prediction for a new test example $\vx_{ts}$ is computed by 
\begin{align}
	y_{ts} = \sign\left({\sum_{l=1}^{k}c^l(\sign{\ip{\vw^l}{\varphib(\vx_{ts})}+b^{l}}})\right) \label{voted_perceptron_prediction}
\end{align}
using the weight vectors and the corresponding weights output from the algorithm (line~\ref{perceptron_algorithm_average}).
\begin{algorithm}
\caption{Voted Perceptron Learning Algorithm}
\label{voted_perceptron}
\begin{algorithmic}[1]
	\REQUIRE Training sample $\lbrace (x_i,\vy_i)\rbrace_{i=1}^m$, iteration limit $T$
	\ENSURE Weight parameter $\vw$
	\STATE $\vw^{1}=\vzero,c^1=0$
	\STATE $k=1$
	\FOR{$t=1\cdots T$}
		\FOR{$i=1\cdots m$}
			\STATE Compute $\hat{y} = \sign(\ip{\vw}{\varphib(\vx_i)})$
			\IF{$\hat{y} = y_i$}
				\STATE $c^k = c^k + 1$
			\ELSE
				\STATE $\vw^{k+1} = \vw^{k} + \rho y_i\varphib(\vx_i)$ \label{perceptron_algorithm_weight_vector}
				\STATE $c^{k+1} =1$ \label{perceptron_algorithm_weight}
				\STATE $k=k+1$
			\ENDIF
		\ENDFOR
	\ENDFOR
	\RETURN $\vw = \{\vw^{k}\}_{l=1}^{k},\,c = \{c^{k}\}_{l=1}^{k}$ \label{perceptron_algorithm_average}
\end{algorithmic}
\end{algorithm}
\fi
% TODO: vote perceptron error bound, add reference 
It is proved that Voted Perceptron (\ref{voted_perceptron_prediction}) guarantees to achieve better generalization error than original Perceptron (Definition~\ref{perceptron}).
However, Voted Perceptron does not work in practice as it needs to store all weight vectors encountered during training.

\textit{Averaged Perceptron} is a simple alternative, where in stead of storing all weight vectors the algorithm only keeps the weighted sum $\sum_{l=1}^{k}c^l\vw^l$. 
The prediction for a new test example $\vx_{ts}$ is computed by 
\begin{align*}
	y_{ts} = 
	%\sign\left(c^l({\sum_{l=1}^{k}{\ip{\vw^l}{\varphib(\vx_{ts})}+b^{l}}})\right) 
	= \sign\left({{\ip{\sum_{l=1}^{k}c^l\vw^l}{\varphib(\vx_{ts})}+\sum_{l=1}^{k}c^lb^{l}}}\right),
\end{align*}
which is more efficient to compute and is extremely similar to (\ref{voted_perceptron_prediction}) except for the inside $\sign$ operation.

%
% logit
\subsection{Logistic Regression (\lr)}\label{sc_lr}

% general introduction
Logistic Regression is another important method in statistical machine learning and is closely related to Perceptron (Section~\ref{sc_perceptron}) and Support Vector Machines (Section~\ref{sc_svm}).
It is in fact a classification model rather than regression \citep{Bishop07}.
It models the conditional probability $P(y=+1|\vx)$ for a binary output variable $y\in\Ycal$.
%Without loss of generality, in the rest part of this section we use $P(\vx)$ to denote above conditional probability of $P(y|\vx)$ when fixing $y=1$.
To model the probability, we do not restrict to any particular form, as any unknown parameters can be estimated by \textit{Maximum Likelihood Estimation} (\mle).
However, we are most interested in the simple linear model as presented in (\ref{linearclassifier}).

% detailed model, decision boundary
To use linear model in Logistic Regression, 
\iffalse
the first choice is to let $P(y=+1|\vx)$ be a linear function of $\vx$, while the problem is the linear function is unbounded but the probability $P(\vx)\in[0,1]$.
Another choice is to let $\log P(y=+1|\vx)$ be a linear function of $\vx$. 
However, the problem is the log-likelihood ranges from zero to infinite but the linear function is unbounded.
The choice in Logistic Regression is to use 
\fi
we apply logistic transformation of the original probability function
\begin{align*}
	\log\frac{P(y=+1|\vx)}{1-P(y=+1|\vx)} = \ip{\vw}{\varphib(\vx)} + b,
\end{align*}
which by solving for $P(y=+1|\vx)$ results in 
\begin{align}
	P(y=+1|\vx;\vw,b) = \frac{1}{1+\exponential{-\ip{\vw}{\varphib(\vx)}-b}}. \label{lr_1}
\end{align}
We can also compute
\begin{align}
	P(y=-1|\vx;\vw,b)=1-P(y=+1|\vx;\vw,b)=\frac{1}{1+\exponential{\ip{\vw}{\varphib(\vx)}+b}}. \label{lr_2}
\end{align}
By putting (\ref{lr_1}) and (\ref{lr_2}) together, we define Logistic Regression as
\begin{definition}{Logistic Regression.} \label{logistic_regression}
	\begin{align*}
		P(y|\vx;\vw,b) = \frac{1}{1+\exponential{-y(\ip{\vw}{\varphib(\vx)}-b)}}.
	\end{align*}
\end{definition}
Naturally, we expect the prediction $y=+1$ when $P(y=+1|\vx;\vw,b)\ge0.5$ and $y=-1$, when $P(y=+1|\vx;\vw,b)<0.5$.
The decision rule is similar to Perceptron where we predict $y=+1$ when $\ip{\vw}{\varphib(\vx)}+b\ge0$, and $y=-1$ otherwise.
Besides decision boundary, Logistic Regression can output the class probability from (Definition~\ref{logistic_regression}) as the the ``distance'' of the data point to the decision boundary.
It is the probabilistic output that makes Logistic Regression no more than a classifier, as it outputs detailed predictions (class probability).

% optimization
As the model can output class probability, to obtain a Logistic Regression model we attempt to learn parameters $\vw$ and $b$ by maximizing the probability (likelihood) of the training data.
In particular, the probability of training data $\vx$ with class label $y$ is $P(y|\vx)$.
The likelihood of parameters given data can be computed from
\begin{align}
	L(\vw,b;D) = \prod_{i=1}^mP(y_i|\vx_i). \label{lr_likelihood}
\end{align}
To apply \mle, it is easier if, instead of maximizing the likelihood, we maximize the log-likelihood, which turns the product (\ref{lr_likelihood}) into sum
\begin{align}
	\log L(\vw,b|D) = \sum_{i=1}^{m}\log P(y_i|\vx_i) = -\sum_{i=1}^{m}\log (1+\exponential{-y_i(\ip{\varphib(\vx_i)}{\vw}+b)}) \label{lr_likelihood_sum}.
\end{align}

% logistic regression of other type
Though \mle\ training for Logistic Regression meets our need of fitting the training data, it does not guarantee to generalize well on the test data.
To achieve better generalization power, one can apply regularization technique as discussed in Section~\ref{sc_regularization}.
Many regularization methods for Logistic Regression have been proposed \citep{Chen99,Chen00,Goodman03}, among which adding Gaussian prior on weight parameter $\vw$ is one of the standard option.
In practice, we assume $\vw$ is generated according to a zero-mean spherical Gaussian with variance $\sigma^2$.
Thus, the \mle\ problem (\ref{lr_likelihood}) is transformed into \textit{Maximum A-Posteriori} (\map) problem of the following form
\begin{align}
	P(\vw,b|D;\sigma^2) = P(\vw|\sigma^2)\prod_{i=1}^mP(y_i|\varphib(\vx_i))=\exponential{-\frac{\norm{\vw}^2}{\sigma^2}}\prod_{i=1}^{m}{\frac{1}{1+\exponential{-y_i(\ip{\varphib(\vx_i)}{\vw}+b)}}}. \label{lr_posteriori}
\end{align}
Instead of maximizing the posteriori (\ref{lr_posteriori}), it is easier to maximize the log-posteriori
\begin{align}
	\log P(\vw,b|D;\sigma^2) = -{\frac{\norm{\vw}^2}{\sigma^2}} - \sum_{i=1}^{m}\log{(1+\exponential{-y_i(\ip{\varphib(\vx_i)}{\vw}+b)})}. \label{lr_posteriori_log}
\end{align}

Numbers of optimization techniques have been proposed to solve the maximization problem (\ref{lr_posteriori_log}) in Logistic Regression, many of which are covered in the survey \citep{Minka03}.
For example, {Itrative Scaling} methods were proposed and continuously developed in \citep{Darroch72,Pietra97inducing,Berger97,Goodman02Sequential,Jin03a}.
A quasi-Newton method was discussed in \citep{Minka03}.
\citet{Komarek05making,Lin2008trust} have proposed truncated Newton method.
And coordinate descent method was proposed in \citep{Huang09iterative}.

% dual for and optimization
There are also efforts being made to solve the logistic regression from its dual, of which the first algorithm was proposed in \citep{Jaakkola99probabilistic}.
Later on, other dual optimization algorithms have been developed, for example iterative optimization method \citep{Keerthi05a} and dual coordinate descent method \citep{Yu11dual}.


\iffalse
The key is to introduce a set of tighter upper bounds on (\ref{lr_posteriori_log}) that are parameterized by $\valpha$.
The bounds should have a simple form such that the maximizing (\ref{lr_posteriori_log}) over $\vw$ can be solved analytically with $\alpha$.
The solution to the original problem is transferred as finding the tighter upper bounds for  $\vw$, which is to minimize with respect to $\alpha$.
\iffalse
The dual form is given by 
\begin{align*}
	\underset{\valpha}{\minimize} & \quad\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\valpha_iy_ix_ix_jy_j\valpha_j + \sum_{i=1}^{m}\left[\valpha_i\log\alpha_i+(\sigma^2-\valpha_i)\log(\sigma^2-\valpha_i)\right] \\
	\st &\quad 0\le\alpha\le\sigma^2, \, \forall i=\{1,\cdots,m\}.
\end{align*}
\fi
Later on, optimization algorithms that are based on dual form have been developed, for example iterative optimization method \citep{Keerthi05a} and the dual coordinate descent method \citep{Yu11dual}.
\fi
% TODO: add detail deviation of dual form of logistic regression
% TODO: explain the loss and gain of different optimzation algorithm


%
% 
\subsection{Support Vector Machines (\svm)}	\label{sc_svm}

% some history
Support Vector Machines (\svm) is probably the most widely used classification algorithm in various real world applications.
It is also among the best plug-and-play machine learning algorithms that usually achieves good classification performance.
In this section we will begin the story of \svm\ by first introducing the \textit{Maximum-Margin Principle} of separating training data.
Then we will discuss the primal-dual optimization strategy and the kernel methods that allow \svm\ to deal with high dimensionality.
In the end we will briefly cover the optimization strategy developed for \svm.

The algorithm framework of \svm\ was firstly introduced in \citep{Boser92,Cortes95support}.
Theory and algorithm are detailed in book chapters e.g. \citep{Scholkopf02learning,taylor04,Bishop07}.
% hard margin svm
We begin our discussion by considering a very simple case where we assume the data are linearly separable, that is there is a hyperplane in the feature space which separates the training data into two classes.
In addition we assume the \textit{separating hyperplane} has a simple form
\begin{align*}
	f(\vx)=\ip{\vw}{\varphib(\vx)} + b = 0,
\end{align*}
such that we predict $y_i=-1$ if $f(\vx_i)<0$, and $y_i=+1$ if $f(\vx_i)>0$.
Given $\vw$ that achieves correct separation on the training data, we can decide the label of an arbitrary test example $x_{ts}$ by the decision rule $y_{ts}=\sign(f(\vx_{ts}))$.

However, there can be infinite number of separating hyperplanes that solves the separation problem on the training data which is analogue to empirical risk minimization strategy presented in Section~\ref{sc_emp}. 
We wish to find the one that also generalizes well on the test data.
A good strategy is to look for a hyperplane that keeps the maximum distance from examples of two classes.
The strategy is often unknown as \textit{Maximum-Margin Principle}.
To see this, imagining putting a separating hyperplane close to one class of examples which will achieve better classification on the test examples from the other class.

Based on Maximum-Margin Principle, we further denote the \textit{margin} for the $i$'th example by $\gamma_i$ which is defined as the geometric distance from the data point to the separating hyperplane
\begin{align*}
	\gamma_i = \frac{y_i(\ip{\vw}{\varphib(\vx_i)}+b)}{\norm{\vw}}.
\end{align*}
We notice if we scale $\vw$ and $b$ with any constant (e.g. $\vw\leftarrow\kappa\vw, b\leftarrow\kappa b, \kappa\in\RR$, ) the margin $\gamma_i$ stays unchanged. 
We still achieve the same classification performance and generalization power.
As parameters are invariance to scaling, we set $\norm{\vw}=1$.
Given a set of training example $S$, we also define the margin with respect to $S$ as the minimum margin of individual training example
\begin{align*}
	\gamma = \underset{i\in\{1,\cdots,m\}}{\minimize}\gamma_i.
\end{align*}

Our goal is to find the separating hyperplane such that it maximizes the margin with respect to all training examples while separating the training examples into two classes.
Naturally, this corresponds to finding a ``big-gap'' between two classes.
The max-margin solution is given as \citep{Bishop07}
\begin{align*}
	\underset{\vw,b,\gamma}{\maximize} &\quad \gamma\\
	\st &\quad y_i(\ip{\vw}{\varphib(\vx_i)}+b) \ge \gamma, \, \norm{\vw}=1, \, \forall i\in\{1,\cdots,m\}.
\end{align*}
The objective is to maximize the minimum margin over all training examples such that all examples are correctly separated with margin at least $\gamma$.
However, this is very difficult to optimize not only because the constraint $\norm{\vw}=1$ is non-convex, but also the optimization is not in any standard form.
By replacing $\vw$ with $\frac{\vw}{\gamma}$, we obtain 
\begin{align*}
	\underset{\vw,b}{\maximize} &\quad \frac{1}{\norm{\vw}}\\
	\st &\quad y_i(\ip{\vw}{\varphib(\vx_i)}+b) \ge 1, \, \forall i\in\{1,\cdots,m\},
\end{align*}
which is equivalent to
\begin{definition}{Primal Hard-Margin \svm\ Optimization Problem.}\label{hardsvmprimal}
	\begin{align*}
		\underset{\vw}{\minimize} & \quad\frac{1}{2}\norm{\vw}^2\\
		\st & \quad y_i(\ip{\vw}{\varphib(\vx_i)} + b) \ge 1, \forall i\in\{1,\cdots,m\},
	\end{align*}
\end{definition}
where the objective is to find the weight vector of minimum norm that corresponds to maximize the margin between two class of examples.
The constraints states that the training data should be well separated.

% soft margin svm
We do not use Definition~\ref{hardsvmprimal} in practice for two reasons. 
First, many real world data are not linearly separable, where the solution to Definition~\ref{hardsvmprimal} does not always exist.
Secondly, the data usually come with noises and errors and we do not want the resulting classifier to over-fit the training data.
Therefore, we relax the constraints by introducing a \textit{margin slack} parameter $\xi_i$ for each training example $x_i$ and rewrite the constraints as 
\begin{align}
	y_i(\ip{\vw}{\varphib(\vx_i)} + b) \ge 1-\xi_i, \xi_i\ge 0, \forall i\in\{1,\cdots,m\}. \label{softsvmconstraint}
\end{align}
The margin slack parameter $\xi_i$ will allow example with margin less than $1$ (the violation of the constraints).
In particular, with $\xi_i=0$ the data point $x_i$ is correctly classified, and lies either on the margin or on the correct side.
With $0<\xi_i\le 1$ the data point is correctly classified and lies between margin and separating hyperplane.
With $\xi_i>1$ the data point is misclassified locating on the other side of the hyperplane.
With margin slack parameter, the new goal is to maximize the margin while penalizing the data points that either lies on the wrong side of the hyperplane or has margin less than one.
This can be defined by
\begin{definition}{Primal Soft-Margin \svm\ Optimization Problem.}\label{softsvmprimal}
	\begin{align*}
		\underset{\vw,\xi}{\minimize} & \quad\frac{1}{2}\norm{\vw}^2 + C\sum_{i=1}^{m}\xi_i\\
		\st & \quad y_i(\ip{\vw}{\varphib(\vx_i)} + b) \ge 1-\xi_i, \xi_i\ge 0, \forall i\in\{1,\cdots,m\}.
	\end{align*}
\end{definition}

% dual, kernel svm
Optimization problem is usually transformed into dual form by introducing for each constraint a \textit{Lagrangian multiplier} (dual variable) $\alpha$.
We obtain the following dual optimization problem
\begin{definition}{Dual Soft-Margin \svm\ Optimization Problem.}\label{softsvmdual}
	\begin{align*}
		\underset{\valpha}{\maximize} & \quad\sum_{i=1}^{m}\alpha_i -\frac{1}{2}\sum_{i=1}^{m}\sum_{i=1}^{m}\alpha_i\alpha_jy_iy_j\ip{\varphib(\vx_i)}{\varphib(\vx_j)}\\
		\st & \quad \sum_{i=1}^{m}\alpha_iy_i=0, \, 0\le\alpha_i\le C, \, \forall i\in\{1,\cdots,m\}.
	\end{align*}
\end{definition}
It is not difficult to verify that according to KKT condition only examples with both $\xi_i=0$ and satisfying equality constraints in (\ref{softsvmconstraint}) will be ``active'' and have dual variable $\alpha_i>0$.
These training examples lie on the margin and have $\gamma_i=1$.
They are called \textit{support vectors} in \svm\ optimization.
In fact the number of support vectors is usually smaller than the size of the training set.
The property is extremely useful, as the weight vector can be expressed as the linear combination of the training examples
\begin{align*}
	\vw = \sum_{i=1}^{m}\alpha_iy_i\varphib(\vx_i).
\end{align*}
As most of the dual variables are zero, the evaluation can be done efficiently by maintaining a small set of non-zero $\alpha$s.

In addition, we notice that to work out (Definition~\ref{softsvmdual}) it is not necessary to work on the explicit representation of $\vx$, as only the value of the inner product $\ip{\varphib(\vx_i)}{\varphib(\vx_j)}$ is needed.
As the algorithm can be represented entirely in terms of the inner product, we can also replace the inner product with $\ip{\varphib(\vx_i)}{\varphib(\vx_j)}$.
In particular, we define the \textit{kernel}
\begin{align}
	K(\vx_i,\vx_j) = \ip{\varphib(\vx_i)}{\varphib(\vx_j)}, \label{kernel}
\end{align}
where $\varphib(\cdot)$ is some high-dimensional feature map.
Whenever we need $\ip{\vx_i}{\vx_j}$, we instead use $K(\vx_i,\vx_j)$.
Besides, the property of kernel function \citep{Scholkopf02learning} allows us to compute (\ref{kernel}) without explicitly accessing the high dimensional feature map.
Thus the algorithm can tackle high dimensionality by learning from $\varphib(\cdot)$ at a low computational cost.

In addition, according to \textit{Mercer's Theorem} \citep{taylor04}, every positive semidefinite symmetric function is a kernel.
Kernel functions that heavily used in practice include the \textit{linear kernel}
\begin{align*}
	K(\vx_i,\vx_j) = \ip{\varphib(\vx_i)}{\varphib(\vx_j)},
\end{align*}
the \textit{polynomial kernel}
\begin{align*}
	K(\vx_i,\vx_j) = (\ip{\varphib(\vx_i)}{\varphib(\vx_j)} + b)^k,
\end{align*}
where $b$ is the bias term and $d$ is the degree of polynomial, and the \textit{Gaussian kernel} (\rbf)
\begin{align*}
	K(\vx_i,\vx_j) = \exp{\left(\frac{\norm{{\varphib(\vx_i)}-{\varphib(\vx_j)}}^2}{2\sigma^2}\right)},
\end{align*}
where $\sigma$ is the Gaussian width parameter.

% optimization of support vector machine,, original approach
The optimization techniques for solving \svm\ have been intensively studied.
A ``chunking'' method was developed in \citep{Vapnik82estimation} which takes the advantage that the \svm\ solution only depends on the nonzero Lagrangian multipliers.
The method breaks down the original optimization problem into a set of smaller problems, and in each iteration solves a small problem which contains the nonzero Lagrangian multipliers from last step and a fixed number of worst examples that violate the constraints.
Following the similar idea, \citet{Osuna97an} proposed a decomposition strategy to solve \svm\ on very large dataset.
The algorithm decomposes the dual variables into an active part (working set) and an inactive part.
In each iteration, the algorithm updates the variables in working set while keeps the rest unchanged by solving the smaller optimization problem arising from the working set.
It adds and deletes the same amount of examples in the working set to maintain a constant memory.
It gains advantage of linear memory requirement with respect to the number of support vectors and the size of the working set by compromising on the convergence time.
\citep{Joachims98making} has improved this decomposition strategy by applying a shrinking technique which will identify the dual variables that is less likely to change their values.
Thus, the algorithm effectively maintains a good working set that makes much progress towards the object.
The implementation of the algorithm is well-known as \svmlight.
The decomposition strategy was further developed by solving in each iteration a smallest as possible optimization that only contains two Lagrangian multipliers, which is known as \textit{Sequential Minimum Optimization} (\smo) \citep{Platt98sequential,Platt99fast}.
\smo\ is the basis of another powerful software package \libsvm\ developed by \citet{Chang11libsvm}.
\citet{Perezcruz04double} proposed a double-chunking method to further speed up the decomposition type of training. 
Similarly yet another approach ``digesting'' \citep{Decoste02support} optimizes subsets close to completion before adding new data, which saves huge amount of memory.

Another line of research to speed up \svm\ training is to divide the original problem into subproblems of smaller size, optimize \svm\ on each subproblem, and combine the results from each subproblem.
In \citep{Tresp00a,Schwaighofer01the}, the authors proposed a Bayesian scheme to combine the models from subproblems.
\citet{Collobert02a} proposed a partition scheme.
\citet{Graf05parallel} developed the \svmcascade\ that builds a partition tree of training examples, trains \svm\ on each partition, and returns only support vector from each partition.
Quite recently, \citet{Hsieh14a} proposed a divide and conquer solver that solves the original \svm\ problem with theoretical guarantee.

There are many other active lines of research on \svm\ optimization that aim to make kernel support vector machine scalable on very large scale datasets in terms of both computational time and memory space, to name but a few, representing the training data with a small set of landmark points \citep{Pavlov00towards,Boley04training,Yu05making,Zhang08improved}, greedy method for basis selection \citep{Keerthi06building}, online \svm\ solver \citep{Bordes05fast}, approximating kernel \svm\ objective function \citep{Zhang12scaling, Le13fast} which avoids high computational and memory cost but struggles in prediction performance, and approximating kernel matrix with low-rank matrix \citep{Smola00sparse,Fine02efficient,Drineas05on,Si14memory}.



%------------------------------------------------
%
%
%
%------------------------------------------------
\section{Ensemble Methods} \label{sc_em}

Ensemble methods are one of the prominent classification techniques widely used in machine learning community.
In general, the methods train multiple base classifiers and combine them in order to achiever better classification performance.
Most importantly, there is no requirement for base classifiers to be accuracy as long as they perform better than random guessing.
%To trace the exact originality of the ensemble methods seems impossible, as the methods involve along well with human society by which it is wiser to take in to account advises from multiple human experts.
There are several different types of ensemble algorithms, to name a few, bagging \citep{Breiman96bagging}, boosting \citep{Freund97a,Schapire99improved}, stacking \citep{Smyth99linearly}, Bayesian averaging \citep{Freund04generalization}.
Most of the algorithms have improved the classification performance when compared to their base classifier counterpart, some of them also enjoyed theoretical guarantee \citep{Schapire97boosting,Koltchinskii00empirical,Cortes14semble,Cortes14deep}.
For our purposes, the section will start with and focus on bagging and boosting ensemble since both methods are best known approaches and are most related to this thesis.
In the later part of the section, we will extend the ensemble methods to structured output space by reviewing several recently established algorithms.


\subsection{Preliminary and Notations} \label{sc_em_pn}

In addition to the notations introduced in Section~\ref{singlelabel_preliminary} and \ref{multilabel_preliminary}, we assume there is a hypothesis class $\Fcal$ where we generate weak/base hypothesis $f^t(x)\in\Fcal$.
By $t$ we intend to index the $t$'th weak hypothesis.
We denote by $H(x)$ the ensemble framework that is able to combine multiple weak hypotheses and generate a stronger one.
In many cases, no other information about $f^t(x)$ is available to $H(x)$ except that each weak hypothesis will take in as parameter $x$ and output $y\in\Ycal$ for single label classification ($\vy\in\vYcal$ in multilabel classification respectively).

\subsection{Boosting} \label{sc_boosting}

\textit{Boosting} corresponds to a learning framework or a family of algorithms that take in a weak classifier and tuning it into a strong one.
We begin our discussion from the notion of  \textit{concept class}.
A \textit{concept} is a boolean function over domain $\vXcal$, and a \textit{concept class} is a class of concepts.
% TODO: an illustrative example
A concept class is \textit{strongly learnable} if there exists a polynomial learning algorithm that achieves high accuracy with high probability for all concepts in the class.
On the other hand, a concept class is \textit{weakly learnable} if the learning algorithm achieves arbitrary high accuracy where the only requirement is that the learning algorithm finds a function that performs better than coin flip.
The concept of learnability were proposed in \citep{Kearns94cryptographic} together with the question whether the strong learnability and the weak learnability are equivalent which is known as \textit{hypothesis boosting problem}.
Finding a weak learner which performs better than random guessing is easy in practice, but finding a strong learner is usually difficult.
\citet{Schapire90the} has proved that two classes of learnability are equal which lays the foundation of the boosting algorithm that aim to convert a weak learning algorithm into a strong one that achieves high performance.

Adaptive Boosting (\adaboost) developed by \citet{Freund97a} is the very first practical boosting algorithm and is the most influential one.
In addition, \citet{Schapire99improved} proposed a variant of the algorithm which updates the adaptive parameters in order to minimize the exponential loss of the weak learners.
The exact algorithm is shown in (Algorithm~\ref{adaboost}).
The fundamental idea of \adaboost\ is to maintain a distribution $D$ over all training examples, and update the distribution in each iteration such that difficult-to-classify examples will get more probability mass for the next iteration (line~\ref{update_d}).
In particular, the algorithm computes in each iteration a weak learner $f^t(\vx)$ base on all training examples and the current distribution $D^{t}$ (line~\ref{compuet_f}), calculates the weighted training error $\epsilon^k$ for the current weak learner (line~\ref{compute_error}), and computes the adaptive parameter $\alpha^t$ for the current weak learner (line~\ref{compute_alpha}).
The ensemble is the weighted combination of all weak learners.
\begin{align*}
	H(\vx) = \sign(\sum_{t=1}^T\alpha^tf^t(\vx)).
\end{align*}
\begin{algorithm}
\caption{\adaboost}
\label{adaboost}
\begin{algorithmic}[1]
	\REQUIRE Training sample $\Scal = \{(x_i,\vy_i)\}_{i=1}^m$, learning function $\Wcal$, number of weak learners $T$
	\ENSURE Boosting ensemble $H(\vx)$
	\STATE Initialize $D^0(i)=\frac{1}{m},i\in\{1,\cdots,m\}$
	\FOR{$t=1\cdots T$}
		\STATE $f^t(\vx)\leftarrow\Wcal(\Scal,D^{i-1})$ \label{compuet_f}
		\STATE $y_i = f^t(\vx_i),\forall i\in\{1,\cdots,m\}$
		\STATE $\epsilon^t = \sum_{i=1}^{m}D^t(i)\ind{y_i\neq\hat{y}_i}$ \label{compute_error}
		\STATE $\alpha^{t} = \frac{1}{2}\ln\left(\frac{1-\epsilon^t}{\epsilon^t}\right)$ \label{compute_alpha}
		\STATE $Z=\sum_{i=1}^{m}D^{t-1}(i)\exp(-\alpha^ty_i\hat{y}_i)$
		\STATE $D^t(i) = \frac{1}{Z}D^{t-1}(i)\exp(-\alpha^ty_i\hat{y}_i),\forall i\in\{1,\cdots,m\}$ \label{update_d}
	\ENDFOR
	\RETURN $H(\vx) = \sign(\sum_{t=1}^T\alpha^tf^t(\vx))$
\end{algorithmic}
\end{algorithm}

For each weak learner $f^t(\vx)$, the strategy of updating adaptive parameter $\alpha^t$ is to ensure that the exponential loss of $\alpha^tf^t(\vx)$ is minimized.
To see this, we first compute the exponential loss for $\alpha^tf^t(\vx)$ given current distribution $D^t$ over training example and adaptive parameter for $f^t(\vx)$
\begin{align*}
	\ell_{exp}&(\alpha^t,f^t(\vx),D^t) \\
	&= \sum_{i=1}^mD^t(i)\exp(-\hatf(\vx_i)\alpha^tf^t(\vx_i))\\
	&=\exp(-\alpha^t)\sum_{i=1}^mD^t(i)\ind{\hatf(\vx_i)=f(\vx_i)} + \exp(\alpha^t)\sum_{i=1}^mD^t(i)\ind{\hatf(\vx_i)\neq f(\vx_i)}\\
	&=\exp(-\alpha^t)(1-\epsilon^t) + \exp(\alpha^t)\epsilon^t.
\end{align*}
To minimize $\ell_{exp}(\alpha^t,f^t(\vx),D^t)$, we take partial derivative with respect to $\alpha^t$ and set it to zero
\begin{align*}
	\frac{\partial\ell_{\exp}(\alpha^t,f^t(\vx),D^t)}{\partial\alpha^t} = -\exp(-\alpha^t)(1-\epsilon^t) + \exp(\alpha^t)\epsilon^t=0
\end{align*}
Solve it for $\alpha^t$, we get
\begin{align*}
	\alpha^{t} = \frac{1}{2}\ln\left(\frac{1-\epsilon^t}{\epsilon^t}\right).
\end{align*}
Thus the rule for updating $\alpha^t$ will make sure the exponential loss of $\alpha^tf^t(\vx)$ will be minimized.

It is worth noticing that the \adaboost\ algorithm describe in (algorithm~\ref{adaboost}) requires the learning algorithm $\Wcal$ capable of learning from some specific distribution.
The distribution will reflect the difficulty of the training examples during the learning.
It is usually achieved by \textit{reweighing} which initials a uniform distribution over training examples and updates the distribution after each iteration.
For learning algorithms that cannot handle training data with specific distribution, we can apply \textit{resampling} which samples training examples in each iteration according to some desired distribution.

Most recently, a new boosting algorithm \deepboosting\ was developed in \citep{Cortes14deep}.
\deepboosting\ allows the learning algorithm to use complex hypothesis class contain very deep decision trees, by which it extends \adaboost\ that only uses simple hypothesis class of decision tree with depth one.
In addition, \citet{Cortes14deep} also developed rich learning bounds for \deepboosting\ optimization problem in terms of the Rademacher complexity theory.
The new theory advances the previous performance guarantee of \adaboost\ which is build in terms of margins of the training examples \citep{Schapire97boosting,Koltchinskii00empirical}.


\subsection{Bootstrap Aggregating} \label{sc_ba}

\textit{Bootstrap Aggregating} (\bagging), initially developed by \citet{Breiman96bagging}, is an ensemble method that exploits the independency between the weak learners.
The algorithm is based on the fact that the error can be dramatically reduced by combining independent base learners.
Denote by $f_i$ the $i$'th weak learner.
The ensemble prediction $H(\vx)$ is the averaged prediction over all weak learners
\begin{align}
	H(\vx) = \sign\left(\sum_{i=1}^{T}f_i(\vx)\right). \label{bagging_ensemble}
\end{align}
Denote by $\hatf$ the ground true function.
Suppose base learner has probability $\epsilon$ of making independent mistake
\begin{align*}
	P(f_i(\vx) \ne \hatf(\vx)) = \epsilon.
\end{align*}
As the bagging ensemble (\ref{bagging_ensemble}) makes a mistake when at least half of the weak learners make mistakes, we can compute the probability of bagging ensemble making mistake by
\begin{align*}
	P(H(\vx)\neq\hatf(\vx)) = \sum_{i=0}^{T/2}\left(\begin{tabular}{cc}T\\k\end{tabular}\right)(1-\epsilon)^k\epsilon^{T-k} \le\exp\left(-\frac{1}{2}T(2\epsilon-1)^2\right).
\end{align*}
It is clear that the probability decreases exponentially with respect to the number of the weak learners.
In extreme case, the probability will approach zero when $T$ approaches infinity.
Though it is impractical as the base learners are hardly independent since they are generated from the same training data, we still try to exploit the independency by adding randomness in the algorithm framework.

\bagging\ uses bootstrap sampling \citep{Efron1994introduction} to generate subsets of training examples.
Given a training set of $m$ training examples, a subset of same size is generated by sampling with replacement $m$ time from original training set.
The sampling procedure is then repeated $T$ time in order to generate $T$ subsets for constructing base learners.
Note that the sampled subsets will be similar as they are sampled from the same training set.
However, they will not be too similar which each subset will only cover around $63\%$ of the origin training data under the condition that $m$ is large.
To see this, consider the probability that the $i$'th training examples is not sampled once is $(1-\frac{1}{m})$, and the probability that the training example is not sampled at all is $(1-\frac{1}{m})^m$.
When $m$ is large, the probability will approach $37\%$. 
That is around $37\%$ of the training examples will not be represented in any given subset.
The property of the bootstrap sampling allows us to efficiently estimate the generalization error of the base learner known as \textit{out-of-bag estimation} \citep{Breiman96out,Tibshirani1996bias,Wolpert99an}.

Random Forest developed in \citep{Breiman01random} is an important extension of \bagging.
The major different from \bagging\ is that randomized feature selection is incorporated in random forest.
In practice, the random forest algorithm selects in each step a subset of features and constructs conventional random tree classifier within the selected subset of features.
The feature split process of decision tree is deterministic given a set of features.
\citet{Liu08spectrum} proposed \vrtree\ which further extend random forest algorithm by introducing randomness in both feature selection and split processes.





%------------------------------------------------
%
%	chapter
%
%------------------------------------------------
\chapter{Flat Multilabel Classification} \label{ch_fmlc}

Multilabel classification is a natural extension to single-label classification presented in Section~\ref{sc_slc}.
In multilabel classification each input (instant) is simultaneously associated with multiple outputs (labels).
The research of the multilabel classification has progressed rapidly in the past two decades with many learning algorithms being developed and applied in real world classification tasks.
In general, there are two lines of research for multilabel classification namely flat multilabel classification and structured output prediction.
In flat multilabel classification, the multiple labels are essentially treated as a ``flat'' vector of labels.
The structured output prediction, on the other hand, models the multiple labels via a graph structure that connects labels.
This chapter will devote to flat multilabel classification in which we will present several classical algorithm techniques.
The structured output prediction will be covered in the later part of the thesis.

Consider the infeasibility of listing all algorithms for flat multilabel classification, it is easier if we are able to categorize the algorithms into classes.
In this chapter, we adopt the simple categorization presented in \citep{Tsoumakas07multi,Tsoumakas10mining} which gives us two major algorithm categories namely problem transformation and algorithm adaptation.
For problem transformation category, we will present the methods that aim to transfer the flat multilabel classification problem into other well studied problem settings (e.g. single-label classification problem, ranking problem).
For algorithm adaptation category, we will describe the methods that directly modify the popular learning techniques (e.g. \lr\ and ensemble method) to solve the multilabel classification problem.
Nevertheless, the present algorithms will explore the inherited difficulties of flat multilabel classification.
That is the exponential multilabel space and the implicit label correlation.
Besides, the author realizes that it is impossible to cover every lines of research in the field of flat multilabel classification.
Interested readers are pointed out to the recent research review articles, for example, \citep{Tsoumakas07multi,Tsoumakas10mining,Zhang14a}



%------------------------------------------------
%	section
%------------------------------------------------
\section{Preliminary and Notations} \label{sc_mlc_pn}

We borrow most notations from the single-label classification setting described in Section~\ref{sc_slc_pn}.
In particular, we examine the following \textit{multilabel classification} problem.
We assume data are drawn from a domain $\vXcal\times\vYcal$, where $\vXcal$ is a set of inputs (instants) and $\vYcal$ is the set of outputs (multilabels).
$\vYcal = \Ycal_1\times\cdots\times\Ycal_k$ is composed by a Cartesian product of $k$ sets $\Ycal_i=\{-1,+1\}$.
We retain a single-label classification setting by letting $k=1$.
A vector $\vy=(y_1,\cdots,y_k)\in\vYcal$ is called \textit{multilabel} and its element $y_j$ is called \textit{microlabel}.
We use $\vy_i[j]$ to denote the $j$'th microlabel in the $i$'th multilabel.
In addition, we are given a training set consisting of $m$ labeled examples $\Scal=\{(\vx_i,\vy_i)\}_{i=1}^{m}\in\vXcal\times\vYcal$. 
A pair $(\vx_i,\vy)$, where $\vx_i$ is the training input and $\vy\in\vYcal$ is an arbitrary output, is called \textit{pseudo-example}.
It is worthy of noticing that pseudo-example $(\vx_i,\vy)$ can be generated from a different distribution that generates training examples $(\vx_i,\vy_i)$.
The goal is to learn a mapping function $f\in\Hcal:\vXcal\rightarrow\vYcal$ that can compute the multilabel given an arbitrary input such that the predifined loss $\ell$ for the future unseen examples will be minimized.
The loss function is usually tailored for the learning algorithm.



%
%
\section{Problem Transformation}

The problem transformation approach aims to transform the flat multilabel classification problem in to other well established problem settings.
The most typical way is known as \textit{binary relevant} proposed by \citet{Tsoumakas07multi,Tsoumakas10mining}.
\br\ amounts to transform the multilabel classification problem into a set of single-label classification problems and to independently learn a single-label classifier for each microlabel.
We will focus on \br\ in this section.
Another possibilities include, for example, to transform the multiple labels into label power set \citep{Tsoumakas07random} or to transform the problem into label ranking problem \citep{Elisseeff01akernel,Brinker07case,Furnkranz08multilabel,Chiang12a}.
However, learning through label ranking will not be discussed into details as it slightly diverges from the main scope of the thesis.



%
%
\subsection{Multilabel $K$-Nearest Neighbors (\mlknn)} \label{sc_mlknn}

Multilabel $K$-Nearest Neightbors (\mlknn) proposed by \citet{Zhang05a,Zhang07mlknn} is perhaps the most famous binary relevant classifier for flat multilabel classification.
\mlknn\ is also an instance based learning approach \citep{Aha91instance} that is derived from $K$-Nearest Neightbor (\knn) algorithm designed for single-label classification problem.
\mlknn\ transforms the flat multilabel classification problem into a set of single-label classification and processes each microlabel independently.
For each unseen example $\vx$, the \mlknn\ algorithm first identifies the set of $K$-nearest neighbors $N(\vx)$ from the training sets.
After that the algorithm computes the multilabel $\vy$ of the unseen example $\vx$ by examining the set of labels collected from its $K$-nearest neightbors.

Mathematically 
let $C_{\vx}(j)$ denote the number of the neighbors of $\vx$ with $j$'th label being ``$+1$'',
let $H^b_{\vx}(j)$ denote the event that the $j$'th label of $\vx$ is $b\in\Ycal_j$,
and let $E_{\vx}^l(j)$ denote the event that $0\le l \le K$ neighbors of $x$ have the $j$'th label being ``$+1$''.
\mlknn\ processes each microlabel at a time and determines the value of the microlabel by examining the following Maximize A-Posteriori (\map) problem
\begin{align*}
	\vy[j]^* = \underset{b\in\Ycal_j}{\argmax} \, P( H^b_{\vx}(j) | E_{\vx}^{C_{\vx(j)}}(j) )
	= \underset{b\in\Ycal_j}{\argmax} \, \frac{P( H^b_{\vx}(j) ) P(  E_{\vx}^{C_{\vx(j)}}(j) ) | H^b_{\vx}(j))}{P( E_{\vx}^{C_{\vx(j)}}(j))}.
\end{align*}
The prior probability distribution $H^b_{\vx}(j)$ and the likelihood distribution $P( H^b_{\vx}(j) | E_{\vx}^{C_{\vx(j)}}(j) )$ can be estimated from training data in terms of corresponding relative frequencies.

The central problem with \mlknn\ is that \mlknn\ is ignorant of the label correlations.
In order to exploit the label dependency, other approaches have been proposed \citep{Cheng09combining,Younes11a}.
Besides, there are many algorithm alternatives that also align to the direction of \knn\ typed learning in flat multilabel classification, for example, \citet{Brinker07case,Chiang12a} proposed methods that incorporates \knn\ with ranking model.


%
%
\subsection{Classifier Chains (\cc)}

Classification Chains (\cc) developed in \citep{Read09classifier,Read11classifier} is another problem transformation approach for flat multilabel classification.
\cc\ involves $k$ binary transformations and forms a chain of $k$ binary classifiers $h=\{h_1,\cdots,h_k\}$.
The $j$'th classifier $h_j$ is responsible for learning and predicting the $j$'th microlabel.
In particular, for each microlabel $\vy[j]$, \cc\ first constructs a new training data $S_j$ then learns a classifier $h_j$ via arbitrary single-label classifier.
$S_j$ is built by taking the $j$'th microlabel as output variable and combing the feature space of $\vx$ and all $j-1$ prior microlabels as the input feature which is defined by
\begin{align*}
	S_j = \{((\vx_i[1],\cdots,\vx_i[d],\vy_i[1],\cdots,\vy_i[j-1]),\vy_i[j])\}_{i=1}^{m}.
\end{align*} 
Thus, \cc\ takes the label correlation into account by incorporating label information as concatenated features in the input feature space.
The idea has previously been applied in \citep{Godbole04discriminative}.
However, the addition label information only takes a small part of the input feature space especially when the origin space is already high.
In this case, one can only expect to gain additional performance from concatenated label information with high correlation to the output microlabel.

Besides, \cc\ has been explained in terms of condition probability theory by Probabilistic Classifier Chains (\pcc) presented in \citep{Read09classifier,Dembczynski10bayes}.
Since there is no standard way for classification order of microlabels in \cc, Ensemble Classifier Chains (\ecc) has been developed in \citep{Read11classifier} which further improves \cc\ by generating and combining multiple chains of classifiers. 


%
%
\subsection{Instant Based Logistic Regression (\iblr)}

\citet{Cheng09combining} developed Instance Based Logistic Regression (\iblr) with an extension to flat multilabel classification.
\iblr\ is also an instant base learning approach \citep{Aha91instance} as \mlknn.
It extends \mlknn\ by exploring the label correlation within the neighbors of an instant for posterior inference.
It is also similar to \cc\ but with major difference.
The central idea of \iblr\ is to take the labels of the neighboring examples as the only features in order to evaluate the label of current example.
Similar idea has been applied in collective classification \citep{Ghamrawi05collective} and link based classification \citep{Getoor05link, Getoor07introduction}.

In particular, for each unseen example $\vx$, \iblr\ first identifies the set of $K$-nearest neighbors $N_k(\vx)$ from the training sets.
Then the algorithm builds a logistic regression (\lr) model (\ref{sc_lr}) for each microlabel based on the label information collected from all training examples in $N_k(\vx)$.
Mathematically \iblr\ defines a posterior probability for the $j$'th microlabel of $\vx$ being labeled as $u_j$ by
\begin{align*}
	\pi^{(j)} = P( \vy(\vx)[j]=u_j | N_k(\vx)), u_j\in\Ycal_j.
\end{align*}
It constructs a logistic regression classifier for $\pi^{(j)}$ which can be derived from the following 
\begin{align*}
	\log\frac{\pi^{(j)}}{1-\pi^{(j)}} = w_0^{(j)} + \sum_{i=1}^{k}\alpha_i^{(j)}\cdot w_{i}^{(j)}(\vx),
\end{align*}
where $i$ is the index that iterates over all microlabels.
$w_i^{(j)}(\vx)$ defined by 
\begin{align*}
	w_i^{(j)}(\vx) = \sum_{\vx'\in N_k(\vx)}K(\vx',\vx)\cdot \vy(\vx')[i]
\end{align*}
collects the $i'$th microlabel from each neighbor $\vx'\in N_k(\vx)$ and weights the microlabel according to the similarity between $\vx$ and $\vx'$ encoded in $K(\vx',\vx)$.
$\alpha_i^{(j)}$ is the regression coefficient of the $i$'th microlabel information from the neighbors of $\vx$ to the $j$'th microlabel of example $\vx$.
$w_0^{(j)}$ is a bias term.



%
%
\section{Algorithm Adaptation}

The central idea of the algorithm adaptation approach is to directly modify the popular single-label learning algorithms and apply them to the multilabel classification problems.
The approach includes but is not limited to the ensemble typed learning, the \lr\ typed learning, and regularized learning that are described in the following part of the section.
There also exists methods that are adapted from, for example, perceptron \citep{Crammer03afamily} and neural network \citep{Zhang06multilabel}.
They are not detailed mostly due to the divergence from the main topic of the thesis.



%
%
\subsection{Ensemble Methods for Flat Multilabel Classification} \label{sc_emfmlc}

As it is straightforward to combine multiple scalar output variables, the ensemble methods have been initially developed for single-label classification \citep{Breiman96bagging,Freund97a} or regression  \citep{Breiman96bagging} tasks.
However, it is not immediately apparent how to combine vector valued outputs in flat multilabel space.

\adaboostmh\ is the multilabel variance of original \adaboost\ algorithm proposed by \citet{Schapire99improved}, which is further developed by \citet{Esuli2008boosting}.
The core idea of \adaboostmh\ is to adopt hamming loss instead of 0/1 loss. 
In particular, the algorithm reduces from a multilabel classification problem to a set of single label classification problems by replacing each training example $(\vx_i,\vy_i)$ with $k$ examples $\{(\vx_i,\vy_i[l])\}_{l=1}^k$.
The algorithm is detailed in (Algorithm~\ref{adaboostmh}).
In particular, it maintains a distribution over all examples and labels.
In each iteration, the algorithm takes in the distribution and all training examples, generates a weak learner $f^k(\vx)$ (line~\ref{mh_compuet_f}), computes hamming loss (line~\ref{mh_compute_error}), computes the adaptive parameter $\alpha^t$ (line~\ref{mh_compute_alpha}), and update the distribution (line~\ref{mh_update_d}).
The returned prediction $H(x)$ is the combination of base learners $f^t(x)$ weighted by $\alpha^t$.
\begin{algorithm}
\caption{\adaboostmh}
\label{adaboostmh}
\begin{algorithmic}[1]
	\REQUIRE Training sample $\Scal = \{(x_i,\vy_i)\}_{i=1}^m$, learning function $\Wcal$, number of weak learners $T$
	\ENSURE Boosting ensemble $H(\vx)$
	\STATE Initialize $D^0(i,l)=\frac{1}{mk},i\in\{1,\cdots,m\}$
	\FOR{$t=1\cdots T$}
		\STATE $f^t(\vx)\leftarrow\Wcal(\Scal,D^{i-1})$ \label{mh_compuet_f}
		\STATE $\vy_i = f^t(\vx_i),\forall i\in\{1,\cdots,m\}$
		\STATE $\epsilon^t =\sum_{l=1}^k\sum_{i=1}^{m}D^t(i,l)\ind{\vy_i[l]\neq\hat{\vy}_i[l]}$ \label{mh_compute_error}
		\STATE $\alpha^{t} = \frac{1}{2}\ln\left(\frac{1-\epsilon^t}{\epsilon^t}\right)$ \label{mh_compute_alpha}
		\STATE $Z=\sum_{i=1}^{m}\sum_{l=1}^{k}D^{t-1}(i,l)\exp(-\alpha^t\vy_i[l]\hat{\vy}_i[l])$
		\STATE $D^t(i,l) = \frac{1}{Z}D^{t-1}(i,l)\exp(-\alpha^t\vy_i[l]\hat{\vy}_i[l]),\forall i,\forall l$ \label{mh_update_d}
	\ENDFOR
	\RETURN $H(\vx) = \sign(\sum_{t=1}^T\alpha^tf^t(\vx))$
\end{algorithmic}
\end{algorithm}

Other ensemble flat multilabel classification methods based on boosting or bagging are also developed in \citep{Wang07simple,Yan07model,Kocev13tree}.
Most recently, three other learning algorithms with performance guarantee are proposed in \citep{Cortes14semble}.
Besides, there are quite a few ensemble approaches that are adapted from single label ensemble methods and are dedicated to solve real world classification problems.
For example, \citep{Collins05distrimnative,Zeman05improving,Sagae06parsing,Zhang09kbest} are applied in natural language processing, and \citep{Fiscus97a,Benesty08speech,Petrov10products}
are applied in text and speech recognition.


%
%
\subsection{Correlated Logistic Regression (\corrlog)}

Correlated Logistic Regression (\corrlog) developed by \citet{Bian12corrlog} is a model based approach for flat multilabel classification.
\corrlog\ is a major step forward of \iblr\ by constructing a logistic regression classifier over all microlabels and by modeling pairwise microlabel correlation with a function defined on microlabel pairs.

In fact, \corrlog\ is derived from Independent Logistic Regression (\ilrs) model. 
Given an arbitrary training example and label $(\vx,\vy)$, we can construct a set of independent logistic regreesion classifiers, one for each microlabel.
The posteriori probability can be computed by
\begin{align}
	P_{\text{\ilrs}}(\vy|\vx) 
	= \prod_{j=1}^{k}P_{\text{\lr}}(\vy[j]|\vx)
	= \prod_{j=1}^{k}\frac{\exp(\vy[j]\vw^{\tp}\vx)}{\exp(\vw^{\tp}\vx)+\exp(-\vw^{\tp}\vx)}, \label{ilrs}
\end{align}
where $j$ is the index that iterates over all microlabels. 
The bias term as in Definition~\ref{logistic_regression} is omitted which is equvilent to augment $\vx$ with a constant term \citep{Bian12corrlog}.
Otherwise, (\ref{ilrs}) can be derived into Definition~\ref{logistic_regression} by replacing $\vw$ with $\frac{\vw}{2}$.
\ilrs\ has the problem of ignoring the label correlation and overfitting the training data when the number of microlabel $k$ is high.
To reveal label correlation, \corrlog\ augments the posteriori probability (\ref{ilrs}) by a function $Q(\vy)$ defined on the set of microlabels as
\begin{align}
	Q(\vy) = \exp\left\{\sum_{k<j}\alpha_{k,j}\vy[k]\vy[j]\right\}. \label{ilrs_augment}
\end{align}
Put together (\ref{ilrs}) and (\ref{ilrs_augment}), the formalism of \corrlog\ can be defined as
\begin{align*}
	P_{\text{\corrlog}}(\vy|\vx) &\propto P_{\text{\ilrs}}(\vy|\vx) Q(\vy) \\
		&=\exp\left\{ \sum_{j=1}^{m}\vy(x)[j]\vw^{\tp}\vx + \sum_{k<j}\alpha_{kj}\vy(\vx)[k]\vy(\vx)[j] \right\}.
\end{align*}
Thus, \corrlog\ examines the pairwise label correlation by augmenting joint prediction with a quadratic term $Q(\vy)$ built from microlabel pairs.



%
%
\subsection{Multitask Feature Learning (\mtl)} \label{sc_mtl}

Multitask Feature Learning (\mtl) developed in \citep{Argyriou07multitask} is another algorithm designed for flat multilabel classification.
\mtl\ is quite different from other learning algorithms discussed in the previous sections.
In particular, the \mtl\ approach is based on the assumption that different microlabels are related such that they share a common underlying feature representations.
Similar assumption is also applied in \citep{Caruana97multitask,Baxter00a,BenDavide03exploiting}.

Denote by $f_t(\vx)$ the task specific function for the $t$'th task.
$f_t(\vx)$ can be expressed as
\begin{align*}
	f_t(\vx) = \ip{\va_t}{h(\phib(\vx))} = \sum_{i=1}^{d}\va_{t}[i]h(\phib(\vx))[i],
\end{align*}
where $\va_t\in\RR^d$ is the weight vector parameters for the $t$'th task.
$h(\phib(\vx))$ is a linear feature mapping composed by
\begin{align*}
	h(\phib(\vx))[i] = \ip{\vu_i}{\phib(\vx)},
\end{align*}
where $\phib(\vx)\in\RR^d$ is in original feature space and $\vu_i\in\RR^{d}$ is a linear feature map.
We further denote by $A$ the matrix composed by $\va_{t}$, and denote by $U$ the matrix composed by $\vu_i$.
\mtl\ assumes that tasks share a small set of features where $A$ is supposed to be sparse with many entries equal to zero.
The optimization problem of \mtl\ is defined as
\begin{definition}{\mtl\ Optimization Problem in Primal}\label{mtl_opt}
	\begin{align*}
		\underset{\substack{\vu\in\RR^{d\times d}\\A\in\RR^{d\times T}}}{\minimize} \left\{\sum_{t=1}^{T}\sum_{i=1}^{m}\ell(\vy_{i,t},\ip{\va_t}{\ip{U}{\phib(\vx_i)}})+C\norm{A}_{2,1}^2\right\},
	\end{align*}
\end{definition}
where the objective is to minimize the empirical error $\ell(\vy_{i,t},\ip{\va_t}{\ip{U}{\phib(\vx_i)}})$ and to keep a small proportion of the non-zero element in $\va_t$.
$C$ is the parameter to balance these two aspects.
As (Definition~\ref{mtl_opt}) is non-convex and the second term is non-smooth, it is transformed into an equivalent form and solved by an alternative optimization approach.

An extension of \mtl\ algorithm was proposed in \citep{Argyriou08convex} which introduced a nonlinear generalization using kernel methods.
In addition, similar but not identical algorithms \citep{Argyriou08an,Jacob09cluster} are also proposed which assume that the tasks form clusters such that task specific weight vectors should be similar within the clusters.
Recently, \citet{Paredes12exploit} proposed a method which exploits the information between unrelated microlabels, based on a similar assumption that microlabels of different groups tend not to share any features.




%------------------------------------------------
%
%	chapter
%
%------------------------------------------------
\chapter{Structured Output Prediction} \label{ch_multi}

Structured output prediction is a natural extension to flat multilabel classification presented in Chapter~\ref{ch_fmlc}.
Unlike flat multilabel classification which takes multiple output variables as a ``flat'' vector, structure output prediction assumes multiple output variables are correlated and locate in the structured output space.
It builds a graph structure (e.g. chain, tree) to connect multiple output variables by which it gains the advantage of exploring the label correlation compared to flat multilabel classification.
In this chapter, we will start by introducing several classical structure output learning methods developed during the last decade.
We will present our research of applying the structured output learning on molecular activity prediction problem.
We will also present our new algorithm \spin\ that is capable of predicting a directed acyclic graph on network influence prediction problem.



%
%
\section{Preliminary and Notations}

Multilabel classification deals with multiple interdependent output variables (e.g. $\vy\in\vYcal$).
The problem is known as \textit{structured output prediction} when these variables are located in structured output space.
That is the label correlation is described by an \textit{output graph} that connects multiple labels.
In particular, we define the output graph $G=(E,V)$ by a node set $V=\{1,\cdots,k\}$ corresponding to the microlabels $\{y_1,\cdots,y_k\}$ and a edge set $E=V\times V$ representing the dependency between pair of microlabels.
For each edge $e=(j,j')\in E$, we use $\vy_e$ to denote the edge label of edge $\ve$ with respect to multilabel $\vy$ by concatenating the head label $y_j$ and the tail label $y_{j'}$, with the edge label domain $\vy_e\in\vYcal_e=\Ycal_j\times\Ycal_{j'}$.
By $\vy_{i,e}$, we denote the edge label of example pair $(\vx_i,\vy_i)$ on edge $\ve$.
Thus, given a training example $(\vx_i,\vy_i)$ and the output graph $G$, we can uniquely identify the node label $y_i$ and the edge label $\vy_{i,e}$ on the graph.
In addition, we denote the possible label of node $i$ by $u_i$ and the possible label of edge $\ve$ by $\vu_{e}$ where $u_i$ and $\vu_{e}$ do not constraint to any multilabel $\vy$.
Naturally, $u_i\in\Ycal_i$ and $\vu_e\in\vYcal_e$.



%
%
\section{Related Methods}

In this section, we will briefly present several related algorithms for structured output prediction include Structured Perceptron, Conditional Random Field, Structured \svm, and Max-Margin Markov Network.



%
%
\subsection{Structured Perceptron}

The Perceptrion developed by \citet{Rosenblatt58} is one of the oldest learning algorithm in machine learning.
Structured Perceptron, as suggested by its name, can be seen as the generalization of Perceptron algorithm to structured output space.
It was firstly proposed in \citep{collins02a, collins02b} with the formalism incredibly similar to multiclass Perceptron. 
The model assumes a score function $\ip{\vw}{\phib(\vx,\vy)}$ as the inner product between a joint feature map $\phib(\vx,\vy)$ and some feature weight parameter $\vw$.
After weight parameter $\vw$ is obtained, one need to solve the \textit{argmax} problem in order to find the best output for a given input $\vx$
\begin{align}
	\hat{\vy} = \underset{\vy\in\vYcal}{\argmax} &\, \ip{\vw}{\phib(\vx,\vy)}.  \label{sp_inference}
\end{align}
This is solved by Viterbi decoding in \citep{collins02a}.

The weight parameter $\vw$ is learned via standard Perceptron iterative update by solving argmax problem (\ref{sp_inference}) in each iteration.
In particular, the algorithm loops through training examples and updates $\vw$ whenever the predicted label $\hat{\vy_i}$ is different from true label $\vy_i$ defined by
\begin{align}
	\vw \leftarrow \vw + \left(\phib(\vx_i,\vy_i) - \phib(\vx_i,\hat{\vy}_i)\right). \label{sp_update}
\end{align}
The update usually leads to over-fitting.
A simple refinement is usually applied the idea of which is also used in ``Average Perceptron'' \citep{Freund99large}.

\iffalse
The update usually leads to over-fitting and a simple refinement, called "average parameter" similar to \citep{Freund99large}, is shown in (Algorithm~\ref{algorithm_structured_perceptron}).
\begin{algorithm}
\caption{Structured Perceptron with Parameter Averaging}
\label{algorithm_structured_perceptron}
\begin{algorithmic}[1]
	\REQUIRE Training sample $\lbrace (\vx_i,\vy_i)\rbrace_{i=1}^m$
	\ENSURE Weight parameter $\vw$
	\STATE $\vw^{t,i}=[0,\cdots,0],\,\forall t\in\{1,\cdots,T\},\,\forall i\in\{1,\cdots,m\}$
	%\STATE $c = 1$
	\FOR{$t=1\cdots T$}
		\FOR{$i=1\cdots m$}
			\STATE $\hat{\vy} = \underset{\vy\in\vYcal}{\argmax} \quad \ip{\vw^{t-1,i-1}}{\phib(\vx_i,\vy)}$
			\IF{$\hat{\vy}\neq\vy$}
				\STATE $\vw^{t,i} = \vw^{t-1,i-1} + \phib(\vx_i,\vy_i) - \phib(\vx_i,\hat{\vy}_i)$
			\ENDIF
			%\STATE $c=c+1$
		\ENDFOR
	\ENDFOR
	\RETURN $\vw = \frac{1}{Tm}\sum_{t=1}^{T}\sum_{i=1}^{m}\vw^{t,i}$
\end{algorithmic}
\end{algorithm}
\fi

The central problem with Structured Perceptron learning is its loss function.
In fact, Structured Perceptron tacitly applies $0/1$ loss over output variables defined as
\begin{align*}
	\vell(\vy,\hat{\vy})=\ind{\vy\neq\hat{\vy}}.
\end{align*}
As a result, it is impossible to distinguish the nearly correct and the completely incorrect output multilabels.
Both will lead to the same update to the feature weight parameter during the training in (\ref{sp_update}) .



%
% 
\subsection{Conditional Random Field (\crf)} \label{sc_crf}

Condition Random Field (\crf) developed in \citep{lafferty01,taskar02} is a discriminative framework that constructs a conditional model $P(\vy|\vx)$ from input variable $\vx\in\vXcal$ and output variables $\vy\in\vYcal$.
It optimizes a log-loss which is analogue to $0/1$ loss over the structured output space.

Mathematically, let $Y=\{\vy_1,\cdots,\vy_m\}$ denote a set of output random variables and $X=\{\vx_1,\cdots,\vx_m\}$ denote a set of input random variables to condition on.
Let $G=(E,V)$ be an output graph such that $\vy = (y_v)_{v\in V}$.
A \textit{Conditional Random Field} (\crf) defines the conditional probability distribution
\begin{align*}
	P(\vy|\vx) &= \frac{1}{Z_{\vx,\vw}}\exp{\ip{\vw}{\phib(\vx,\vy)}},
\end{align*}
where $\phib(\vx,\vy)$ is the joint feature map and $Z_{\vx,\vw}$ is the partition function dependent on $\vx$.
$Z_{\vx,\vw}$ sums over all possible multilabels 
\begin{align}
	Z_{\vx,\vw} &= \sum_{\vy'\in\Ycal}\exp{\ip{\vw}{\phib(\vx,\vy')}}. \label{crf_partition_function}
\end{align}
Thus, when conditioned on $\vx$, the random variables $y_v$ obey the Markov property with respect to output graph $G$.

Apply similar regularization technique as used on \lr\ in Section~\ref{sc_lr}, the feature weight parameter $\vw$ can be solved by introducing a Gaussian prior and maximizing the logarithm of the resulting \textit{Maximize A-Posteriori} (\map) problem \citep{taskar02} defined as
\begin{align}
	L(\vw) = \sum_{i=1}^{m}\left[\ip{\vw}{\phib(\vx_i,\vy_i)}-\log{\sum_{\vy'\in\vYcal}{\exp{\ip{\vw}{\phib(\vx_i,\vy')}}}}\right] - \frac{1}{\sigma^2}\norm{\vw}^2. \label{crf_inference}
\end{align}
The optimization problem derived from (\ref{crf_inference}) is solved in \citep{lafferty01} by Improved Iterative Scaling algorithm (\iis) \citep{Pietra97inducing}.
In order to make \crf\ work in practice, one has to make sure that the partition function (\ref{crf_partition_function}) can be computed efficiently.



%
% 
\subsection{Max-Margin Markov Network (\mmmn)} \label{sc_mmmn}

\citet{Taskar04max} designed Max-Margin Markov Network (\mmmn) that combines the framework of kernel based discriminative learning and probabilistic graphical model.
\mmmn\ is extended from \svm\ (Section~\ref{sc_svm}) to structured output space.
It also improves \crf\ (Section~\ref{sc_crf}) in terms that the evaluation of the partition function (\ref{crf_partition_function}) can be avoided by introducing the odd-ratio typed learning that is not dissimilar to \lr\ (Section~\ref{sc_lr}).

\mmmn\ defined a log-linear Markov network over multiple output labels which explores the correlation in the label space.
The compatibility score define by 
\begin{align}
	F(\vx,\vy;\vw) = \ip{\vw}{\phib(\vx,\vy)} \label{mmmn_score}
\end{align}
can be seen as the affinity of the multilabel $\vy$ to the input $\vx$ according to the Markov network.
Feature weight parameter $\vw$ in (\ref{mmmn_score}) ensures the example with correct multilabel will reach higher score than with incorrect multilabel.
To learn the feature weight parameter $\vw$, \mmmn\ defines the margin as the difference in compatibility scores between the correct example $(\vx_i,\vy_i)$ and the psudo-example $(\vx_i,\vy)$.
Under the Maximum-Margin principle (Section~\ref{sc_svm}), \mmmn\ requires the margin to be at least $\vell(\vy_i,{\vy})$.
The primal optimization problem of \mmmn\ can be formulated as
\begin{definition}{\mmmn\ Optimization Problem in Primal.}\label{def_mmmn}
	\begin{align*}
		\underset{\vw,\vxi_i}{\minimize} & \quad\frac{1}{2}\norm{\vw}^2 + C\sum_{i=1}^{m}\xi_i \\
		\st & \quad\ip{\vw}{\phib(\vx_i,\vy_i)} - \ip{\vw}{\phib(\vx_i,\vy)} \ge \vell(\vy_i,\vy)-\xi_i,\\
		& \quad \forall \xi_i\ge0,\, \forall \vy\in\vYcal/\vy_i,\, \forall i\in\{1,\cdots,m\},
	\end{align*}
\end{definition}
where $\xi_i$ is the slack allotted to each example to make sure solution can always be found, $\vell(\vy_i,\vy)$ is the loss function between the correct multilabel $\vy_i$ and the incorrect multilabel $\vy$, $C$ is the slack parameter that controls the amount of regularization in the model.
For each example $\vx_i$, the optimization calls for maximizing the margin between the correct label $\vy_i$ and incorrect labels $\vy$.
The margin is scaled by the loss function $\vell(\vy_i,{\vy})$ such that the completely incorrect multilabel will incur bigger loss than the nearly correct multilabel.


The primal optimization problem of \mmmn\ in Definition~\ref{def_mmmn} is difficult to solve as there are exponential numbers of constraints, one for each pseudo-example $(\vx_i,\vy)$.
The corresponding dual form is also difficult due to exponential number of dual variables \citep{Taskar04max}.
By exploring the Markov network structure, the original optimization problem (Definition~\ref{def_mmmn}) can be formulated into factorized dual quadratic programming, as long as the loss function $\vell$ and joint feature map $\phib(\vx,\vy)$ are decomposable over the Markov network.

As the number of parameters is quadratic in the number of training examples and edges in the network, it still cannot fit into standard QP solver. 
\citep{Taskar04max} developed a coordinate descent method analogous to the Sequential Minimal Optimization (\smo) used for SVM training \citep{Platt98sequential,Platt99fast}.
Subsequently, other efficient optimization algorithms have been proposed, which include exponential gradient optimization methods from \citet{bartlett04}, extra-gradient methods from \citet{taskar06}, sub-gradient methods from \citet{Ratliff07}, and conditional gradient methods from \citet{rousu06, rousu07}.

In order to use \mmmn\ in practice, one have to solve the \textit{loss augmented} inference problem defined as
\begin{align}
	\hat{\vy} = \underset{\vy\in\vYcal/\vy_i}{\argmax} &\, \ip{\vw}{\phib(\vx_i,\vy)} + \vell(\vy_i,\vy) \label{mmmninference}.
\end{align} 
To compute (\ref{mmmninference}) efficiently, the loss function need to be decomposable over the Markov network.
Nevertheless, \mmmn\ improves \crf\ by avoiding partition function and allowing complex loss function.


%
% 
\subsection{Support Vector Machines for Interdependent and Structured Outputs (\svmstruct)} \label{sc_svmstruct}

Support Vector Machine for interdependent and structured output space (\svmstruct) is developed in \citep{THJA04,TJTA05}.
The formalism of \svmstruct\ is quite similar to \mmmn\ described in (Section~\ref{sc_mmmn}).
Compared to \mmmn\ which scales the margin by the loss function, \svmstruct\ scales the margin errors ({\em slacks}) by the loss function.
The primal optimization problem of \svmstruct\ can be defined as
\begin{definition}{\svmstruct\ Optimization Problem in Primal.}\label{def_svmstruct}
	\begin{align*}
		\underset{\vw,\xi_i}{\minimize} & \quad\frac{1}{2}\norm{\vw}^2 + \frac{C}{m}\sum_{i=1}^{m}\xi_i \\
		\st & \quad\ip{\vw}{\phib(x_i,\vy_i)} - \ip{\vw}{\phib(x_i,\vy)} \ge 1-\frac{\xi_i}{\vell(\vy_i,\vy)},\\
		& \quad \forall \xi_i\ge0,\, \forall \vy\in\vYcal/\vy_i,\, \forall i\in\{1,\cdots,m\},
	\end{align*}
\end{definition}
where $\xi_i$ is the slack allotted to each example, $\vell(\vy_i,\vy)$ is the loss function between pseudo-label and the correct label, and $C$ is the slack parameter that controls the amount of regularization in the model.
The interpretation of Definition~\ref{def_svmstruct} is also similar as of Definition~\ref{def_mmmn}.
Besides, \citet{THJA04} suggest that \mmmn\ will work hard on the pseudo-example $(\vx_i,\vy)$ that incur big loss though they may not even close to be confusable to the true multilabel $\vy_i$.

On the other hand, the optimization techniques employed by \svmstruct\ differ significantly compared to \mmmn.
\svmstruct\ will have to work with the exponential number of constraints as the optimization is not decomposable over the Markov network.
\citet{THJA04} developed an iterative optimization approach that creates a nested sequence of successively tighter relaxation of the original problem via a cutting-plane methods \citep{Bishop07,JFY09}.
Constraints are added as necessary and the iterative optimization approach will converge to some optimal solution of $\epsilon$ precision within a polynomial number of iterations.

Besides the issue during the optimization, another problem with \svmstruct\ is the intractability of the inference problem.
To find the most violating constraint, the model need to compute the loss-augmented inference problem \citep{TJTA05} defined as
\begin{align}
	\hat{\vy} = \underset{\vy\in\vYcal/\vy_i}{\argmax} &\, [1-\ip{\vw}{\phib(\vx_i,\vy)}]\,\vell(\vy_i,\vy). \label{svmstructinference}
\end{align}
The loss function appears as a multiplicative term making (\ref{svmstructinference}) not decomposable over the Markov network.
This gives a intractable inference problem in general.
In exchange of intractability, \svmstruct\ can work with complex loss functions that do not assume any property of decomposition.
The generality of the loss function can be seen as an advantage compared to \crf\ and \mmmn.





\section{Structured Output Prediction for Molecular Activity Classification} \label{sc_su10}

The molecular activity classification problem has long been tackled with single-label classification approaches \citep{Dutt12classification}.
On the other hand, multiple interdependent molecular activities are often screened simultaneously \citep{Shoemaker06the}, which poses two challenges for single-label classification approaches.
The first is about scalability which a set of single-label classifiers need to be built independently in order to predict multiple activities.
This soon becomes infeasible when large amount of activities need to be predicted.
The second lies in the fact the single-label classification approach cannot model label correlation.
In \citepub{su10} we investigate the potential of using structured output learning to tackle the molecular activity classification problem.


\subsection{Background and Introduction}

Molecular classification, the goal of which is to predict the anticancer potentials of the drug-like molecules, is one crucial step in drug discovery and has draw enormous attention from machine learning community.
Viable molecular structures are scanned, searched, or designed for therapeutic efficacy.
In particular, expensive preclinical \textit{in vitro} and \textit{in vivo} drug tests can be largely avoided and special efforts can be devoted to few promising candidate molecules, once accurate \textit{in silico} models are available \citep{Burbidg01drug}.

Various machine learning methods have been developed for the task, to name but a few, inductive logic programming \citep{King96structure}, artificial neural network \citep{Bernazzani06predicting}, kernel methods for nonlinear molecular properties \citep{Trotter01drug,Ralaivola05graph,Swamidass05kernel,Ceroni07classification}, and \svm\ based methods for reliable predictions \citep{Trotter01drug,Byvatov03comparison,Xue04effect}.
Albeit with the large quantity of the computational models, they only focus on predicting single target variable (e.g. inhibition potential in one cell line). 
On the other hand, relative larger number of cell lines are often screened at the same time.
For example in the recent \textit{NCI-60} {Human Tumor Cell Line Screen} project \citep{Shoemaker06the}, thousands of molecular structures are tested agains hundreds of cell lines.


\subsection{Methods}

To efficiently and accurately predict molecular activities in multiple cell lines at the same time, we applied a structured output learning approach in \citepub{su10} which is to our knowledge the first multi-task learning approach for molecular classification problem.
The algorithm is an instantiation of the structured output learning method \mmcrf\ developed in \citep{rousu06}.
It can also be seen as a sibling method of \mmmn\ presented in Section~\ref{sc_mmmn}.
In particular, the model defines a compatibility score through inner product for a molecular structure $\vx$ and the activities in multiple cell lines $\vy$
\begin{align*}
	F(\vx,\vy;\vw) = \ip{\vw}{\phib(\vx,\vy)},
\end{align*}
where $\vw$ is the feature weight parameter that ensures the molecular structure with correct activities will be scored higher than with incorrect activities.
$\vw$ are learned by maximizing the minimum loss-scaled margin between correct examples $(\vx_i,\vy_i)$ and incorrect examples $(\vx_i,\vy)$ over the whole training set
\begin{align*}
	\underset{\vw,\vxi_i}{\minimize} & \quad \frac{1}{2}\norm{\vw}^2 + C\sum_{i=1}^{m}\xi_i\\
	\st &\quad \ip{\vw}{\phib(\vx_i,\vy_i)} - \ip{\vw}{\phib(\vx_i,\vy)} \ge \ell(\vy_i,\vy)-\xi_i,\\
	&\quad \xi_i>0, \forall i\in\{1,\cdots,m\}, \forall \vy\in\Ycal,
\end{align*}
where $\ell(\vy_i,\vy)$ is the loss function defined as
\begin{align*}
	\ell(\vy_i,\vy) = \sum_{j=1}^{k}\ind{\vy_i[j]\neq \vy[j]}.
\end{align*}
The loss scaled margin optimization will push high-loss pseudo-examples further away from the correct example than the low-loss pseudo-examples.
The model is optimized by conditional gradient optimization \citep{Bertsekas95nonlinear} in marginalized dual space \citep{Taskar04max}, which not only enjoys a polynomial-size parameter space in marginalized dual representation but also enables the kernel function that is capable of dealing with non-linearity of the complex molecular structures.

%Kernel is the classical way of defining the similarity between complex objects.
In \citepub{su10}, ee used graph kernel to measure the similarity of pair of molecular structures.
The common way to represent the structure of a molecule is to use an undirected labeled graph $G=(V,E)$, where vertices $V=\{v_1,\cdots,v_n\}$ corresponds to atoms and edge $E=\{e_1,\cdots,e_m\}$ corresponds to the covalent bonds.
The adjacency matrix $A$ of graph $G$ is defined such that its $(i,j)$'th entry $A_{i,j}$ equals to one if there is an edge between atom $i$ and atom $j$.

% walk kernel
\textit{Walk Kernel} \citep{Kashima03marginalized,Gartner03a} computes the sum of matching walks in a pair of graphs. 
The contribution of each matching walk is down scaled exponentially according to its length.  
We denote by $w_m$ a walk of length $m$ in graph $G$ such that there exists a edge for each pair of vertices $(v_i,v_{i+1})$ for all $i\in\{1,\cdots,m-1\}$.
In addition, we denote by $G_{\times}(G_1,G_2)$ the direct product graph of two graph $G_1$ and $G_2$, of which the vertices of a product graph are computed from 
\begin{align*}
	V_{\times}(G_1,G_2) = \{(v_1,v_2)\in V_1\times V_2, label(v_1)=label(v_2)\},
\end{align*}
and edges are computed from
\begin{align*}
	E_{\times}(G_1,G_2) = \{((v_1,v_2),(u_1,u_2))\in V_{\times}\times V_{\times},(v_1,u_i)\in E_1\wedge (v_2,u_2)\in E_2\}.
\end{align*}
The walk kernel can be defined on the adjacency matrix of $G_{\times}$
\begin{align*}
	K_{wk}(G_1,G_2) = \sum_{i,j=1}^{|V_{\times}|}\left[\sum_{n=0}^{\infty}\lambda^{n}A_{\times}^n\right]_{i,j},
\end{align*}
where $0<\lambda<1$.
Using exponential series or geometric series the walk kernel can be compute in cubic time \citep{Gartner03a} by
\begin{align*}
	K_{wk}(G_1,G_2) = \ve\tp(\vI-\lambda A_{\times})^{-1}\ve,
\end{align*}
where we use $\vI$ to denote the identity matrix and $\ve$ to denote the matrix of ones.

% weighted decomposition kernel and others
\textit{Weighted Decomposition Kernel} developed in \citep{Menchetti05weighted,Ceroni08classification} is an extension of substructure kernel \citep{Haussler99convolution}.
The kernel weights identical part in two graph with contextual information.
It evaluates the matching subgraphs (\textit{contextor}) in the neighborhood of an atom (\textit{selector}).
We also used \textit{Tanimoto Kernel} \citep{Ralaivola05graph} on a finite set of molecular fingerprints \citep{Wang09pubchem}.
See \citep{Vishwanathan10graph} for a more comprehensive survey on graph kernels.

%\subsubsection{Generation of Markov Network}
To apply structured output learning model described above, we assume the underlying Markov network structure is fully observed.
In other words, we need to build an output graph with nodes correspond to cell lines and edges correspond to potential statistical dependencies.
Auxiliary datasets are available for this purpose \citep{Shoemaker06the} which implicitly describes the correlation between cell lines.
To build the output graph, we first compute the covariance matrix for cell lines based on auxiliary data, then extracte the network structure by applying the following two methods.
In \textbf{maximum spanning tree approach}, we take the minimum number of edges that make a connected network whilst maximizing the edge weights.
In \textbf{correlation thresholding approach}, we take all edges that exceed a fixed threshold, which typically generates a non-tree graph.


%
%
\section{\spin\ for Network Response Prediction}\label{sc_su14b}

In \citepub{su14a}, a structured output learning model for network response prediction problem is developed.
The proposed model, \spin, is \textit{context-sensitive} and is different from the previous the state-of-the-art methods which only model the influence in terms of the network connectivity.
The inference problem of \spin\ is \nphard\ in general which was solved by a semidefinite programming algorithm \sdp\ with approximation guarantee and by a fast \greedy\ heuristics.

\subsection{Background and Introduction}

With the extensive availability of the large scale networks, there are tremendous interest in studying the phenomena of the network influence, in particular, the structure, the function and the influence dynamics. 
The outcome of the network influence research have been widely applied in many areas, to name but a few, spreadness of pathogens or infectious diseases \citep{Hethcote00the,Anderson02infectious}, diffusion of medical and technology innovation \citep{Strang98diffusion,Rogers03the}, opinion and news formation \citep{Adar04implicit,Gruhl04information,Adar05tracking,Leskovec07cascading,Nowell08tracing,Leskovec09meme}, viral market \citep{Domingos01mining,Kempe03maximizing,Liben-Nowell03the}.

In the field of studying network influence, one primary interest is to discover the latent structures that reveal the dynamics of influences.
In general, the problem can be defined into two different ways depending on the availability of the underlying network structure.
On one hand, one would assume that the underlying structure is hidden or incomplete and the only observation is a cascade of actions.
The instantiation of the setting can be, for example, online news agents sharing information but not physically connected, or in epidemiological study where people are affected by pathogens through various ways.
In this case, the task is to infer network structure in terms of edges connecting nodes given a set of action cascade.
%With early work on independent cascade and linear threshold model \citep{Kempe03maximizing},
Many algorithms are designed to solve the problem in this setting from \netinf\ \citep{GomezRodriguez10inferring}, \netrate\ \citep{Rodrigues11unconvering}, \kernelcascade\ \citep{Du12learning} to the most recent work about two stage model for inferring influence \citep{Du14influence}, inference algorithm using cascade without timestamp \citep{Amin14learning}, and general framework of inferring diffusion structure \citep{Daneshmand14estimating}.
On the other hand, we argue the problem is unnecessarily hard to solve as in many cases the structure of the network is given (e.g. friendship network).
There are also related researches aim to discover the hidden variables in the network \citep{Lovrek08prediction,Goyal10learning}

However, none of them consider the property of the action.
In particular, our network influence problem is motivated by the following observation: for a given action $\va$ performed on the network $G$, the influence from node $u$ to $u'$ not only depends on their connections but also depend on the action under consideration.
For example, $u'$ follows $u'$ in twitter network, $u'$ will retweet the message from $u$ if the message is related to \textit{science} but not related to \textit{politics}.
Therefore, we propose the following definition of the network influence problem
\begin{definition}{Network Influence Problem.}\\
	Given a complex network and an action performed on the network, predict the subnetwork that responses to the action. In particular, which nodes will perform the action and which directed edges relay the action from one node to its neighbors.
\end{definition}

%
\subsection{Methods}
We approach the problem by structured output learning, where we define a computability score as a inner product of the action $\va$ and the response network $\Gva$
\begin{align*}
	F(\va,\Gva;\vw) = \ip{\va}{\phib(\va,\Gva)}.
\end{align*}
It is easy to understand that the action $\va$ with correct response network $\Gva$ will achieve high score than with any incorrect response network $\Gva'$.
As introduced in previous section, the joint feature map $\phib(\va,\Gva)$ is composed by the tensor product between action feature $\varphib(\va)$ and the features of the response network $\Upsilonb(\Gva)$.
In particular, $\varphib(\va)$ can be a bag-of-words features of the action (e.g. blog post) and $\Upsilonb(\Gva)$ is a vector of edges and labels of the response network $\Gva$.

% optimization
The feature weight vector $\vw$ is learned through maximum-margin structured output learning by solving the following optimization problem
\begin{definition}{Primal \spin\ Optimization Problem.}
\begin{align*}
	\underset{\vw,\vxi_i}{\minimize} &\quad \frac{1}{2}\norm{\vw}^2+C\sum_{i=1}^{m}\xi_i\\
	\st &\quad F(\va_i,G_{\va_i};\vw) > \underset{\substack{G_{\va_i}'\in\Hcal(G)/G_{\va_i}}}{\maximize}(F(\va_i,G_{\va_i}';\vw)+\vell_{G}(G_{\va_i},G_{\va_i}')) - \xi_i,\\
	&\quad \xi_i\ge0, \forall i\in\{1,\cdots,m\},
\end{align*}
\end{definition}
where by $\Hcal(G)$ we denote the set of directed acyclic graphs of $G$.

% inference
To solve the above optimization problem, we have to compute both in training and in prediction the highest-scoring subgraph for an action. 
In particular, during the training the goal is to find the worst margin violating subgraph which corresponds to solving the loss-augmented maximization problem
\begin{align*}
	H^*(\va_i) = \underset{G_{\va_i}'\in\Hcal(G)/G_{\va_i}}{\argmax}\, (F(\va_i,G_{\va_i}';\vw)+\vell_{G}(G_{\va_i},G_{\va_i}')).
\end{align*}
During the prediction, the goal is to find the subgraph with maximum compatibility given the action
\begin{align}
	H^*(\va_i) = \underset{H\in\Hcal(G)}{\argmax}\, F(\va_i,H;\vw) \label{spin_inference}.
\end{align}
As the two problems are different only in terms of the definition of the score, we explain our inference algorithm based on (\ref{spin_inference}) by writing the problem explicitly in terms of weight vectors and the feature maps
\begin{align}
	H^*(\va_i) &= \underset{H\in\Hcal(G)}{\argmax}\, \ip{\vw}{\varphib(\va)\otimes\Upsilonb(H)} \nonumber \\
	&= \underset{H\in\Hcal(G)}{\argmax}\, \sum_{e\in E^H}s_{\vy_e}(e,\va), \label{spin_np}
\end{align}
where we denote by $s_{\vy_e}(e,\va) = \sum_{i}\vw_{i,e,\vy_e}\varphib(\va)$ the score of edge $e$ with edge label $\vy_e$.

\begin{lemma}
	Finding the graph that maximizes (\ref{spin_np}) is an \nphard\ problem.
\end{lemma}
In supplementary material of \citepub{su14a}, we have proved the \nphardness\ by forming a reduction from the \maxcut\ problem \citep{Garey90computers}.
In addition, we proposed two algorithms to solve (\ref{spin_np}).
The first is called {\sdp\ inference} which introduces for each node $u\in V$ a binary variable $x_u\in\{-1,+1\}$ and transfer the inference problem into an integer quadratic program (\iqp) problem.
The \iqp\ is then tackle by similar technique introduced in \citep{Geomans1995improved}, such that each variable $x_u$ is relaxed to a vector $\vv_u\in\RR^n$ and the relaxed problem is solved by semidefinite programming (\sdp). 
The resulting vector is rounded back into binary value by incomplete Cholesky decomposition.
The benefit from \sdp\ inference algorithm is the approximation guarantee. 
%If we denote by $Z^*$ ($Z_R$, and $Z$ respectively) the optimal value achieved by \iqp\ (the relaxed problem, and the \sdp\ inference respectively), we have $Z_R\ge Z^*$ and $E[Z]\ge(\alpha-\epsilon)Z_R$ where $\alpha\ge0.796$.
In particular, the proposed \sdp\ inference algorithm is a $0.796$ approximation of the original \iqp.

As \sdp\ inference is not scalable on large networks, we developed a \greedy\ heuristic base on observation stated as the following lemma:
\begin{lemma}
	The inference problem (\ref{spin_np}) can be stated equivalently as a function of activated vertices
	\begin{align*}
		H^{*}(\va) = \underset{{H \in {\cal H}(G)}}{\argmax} \sum_{ \substack{v_i\in V^H_{\p}}}F_m(v_i). \nonumber 
	\end{align*}
\end{lemma}
The proof is given in supplementary material of \citepub{su14a}. 
As a results, the \greedy\ algorithm starts with an empty vertex set and adds one vertex at a time such that the increment of the score is maximized over all inactivated vertices.
The iteration ends when the objective cannot be improved.  
It is worthy noticing that we are not able to give any approximation guarantee for the solutions produced by the \greedy\ algorithm.
The property of submodularity, which is often used to analyze the greedy algorithm, only holds for the special case of our inference problem.

% loss function
The decomposable loss function $\ell_G(G_{\va},G_{\va}')$ will penalize the mistakes uniformly over the network.
However, we wish the learning algorithm focus on the vicinity of the \textit{foci} based on the observation that one certain action will only affect a small part of the network.
This suggests we should penalize the mistake according to the distance to the \textit{foci}.
Therefore, we define the \textit{symmetric-difference loss}, also known as \textit{hamming loss}, applied on the nodes and edges of the network. 
In addition, we define the scaling function $\gamma_G$ according to the distance to \textit{foci}.
The resulting loss function is 
\begin{align*}
	\ell_G(G_{\va},G_{\vb}) = \sum_{u\in V}\ell_v(G_{\va},G_{\vb})\gamma_G(u_k;u) + \sum_{(u,u')\in V}\ell_e(G_{\va},G_{\vb})\gamma_G(u_k;u).
\end{align*}
We proposed two scaling function in \citepub{su14a}.
One is \textit{exponential scaling} where mistakes are penalized by $\lambda$ and $\lambda$ is weighted exponentially according to the shortest path distance to \textit{foci}.
The other is \textit{diffusion scaling} where mistakes are penalized by the values computed from diffusion kernel \citep{Kondor02diffusion}.
Diffusion scaling has the effect of shrinking the distance to the nodes that are connect to \textit{foci} by many paths.












%------------------------------------------------
%
%------------------------------------------------
\chapter{Structured Output Learning with Unknown Structure}

%
%
\section{Graph Labeling Ensemble (\mve)}\label{sc_su11}

Multi-task molecular activities classification with structured output learning that relies on the representation of the cell-lines structures allows us to model the dependencies betweem different output variables.
When apply structured output learning, it is explicitly assumed the structure of the output network is fully observed.
In \citepub{su10}, we supposed the feasibility of extracting the structures assuming it is given implicitly in the auxiliary datasets.
For many real world structured output learning problems, we cannot take the structure as granted as it is often difficult to get the underlying dependency structure.
Therefore, we explored in \citepub{su11} the possibility of constructing an simple ensemble of multiple structured output learning models built on random output graphs and applied the technique on molecular activity classification task.


\subsection{Methods}

We use \mmcrf\ as base classifier trained on a set of random output graphs.
Particularly, for each base model, a random graph $G_t$ is generated to couple the output vector variables $\vy$ which are the activities of molecular structure $\vx$ over all cell lines.
The base model \mmcrf\ is then learned based on training data $S=\{(\vx_i,\vy_i)\}_{i=1}^{m}$ and the output graph $G_t$.
After ensemble has been generated, the predictions are extracted from each base classifier and are collected for a post-processing step: we compute a majority vote over the graph labeling from the sign on the means of the base classifier's prediction
\begin{definition}{Majority Voting Ensemble (\mve).}
\begin{align*}
	F_j^{\text{\mve}}=\underset{y_j\in\Ycal_j}{\argmax}\,\left(\frac{1}{T}\sum_{i=1}^{T}\ind{F_j^{(t)}(x)=y_j}\right), \forall j\in\{1,\cdots,k\},
\end{align*}
\end{definition}
where by $T$ we denote the size of the ensemble, and by $F^{(t)}(\vx)=\{F^{(t)}_j(x)\}_{j=1}^{k}$ we denote the predicted multilabel in the $t$'th base classifier.
In words, the ensemble prediction on each microlabel is the most frequently appearing prediction among the base classifiers.
It is also worthy of noticing that the \mve\ framework can be extended with any structured output learning models as long as they incorporate the output structure into learning process and make predictions based on the structure.

We designed two approaches to generate random output graphs.
In random spanning tree approach, we first generate a random correlation matrix and extract the spanning tree out from the matrix.
The approach will eventual output a tree structure connecting all vertices.
In random pairing approach, we randomly draw two vertices at a time and couple the two with an edge.
The approach will output disconnected pairs.


\section{Random Graph Ensemble (\amm, \mam)}\label{sc_su14a}

Through extensive experiments in \citepub{su11}, we have validated the feasibility of constructing a majority voting ensemble (\mve) of structured output learner built on a set of random output graphs.
The proposed method also improved the performance on molecular activity classification problem.
In \citepub{su14b}, we aim to develop elegant aggregation techniques (\amm\ and \mam\ respectively) that will perform inference before or after forming ensemble.
We tested the proposed techniques on a set of heterogeneous multilabel datasets from various domains to verify the wide applicability.
In addition, we have brought forward a theory that explains the improvement of \mam\ framework.
The theory is based on the reconstruction error of the compatibility score.


\subsection{Background and Introduction}

% compatibility score
Besides the assumption made for \mve\ which the base classifier will devise the prediction according to the structure of the output graph, we also assume that the base classifier for \amm\ and \mam\ is modelled with a Markov network structure $G=(E,V)$.
That is the base classifier defines a compatibility score $\psi(\vx,\vy)$ for pairs $(\vx,\vy)\in\vXcal\times\Ycal$ according to the output graph $G$, indicating how well the input and the output goes together
\begin{align*}
	\psi(\vx,\vy) = \ip{\vw}{\phib(\vx,\vy)} = \sum_{e\in E}\ip{\vw}{\phib_e(\vx,\vy_e)}=\sum_{e\in E}\psi_e(\vx,\vy_e),
\end{align*}
where by $\psi_e(\vx,\vy_e)$ we denote edge compatibility score, or \textit{edge potential}, between input $\vx$ and edge label $\vy_e$ on edge $e$.
$\vw$ is the feature weight parameters that naturally ensures input $\vx$ with correct output $\vy$ achieves higher compatibility score than with incorrect output.

% edge potential
In addition, we assume we have access to the edge potentials between input and edge labels for each base classifiers
\begin{align*}
	\psib_E^{(t)} = (\psi_e^{(t)}(\vx,\vu_e))_{e\in E^{(t)},\vu_e\in\Ycal_e}.
\end{align*}
With edge compatibility scores, we can infer the max-marginal of label $u_i$ on node $y_i$ \citep{Wainwright05map}.
\begin{align*}
	\tilde{\psi}_j(\vx,u_j) = \underset{\vy\in\Ycal,y_j=u_j}{\maximize}\sum_{e\in E}\psi_e(\vx,\vy_e).
\end{align*}
That is the maximum score of the multilabel consistent with $y_i=u_i$.
We denote the collection of max-marginals by $\tilde{\psib} = (\tilde{\psi}_j(\vx,u_j))_{j\in V,u_j\in\Ycal_j}$.

\subsection{Methods}

% amm
Denote by  $\Gcal=\{G^{(1)},\cdots,G^{(T)}\}$ a set of random output graphs, and by $\{\tilde{\psib}^{(1)},\cdots,\tilde{\psib}^{(T)}\}$ the max-marginal vectors from base classifiers that is built on each output graph.
The \amm\ prediction on each node is obtained by averaging the max-marginals of base classifiers and choose the maximizing microlabel for the node
\begin{definition}{Average of Max-Marginal Aggregation (\amm).}
	\begin{align*}
	F_j^{\text{\amm}}=\underset{u_j\in\Ycal_j}{\argmax}\,\frac{1}{T}\sum_{t=1}^{T}\tilde{\psi}_{j,u_j}^{(t)}(\vx),
	\end{align*}
\end{definition}
and the predicted multilabel is composted by the predicted microlabels
\begin{align*}
	F^{\text{\amm}}=\left(F_j^{\text{\amm}}\right)_{j\in V}.
\end{align*}

% mam
\amm\ performs the inference to find the max-marginals before forming ensemble. 
On the other hand, the maximum of average marginals aggregation (\mam) will first collect the local edge potentials $\psib_E^{(t)}$ from each base classifier, average them and finally performs inference on the global consensus graph $\hat{G}=(\hat{E},V)$ with averaged edge potentials
\begin{definition}{Maximum of Average marginals Aggregation (\mam).}
	\begin{align*}
		F^{\text{\tiny MAM}}(x) &= \underset{\vy \in \Ycal }{\argmax}\,\sum_{e\in \hat{E}}\frac{1}{T}\sum_{t=1}^{T}\ \psi^{(t)}_e(x,\vy_e)
		%\vy^* &= \underset{\vy}{\argmax}\, \frac{1}{T}\sum_{t=1}^{T}\sum_{e\in E_t}{w^t_e}^T\varphib_e(x,\vy)
		= \underset{\vy \in \Ycal}{\argmax}\, \frac{1}{T} \sum_{t=1}^T \sum_{e\in \hat{E}} \ip{\vw_e^{(t)}}{\phib_e(x,\vy_e)}. %\label{ensemble3}
	\end{align*}
\end{definition}
\citepub[Lemma 1]{su14b} allows us to simplify the computation of \mam\ in terms of dual variables and kernels.
As a results, the \mam\ ensemble can be equivalently stated as
\begin{align*}
F^{\text{\tiny MAM}}(x) 
 & =\underset{\vy \in \Ycal}{\argmax}\, \frac{1}{T} \sum_{t=1}^T \sum_{i,e,\vu_e} \mu^{(t)}(i,e,\vu_e) \cdot H_e(i,\vu_e;x,\vy_e)\nonumber \\
 & =\underset{\vy \in \Ycal}{\argmax}\,   \sum_{i,e,\vu_e}  \bar\mu(i,e,\vu_e) H_e(i,\vu_e;x,\vy_e),
\end{align*}
where by $\bar\mu(i,e,\vu_e)=\frac{1}{T}\sum_{t=1}^T\mu^{(t)}(i,e,\vu_e)$ we denote the marginalized dual variable averaged over ensemble, and
\begin{align*}
	H_e(x_i,\vu_e;x,\vy_e)  = K_{\phib}(x,x_i)\cdot\left(K_{\Upsilon,e}(\vy_{ie},\vy_e)-K_{\Upsilon,e}(\vu_e,\vy_e)\right).
\end{align*}
	
Besides the algorithm frameworks for random graph ensemble, we also developed theoretical analysis to explain the improvement of \mam\ ensemble.
The analysis extends the theory on single task ensemble developed by \citet{Brown10good}.
In general, \citepub[Theory 1]{su14b} states that the reconstructive error from \mam\ ensemble is guaranteed to be less than or equal to the average reconstruction error from the base classifiers.
The improvement can be decomposed into two terms, namely \textit{diversity} and \textit{coherence}.
The former measures the variability of individual classifiers learned from different perspectives which shares the same argument as the analysis of single task ensemble model \citep{Brown10good}, the later measures the correlation of microlabel predictions which higher correlation gives better improvements.

%
% RTA
%
\section{Random Spanning Tree Approximation (\rta)}\label{sc_su14c}

Random Spanning Tree Approximation (\rta) model developed in \citepub{su14c} is a major step forward of \mam.
The goal of \rta\ is to bring in joint learning and inference framework as well as to provide extensive theory to guarantee the performance and the tractability of the inference.
Especially, \rta\ explores the potential of tackling the intractable graph inference problem with a set of random spanning trees.
Thus, it lays the foundation of performing inference on unknown structure to achieve accurate optimization with attainable computational efforts.

\subsection{Background and Introduction}

Limiting the applicability of structured output learning methods is the fact that the output structure is assume to be observed.
However, it is often prohibitive to obtain the correct output structure, sometimes it is arguably a even harder problem that the structured output learning itself \citep{Chickering94learning}.
In stead, one can opt to the \textit{complete graph}, in which there is an edge connecting each pair of vertices, as the output graph structure and rely on the learning algorithm to discover the proper 'parameters' defined for the edges (e.g. edge potentials).
To understand this, think the edges that are not from the correct output graph will have zero 'parameters' in the complete graph.

Structured output learning with complete graph is by no mean a easier problem as the inference is \nphard\ in nature, which is often instantiated as finding the \textit{maximum a posteriori} (\map) configuration on a graph-structured probability distribution.
In terms of the intractability issue of graph inference problem, many techniques have been proposed but with important differences.
\citet{Jordan04semiefinite} developed semi-definite programming convex relaxation for inference on graph with cycles.
\citet{Wainwright05map} proposed MAP inference with tree-based and \textit{linear programming} (LP) relaxation.
Efficient inference on special case of graph has also been extensively studied in \citep{Globerson07approximate}.

\citepub{su14c} is motivated by well-established max-margin assumption presented in Section~\ref{sc_svm} and investigate whether the problem of inference over a complete graph in structured output learning can be avoided in a analogous way.
Start from a sampling results, \citepub[Lemma 3]{su14c} shows that with high probability a big fraction of margin from the complete graph can be obtained by a sample of spanning trees of small size.
Besides, \citepub[Theorem 5]{su14c} shows the good generalization can also be guaranteed when learning with in stead of complete graph a sample of spanning trees.

Thus, in addition to \citepub[Theory 1]{su14b}, we further provide the theoretical justification of \mam\ that constructs a set of base classifiers trained on random spanning trees. 
Besides, \citepub[Theorem 5]{su14c} suggests we should optimize the joint margin from all spanning trees, instead of optimizing them separately as \mam, which leads to the \rta\ model described in the following section.

\subsection{Methods}

Denote by $\Tcal=\{T_1,\cdots,T_n\}$ a sample of $n$ random spanning trees, and by $\{\vw_{T_t}|T_t\in\Tcal\}$ the feature weights to be learner on each tree.
The goal of optimization is to maximize the joint margin from all spanning trees between correct training examples and other examples.
\begin{definition}{\bf Primal $L_2$-norm Random Tree Approximation (\rta).}\label{primalrta}
	\begin{align*}
		\underset{\vw_{T_t},\xi_i}{\minimize} & \quad \frac{1}{2}\sum_{t=1}^{n}\norm{\vw_{T_t}}_2^2 + C\sum_{i=1}^{m}\xi_i\\
		\st & \quad \sum_{t=1}^{n}{ \langle \vw_{T_t}, \phib_{T_t}(\vx_i,\vy_i) \rangle} - \underset{\vy \neq \vy_i}{\maximize\ } \sum_{t=1}^{n}{\langle \vw_{T_t}, \phib_{T_t}(\vx_i,\vy) \rangle } \geq 1 -  \xi_i, \\
		& \quad \xi_i\ge0\, , \forall\ i \in \set{1,\dots,m},
	\end{align*}
\end{definition}
where $\phib_{T_t}(\vx,\vy)$ is the feature map that is local on each tree $T_t$, $\xi_i$ is the margin slack allocated for each $\vx_i$, and $C$ is the slack parameter that controls the amount of regularization.

The key for the optimization is to solve the '$\argmax$' problem efficiently.
This is a \nphard\ problem in practice, as the size of the multilabel space is exponential in terms of the number of microlabels.
For the optimization problem, we proposed in \citepub{su14c} a $K$-best inference algorithm working in $\Theta(Knk)$ time per data point, where $k$ is the number of microlabels in the multilabel vector and $K$ is the number of best multilabels we compute from each random spanning tree.

In particular, it is known that the exact solution for the inference problem on individual tree $T_t$ is tractable \citep{Koller09probabilistic}, for which 
\begin{align}\label{rta:maximizer_onetree}
	\hat{\vy}_{T_t}(x) = \underset{\vy\in\Ycal}{\argmax}\, F_{\vw_{T_t}}(x,\vy)\ =\ \underset{\vy\in\Ycal}{\argmax}\, \langle \vw_{T_t}, \phib_{T_t}(x,\vy)\rangle,
\end{align}
can be solved in $\Theta(k)$ time by \textit{dynamic programming} also known as \textit{max-product} or \textit{min-sum}.
However, there is no guarantee the the maximizer of (\ref{rta:maximizer_onetree}) is also the global maximizer of (Definition~\ref{primalrta}) over all spanning trees.
Therefore, we compute for each random spanning tree $K$-best multilabels, which in total costs $\Theta(Knk)$ time for all spanning trees.
\citepub[Lemma 7]{su14c} provides a method to retrieve the best multilabel from $K$-best list in linear time.
Now the question is to make sure the global violation multilabel is within the list.
\citepub[Lemma 8]{su14c} guarantees that with high probability the global violation multilabel is in the list and $K$ does not need to be large.

In addition, we derived the marginalized dual representation of (Definition~\ref{primalrta})
\iffalse
\begin{definition}{\bf $L_2$-norm RTA Marginalized Dual.}
	\begin{align*}
		%\underset{\mub\in\Mcal^m}{\maximize} &\quad \frac{1}{|E_\Tcal|}\sum_{k,e,\vu_e}\mu(k,e,\vu_e)   -\frac{1}{2}\sum_{k,e,\vu_e} \mu(k,e,\vu_e) K_\Tcal^e(k,\vu_e;k',\vu'_e) \mu(k',e,\vu'_e)\, ,
		\underset{\vmu\in\Mcal^m}{\maximize} &\quad \frac{1}{|E_\Tcal|}\sum_{e,k,\vu_e}\mu(k,e,\vu_e)   -\frac{1}{2}\sum_{\substack{e,k,\vu_e,\\k',\vu_e'}} \mu(k,e,\vu_e) K_\Tcal^e(x_k,\vu_e;x_k',\vu'_e) \mu(k',e,\vu'_e)\, ,
	\end{align*}
\end{definition}
\fi
which not only enjoys a polynomial sized parameter space but also enable kernel function to deal with complex input space.





%------------------------------------------------
%
%------------------------------------------------
\chapter{Evaluation and Comparison Methods} \label{ch_ecm}

In this chapter, the author will briefly introduce the performance measures and statistical test methods that are involved in the research. 
These methods enable us to quantitatively evaluate the performance of the proposed models and to measure the significancy comparing to other well established methods.

%------------------------------------------------
%
%------------------------------------------------
\section{Performance Measures} \label{sc_pm}

It is often difficult and expensive to obtain labeled data. 
In order to maximize the outcomes of the valuable labeled data, the common approach adopted by machine learning community is known as \textit{$K$-fold cross-validation}.
In particular, the origin set of labeled examples are partitioned into $K$ disjoint subsets of same size denoted by $\{S_1,\cdots,S_K\}$.
The cross-validation framework will repeat training and test procedures $K$ times.
In the $i$'th iteration, the method uses $K-1$ subsets of labeled examples $\cup_{j\in\{1,\cdots,K\}/i}S_j$ to build a training set in order to construct a classifier.
The examples in the $i$'th subset is taken as test data of which the labels are assumed to be unknown to the classifier.
The resulting classifier is used to predict the label of each example from the test set $S_i$.
The performance is evaluated by comparing to the true labels from the test set $S_i$.
In extreme case where we have $K$ equal to the number of original labeled examples, there will be only one example in each test set.
This is typically known as \textit{leave-one-out cross-validation} (\loo).

Most machine learning algorithms explicitly assume that the training data and the test data are drawn from the same distribution.
We should make sure that the statistical property of the original labeled data is well preserved when partitioning the original labeled examples into subsets.
In single-label classification problem we can easily preserve the label distribution if we randomly sample labeled examples without replacement from the origin dataset.
However, it is not clear how to preserve label distribution under multilabel classification setting.
To this end, we use \textit{stratification} (\textit{stratified sampling}) method where we first group the labeled examples into equivalent classes based on the number of positive labels they have. 
Each class is then randomly split into $K$ local fold, after that local folds are merged to create $K$ global fold.
The stratification scheme ensures that also smaller classes have representations in all folds. 

After partitioning the original data into training set and test set, we adopt several well-known performance measures in order to quantitively evaluate the performance of difference classifiers.
We report \textit{multilabel accuracy} (\textit{0/1 accuracy}) which counts the percentage of multilabel predictions that have all of the microlabel being correct.
The counterpart is often known as \textit{multilabel loss} (\textit{0/1 loss}) defined as
\begin{align*}
	\ell_{0/1} = \frac{1}{m}\sum_{i=1}^{m}\ind{\vy_i \neq \hat{\vy}_i},
\end{align*}
where by $\vy_i$ we denote the predicted multilabel of the $i$'th example $\vx_i$, and by $\hat{\vy}_i$ we denote ground true multilabel.
We also report \textit{microlabel accuracy} as the proportion of microlabel being correct, of which the counterpart is often known as \textit{hamming loss} defined as
\begin{align*}
	\ell_{h} = \frac{1}{mk}\sum_{i=1}^{m}\sum_{j=1}^{k}\ind{\vy_i[j] \neq \hat{\vy}_i[j]}.
\end{align*}
In addition, we report \textit{$F_1$ score} as the harmonic mean of \textit{microlabel precision} and \textit{microlabel recall} defined as
\begin{align*}
	F_1 = 2\cdot\frac{Pre\times Rec}{Pre+Rec}.
\end{align*}
Microlabel precision and microlabel recall are defined as
\begin{align*}
	Pre = \frac{TP}{TP+FP}, \quad Rec = \frac{TP}{TP+FN},
\end{align*}
where we use $TP$, $FP$, and $FN$ to denote \textit{true positive}, \textit{false positive}, and \textit{false negative} predictions respectively.


%------------------------------------------------
%
%------------------------------------------------
\section{Statistical Tests} \label{sc_st}

Once we have computed the performance measures of multiple machine learning algorithms, we want to compare the performance in order to decide the best model under consideration. 
To serve as a starting point, we assume a very basic setting where we already have the performance of multiple machine learning algorithms on one selected dataset.
It is straight forward to compare the performance of the algorithms in terms of the performance metrics and choose the best model based on the value of the metric without resorting to any statistical methods.
We only assume that the metrics accurately reveal the performance of the algorithms.
However, when a new learning algorithm is proposed, it is usually evaluated over multiple dataset to rule out the probability that it performs well on one particular dataset by chance.
Statistical evaluation of experimental results have been considered as an essential part of comparison steps in such complicate scenarios.
We will base our discuss on \citep{Demsar06statistical,Garcia08an} in the following part of the section.

We first consider the case where we want to compare two learning algorithms over multiple datasets.
In order to decide the best performing method, one way is to average the performance over all test datasets.
However, it is debatable that the error rate from different domains (test datasets) are commensurable.
Beside, the averaging is susceptible to the outliers.
Another approach is to use paired t-test to check if the average difference of two learning algorithms over datasets is significantly different from zero.
Paired t-test is problematic due to commensurability.
Besides, the result will not be significant unless the sample size is larger enough.
\citet{Demsar06statistical} proposed Wilcoxon signed-ranks test which is a non-parametric alternative to the paired t-test to compared two algorithms over multiple datasets.
Meanwhile, sign-test is also proposed in \citep{Demsar06statistical}.
As the setting is not very interesting for our purpose, we will not expand our discussion on the details.

Secondly, we will consider a more common situation where we want to compare multiple learning algorithms over many datasets.
In practice, we want to answer the question, for example, whether algorithm A is significantly better than algorithm C, and algorithm C outperformances algorithm B in a significantly manner.
In statistics, this amounts to multiple hypothesis test.
The common way is to use repeat-measures ANOVA \citep{Fisher59statistical} with post-hoc Tukey test \citep{Turkey49comparing}. 
The test procedure is based on two assumptions: the first is known as \textit{normality} meaning samples are drawn from normal distributions; the second is known as \textit{sphericity} meaning samples should have equal variant.
It is not difficult to see that the ANOVA is ill-posed as none the assumptions are met within the context of comparing the performance of learning algorithms over datasets.

To alleviate the problems, \citet{Demsar06statistical} suggest to use Friedman test \citep{Friedman37the,Friedman40Comparison} with post-hoc Nemenyi test \citep{Nemenyi63distribution}.
The goal of the test procedure is to examine whether a pair of algorithms are significantly different from each other under multiple hypothesis test setting.
To perform Friedman test, we first compute the average rank $R_j$ of the $j$'th algorithms.
The Friedman test then compares the average ranks and under null-hypothesis states that the average ranks are equal.
The test statistics can be computed according to the procedure discussed in \citep{Demsar06statistical}.
If null-hypothesis is rejected, we can proceed with post-hoc Nemenyi test to compare algorithms with each other.
In particular, we can compute the \textit{critical different} ($CD$) and state that the performance of two classifiers are significantly different if the corresponding average rank differs by at least $CD$.
As the post-hoc Nemenyi test is usually conservative and has little statistical power, we sometimes use Bonferroni-Dunn test \citep{Dunn61mulitple} to compare the learning algorithms against the selected control which is usually the newly proposed algorithm.
The test results are often visualized as \textit{$CD$ diagram}.









%------------------------------------------------
%
%	chapter
%
%------------------------------------------------
\chapter{Implementations} \label{ch_implementations}

Developed methods are implemented as software packages and can be located from public domains.
\begin{enumerate}
	\item \rta\ for joint learning and inference on a random sample of spanning trees for multilabel classification.
	\item \spin\ for network influence prediction.
	\item \mve, \amm, \mam\ for learning an ensemble on a set of structured output learners built from random spanning trees.
	\item Structured output prediction model for molecular activities classification problem.
\end{enumerate}




%------------------------------------------------
%
%	chapter
%
%------------------------------------------------
\chapter{Conclusion}\label{ch_conclusion}

%------------------------------------------------
%	section
%------------------------------------------------
\section{Discussion}

In this thesis, we study supervised learning in structured output space of which the task is to predict the best values for multiple interdependent variables given an arbitrary input variable.
In particular, we focus on the problem of structured output learning when the output structure is unknown.
We propose a learning framework that aims to optimize on a random sample of spanning trees as output structure.
The novel learning framework is well motivated in terms of margin achieved by training examples, and the performance in terms of generalization error is also guaranteed.
In addition to the new learning framework, we also develop a new structured output prediction method that is able to predict a directed acyclic graph (DAG) and apply the methods on network influence prediction problem.
We also tackle the optimization problem of the proposed models for which the exponential sized parameter space and the \nphard\ inference problem can usually be avoided.

We have posed several research questions at the beginning of the thesis which serve as the guide through the presentation of the work.
Section~\ref{sc_single} and Section~\ref{sc_multi} introduce the backgrounds of both single-task and multi-task learnings.
The sections also present several well-established algorithms that are relevant to the thesis.
Section~\ref{sc_ensemble} discuss a few ensemble approaches that are close related to the learning framework developed in the thesis.
After the introduction, Chapter~\ref{ch_models} devotes to bringing in novel statistical models for structured output prediction that are developed by the author.
We will revisit the research questions with the insights and the results we gain from the novel statistical models that are briefly presented in the thesis as well as detailed in the publications.
In particular, we give the answer to the following questions.

\begin{enumerate}[label=\textbf{Q \Roman*}:]
\item Is the structured output prediction models more suitable for predicting multiple interdependent variables in structured output space?

Multi-task structured output prediction problem presented in Section~\ref{sc_multi} requires multiple output variables to be predicted at the same time.
The structured output prediction models are more suitable for the problem as it describes the interdependencies between the output variables and is more efficient compared to the single-task models presented in Section~\ref{sc_single}.
Besides, we have demonstrated the performance of the structured output prediction models in molecular activity prediction problem presented in \citepub{su10} and in network influence prediction problem presented in \citepub{su14a}.

\item How to tackle the problem of structured output learning without observed output structure?

As it is often difficult to get the output structure that describes the interdependencies of the output variables, we have developed several algorithms which eventually lead to a general learning framework that learns from a random sample of spanning trees.
In particular, we have developed the following learning algorithms.
\mve\ which collects and postprocesses the predictions from each spanning tree.
\amm\ and \mam\ both performs parameter learning based on individual random spanning tree.
The only difference is that the inference of \amm\ is also computed on individual tree, on the other hand, the inference of \mam\ is based on the consensus graph by pooling edges from all spanning trees.
Furthermore, \rta\ brings in the joint learning and inference such that all random spanning trees are optimized toward the same global objective.

\item What is the motivation of the proposed structured output learning framework? Can we explain the improvement and guarantee the empirical risk of the proposed models?

The proposed learning framework is designed for structured output prediction problem with unknown output structure.
Learning with any observed structure can be seen as a special case of learning with a complete graph.
The idea of learning from a random sample of spanning trees as output graph has been explained in \citepub{su14c} as an approximation to learning with a complete graph in terms of margin achieved on a set of training examples.
Thus, the proposed new learning framework is well motivated.
The improvement of the performance can be explained in terms of reconstruction error of the compatibility scores as in \citepub{su14a}.
\citepub{su14c} also provide the generalization error for the proposed model.

\item Can we efficiently optimize the proposed structured output prediction models?

The optimization problem in structured output prediction is usually difficult to optimize for two reasons, exponential sized parameter space due to the number of possible solutions and the \nphard\ inference problem instantiated as MAP configuration on the general graph.
To avoid the exponential sized parameter space, we use marginalize dual decomposition technique which reduces the parameter space to polynomial size in terms of the number of edges in the output graph.
We used different strategies to overcome the \nphard\ inference problem.
We applied loopy belief propagation for the inference problem defined on the general graph structure with approximate solution.
We used dynamic programming for the inference problem defined on a tree structure where the exact solution is guaranteed.
We developed $K$-best inference algorithm in \citepub{su14c} for the inference problem defined on a set of random spanning trees.
We have also developed \sdp\ inference algorithm for the inference problem defined on directed acyclic graph (DAG).
\end{enumerate}


%------------------------------------------------
%	section
%------------------------------------------------
\section{Future Work}

What has been shown in the thesis is a new learning framework with algorithm instantiations for structured output prediction problems where the structure of the output is not known but is expected to play an important role.
An immediate extension is to explore its enormous potentials in real world practical data analysis applications with the significant benefit that we do not need to know the output structure as \textit{prior}.
In addition, since the inference algorithm designed for optimizing over a random sample of spanning tree is tractable both in theoretic analysis and empirical results, we can apply the model on the structure prediction problems for which the output structure is rather complex.

With respect to algorithm development and theoretical study, we plan to continue with \rta\ framework that performs joint learning and inference over a random sample of spanning trees.
In particular, we will explore the possibility of applying $L_1$ norm combination of the feature weight parameter $\vw_{T_t}$ of individual random spanning tree, instead of using convex $L_2$ norm combination. 
Learning with $L_1$ norm regularization of parameter combination can be expressed into an equivalent form of learning weighted $L_2$ norm regularization \citep{Rakotomamonjy08simplemkl}, where the weight of $\{\vw_{T_t}\}_{t=1}^{T}$ is the extra parameter during the optimization.
We also gain benefit by seeing the weight as the fitness of each spanning tree to current training data.
Similar to $L_2$ norm, the studies on generalization error and the condition for the tractable inference are required.
Besides the alternative optimization for the $L_1$ norm combination in \citep{Rakotomamonjy08simplemkl}, we plan to develop more elegant optimization strategy for the problem. 








%% Examples of article references, remove these from your manuscript!
% Uncomment them, if you want to see the results of these commands in this example document
% Refer to the Journal paper 1 of this example document
%\citepub{su14b} \& \cpub{su14b} \& \cp{su14b} \& \pageref{su14b} \& \ref{su14b}
% Refer to the Conference paper of this example document
%\citepub[p.~2]{su14a} \& \cpub[Sec.~ 1]{su14a} \&  \cp[pp.~1--2]{su14a} \& \pageref{su14a} \& \ref{su14a} 


\bibliographystyle{apalike} 
\bibliography{dissertation}
